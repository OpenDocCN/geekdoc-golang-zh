- en: Goroutine Worker Pools in Go¶
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://goperf.dev/01-common-patterns/worker-pool/](https://goperf.dev/01-common-patterns/worker-pool/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Go’s concurrency model makes it deceptively easy to spin up thousands of goroutines—but
    that ease can come at a cost. Each goroutine starts small, but under load, unbounded
    concurrency can cause memory usage to spike, context switches to pile up, and
    overall performance to become unpredictable.
  prefs: []
  type: TYPE_NORMAL
- en: A worker pool helps apply backpressure by limiting the number of active goroutines.
    Instead of spawning one per task, a fixed pool handles work in controlled parallelism—keeping
    memory usage predictable and avoiding overload. This makes it easier to maintain
    steady performance even as demand scales.
  prefs: []
  type: TYPE_NORMAL
- en: Why Worker Pools Matter[¶](#why-worker-pools-matter "Permanent link")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While launching a goroutine for every task is idiomatic and often effective,
    doing so at scale comes with trade-offs. Each goroutine requires stack space and
    introduces scheduling overhead. Performance can degrade sharply when the number
    of active goroutines grows, especially in systems handling unbounded input like
    HTTP requests, jobs from a queue, or tasks from a channel.
  prefs: []
  type: TYPE_NORMAL
- en: A worker pool maintains a fixed number of goroutines that pull tasks from a
    shared job queue. This creates a backpressure mechanism, ensuring the system never
    processes more work concurrently than it can handle. Worker pools are particularly
    valuable when the cost of each task is predictable, and the overall system throughput
    needs to be stable.
  prefs: []
  type: TYPE_NORMAL
- en: Basic Worker Pool Implementation[¶](#basic-worker-pool-implementation "Permanent
    link")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here’s a minimal implementation of a worker pool:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Cryptography is for illustration purposes of CPU-bound code
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this example, five workers pull from the `jobs` channel and push results
    to the `results` channel. The worker pool limits concurrency to five tasks at
    a time, regardless of how many tasks are sent.
  prefs: []
  type: TYPE_NORMAL
- en: Worker Count and CPU Cores[¶](#worker-count-and-cpu-cores "Permanent link")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The optimal number of workers in a pool is closely tied to the number of CPU
    cores, which you can obtain in Go using `runtime.NumCPU()` or `runtime.GOMAXPROCS(0)`.
    For CPU-bound tasks—where each worker consumes substantial CPU time—you generally
    want the number of workers to be equal to or slightly less than the number of
    logical CPU cores. This ensures maximum core utilization without excessive overhead.
  prefs: []
  type: TYPE_NORMAL
- en: If your tasks are I/O-bound (e.g., network calls, disk I/O, database queries),
    the pool size can be larger than the number of cores. This is because workers
    will spend much of their time blocked, allowing others to run. In contrast, CPU-heavy
    workloads benefit from a smaller, tightly bounded pool that avoids contention
    and context switching.
  prefs: []
  type: TYPE_NORMAL
- en: Why Too Many Workers Hurts Performance[¶](#why-too-many-workers-hurts-performance
    "Permanent link")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Adding more workers can seem like a straightforward way to boost throughput,
    but the benefits taper off quickly past a certain point. Once you exceed the system’s
    optimal level of concurrency, performance often degrades instead of improving.
  prefs: []
  type: TYPE_NORMAL
- en: Scheduler contention increases as the Go runtime juggles more runnable goroutines
    than it has logical CPUs to run them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Context switching grows more frequent, burning CPU cycles without doing real
    work.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory pressure rises because each goroutine holds its own stack, even when
    idle.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cache thrashing becomes more likely as goroutines bounce across cores, disrupting
    locality and degrading CPU cache performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The result: higher latency, increased GC activity, and reduced throughput—the
    exact opposite of what a properly tuned worker pool is supposed to deliver.'
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarking Impact[¶](#benchmarking-impact "Permanent link")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Worker pools shine in scenarios where the workload is CPU-bound or where concurrency
    must be capped to avoid saturating a shared resource (e.g., database connections
    or file descriptors). Benchmarks comparing unbounded goroutine launches vs. worker
    pools typically show:'
  prefs: []
  type: TYPE_NORMAL
- en: Lower peak memory usage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More stable response times under load
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improved CPU cache locality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <details class="example"><summary>Show the benchmark file</summary>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]</details>'
  prefs: []
  type: TYPE_NORMAL
- en: 'Results:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Benchmark | Iterations | Time per op (ns) | Bytes per op | Allocs per op
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| BenchmarkUnboundedGoroutines-14 | 2,274 | 2,499,213 ns | 639,350 | 39,754
    |'
  prefs: []
  type: TYPE_TB
- en: '| BenchmarkWorkerPool-14 | 3,325 | 1,791,772 ns | 320,707 | 19,762 |'
  prefs: []
  type: TYPE_TB
- en: In our benchmark, each task performed a CPU-intensive operation (e.g., cryptographic
    hashing, math, or serialization). With `workerCount = 10` on an Apple M3 Max machine,
    the worker pool outperformed the unbounded goroutine model by a significant margin,
    using fewer resources and completing work faster. Increasing the worker count
    beyond the number of available cores led to worse performance due to contention.
  prefs: []
  type: TYPE_NORMAL
- en: When To Use Worker Pools[¶](#when-to-use-worker-pools "Permanent link")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use a goroutine worker pool when:'
  prefs: []
  type: TYPE_NORMAL
- en: The workload is unbounded or high volume. A pool prevents uncontrolled goroutine
    growth, which can lead to memory exhaustion, GC pressure, and unpredictable performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unbounded concurrency risks resource saturation. Capping the number of concurrent
    workers helps avoid overwhelming the CPU, network, database, or disk I/O—especially
    under load.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You need predictable parallelism for stability. Limiting concurrency smooths
    out performance spikes and keeps system behavior consistent, even during traffic
    surges.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tasks are relatively uniform and queue-friendly. When task cost is consistent,
    a fixed pool size provides efficient scheduling with minimal overhead, ensuring
    good throughput without complex coordination.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Avoid a worker pool when:'
  prefs: []
  type: TYPE_NORMAL
- en: Each task must be processed immediately with minimal latency. Queuing in a worker
    pool introduces delay. For latency-critical tasks, direct goroutine spawning avoids
    the scheduling overhead.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can rely on Go's scheduler for natural load balancing in low-load scenarios.
    In light workloads, the overhead of managing a pool may outweigh its benefits.
    Go’s scheduler can often handle lightweight parallelism efficiently on its own.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Workload volume is small and bounded. Spinning up goroutines directly keeps
    code simpler for limited, predictable workloads without risking uncontrolled growth.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
