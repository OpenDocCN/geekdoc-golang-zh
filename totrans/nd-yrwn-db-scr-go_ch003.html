<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <title>ch003.xhtml</title>
  <style>
  </style>
  <link rel="stylesheet" type="text/css" href="../styles/stylesheet1.css" />
</head>
<body epub:type="bodymatter">
<section id="indexing-data-structures" class="level1">
<h1>02. Indexing Data Structures</h1>
<section id="types-of-queries" class="level2">
<h2>2.1 Types of queries</h2>
<p>Most SQL queries can be broken down into 3 types:</p>
<ol type="1">
<li>Scan the whole data set. (No index is used).</li>
<li>Point query: Query the index by a specific key.</li>
<li>Range query: Query the index by a range. (The index is sorted).</li>
</ol>
<p>There are ways to make scanning fast, such as column-based storage. But a scan is <span class="math inline"><em>O</em>(<em>N</em>)</span> no matter how fast it is; our focus is on queries that can be served in <span class="math inline"><em>O</em>(log <em>N</em>)</span> using data structures.</p>
<p>A range query consists of 2 phases:</p>
<ol type="1">
<li>Seek: find the starting key.</li>
<li>Iterate: find the previous/next key in sorted order.</li>
</ol>
<p>A point query is just seek without iterate; a sorting data structure is all we need.</p>
</section>
<section id="hashtables" class="level2">
<h2>2.2 Hashtables</h2>
<p>Hashtables are viable if you only consider point queries (get, set, del), so we will not bother with them because of the lack of ordering.</p>
<p>However, coding a hashtable, even an in-memory one, is still a valuable exercise. It’s far easier than the B-tree we’ll code later, though some challenges remain:</p>
<ul>
<li>How to grow a hashtable? Keys must be moved to a larger hashtable when the load factor is too high. Moving everything at once is prohibitively <span class="math inline"><em>O</em>(<em>N</em>)</span>. Rehashing must be done progressively, even for in-memory apps like Redis.</li>
<li>Other things mentioned before: in-place updates, space reuse, and etc.</li>
</ul>
</section>
<section id="sorted-arrays" class="level2">
<h2>2.3 Sorted arrays</h2>
<p>Ruling out hashtables, let’s start with the simplest sorting data structure: the sorted array. You can binary search on it in <span class="math inline"><em>O</em>(log <em>N</em>)</span>. For variable-length data such as strings (KV), use an array of pointers (offsets) to do binary searches.</p>
<p>Updating a sorted array is <span class="math inline"><em>O</em>(<em>N</em>)</span>, either in-place or not. So it’s not practical, but it can be extended to other updatable data structures.</p>
<p>One way to reduce the update cost is to split the array into several smaller non-overlapping arrays — nested sorted arrays. This extension leads to B+tree (multi-level n-ary tree), with the additional challenge of maintaining these small arrays (tree nodes).</p>
<p>Another form of “updatable array” is the log-structured merge tree (LSM-tree). Updates are first buffered in a smaller array (or other sorting data structures), then merged into the main array when it becomes too large. The update cost is amortized by propagating smaller arrays into larger arrays.</p>
</section>
<section id="b-tree" class="level2">
<h2>2.4 B-tree</h2>
<p>A B-tree is a balanced n-ary tree, comparable to balanced binary trees. Each node stores variable number of keys (and branches) up to <span class="math inline"><em>n</em></span> and <span class="math inline"><em>n</em> &gt; 2</span>.</p>
<section id="reducing-random-access-with-shorter-trees" class="level3">
<h3>Reducing random access with shorter trees</h3>
<p>A disk can only perform a limited number of IOs per second (IOPS), which is the limiting factor for tree lookups. Each level of the tree is a disk read in a lookup, and n-ary trees are shorter than binary trees for the same number of keys (<span class="math inline">log<sub><em>n</em></sub><em>N</em></span> vs. <span class="math inline">log<sub>2</sub><em>N</em></span>), thus n-ary trees are used for fewer disk reads per lookup.</p>
<p>How is the <span class="math inline"><em>n</em></span> chosen? There is a trade-off:</p>
<ul>
<li>Larger <span class="math inline"><em>n</em></span> means fewer disk reads per lookup (better latency and throughput).</li>
<li>Larger <span class="math inline"><em>n</em></span> means larger nodes, which are slower to update (discussed later).</li>
</ul>
</section>
<section id="io-in-the-unit-of-pages" class="level3">
<h3>IO in the unit of pages</h3>
<p>While you can read any number of bytes at any offset from a file, disks do not work that way. The basic unit of disk IO is not bytes, but sectors, which are 512-byte contiguous blocks on old HDDs.</p>
<p>However, disk sectors are not an application’s concern because regular file IOs do not interact directly with the disk. The OS caches/buffers disk reads/writes in the <em>page cache</em>, which consists of 4K-byte memory blocks called <em>pages</em>.</p>
<p>In any way, there is a minimum unit of IO. DBs can also define their own unit of IO (also called a page), which can be larger than an OS page.</p>
<p>The minimum IO unit implies that tree nodes should be allocated in multiples of the unit; a half used unit is half wasted IO. Another reason against small <span class="math inline"><em>n</em></span>!</p>
</section>
<section id="the-btree-variant" class="level3">
<h3>The B+tree variant</h3>
<p>In the context of databases, B-tree means a variant of B-tree called B+tree. In a B+tree, internal nodes do not store values, values exist only in leaf nodes. This leads to shorter tree because internal nodes have more space for branches.</p>
<p>B+tree as an in-memory data structure also makes sense because the minimum IO unit between RAM and CPU caches is 64 bytes (cache line). The performance benefit is not as great as on disk because not much can fit in 64 bytes.</p>
</section>
<section id="data-structure-space-overhead" class="level3">
<h3>Data structure space overhead</h3>
<p>Another reason why binary trees are impractical is the number of pointers; each key has at least 1 incoming pointer from the parent node, whereas in a B+tree, multiple keys in a leaf node share 1 incoming pointer.</p>
<p>Keys in a leaf node can also be packed in a compact format or compressed to further reduce the space.</p>
</section>
</section>
<section id="log-structured-storage" class="level2">
<h2>2.5 Log-structured storage</h2>
<section id="update-by-merge-amortize-cost" class="level3">
<h3>Update by merge: amortize cost</h3>
<p>The most common example of log-structured storage is log-structure merge tree (LSM-tree). Its main idea is neither log nor tree; it’s “merge” instead!</p>
<p>Let’s start with 2 files: a small file holding the recent updates, and a large file holding the rest of the data. Updates go to the small file first, but it cannot grow forever; it will be merged into the large file when it reaches a threshold.</p>
<pre><code>writes =&gt; | new updates | =&gt; | accumulated data |
               file 1               file 2</code></pre>
<p>Merging 2 sorted files results in a newer, larger file that replaces the old large file and shrinks the small file.</p>
<p>Merging is <span class="math inline"><em>O</em>(<em>N</em>)</span>, but can be done concurrently with readers and writers.</p>
</section>
<section id="reduce-write-amplification-with-multiple-levels" class="level3">
<h3>Reduce write amplification with multiple levels</h3>
<p>Buffering updates is better than rewriting the whole dataset every time. What if we extend this scheme to multiple levels?</p>
<pre><code>                 |level 1|
                    ||
                    \/
           |------level 2------|
                    ||
                    \/
|-----------------level 3-----------------|</code></pre>
<p>In the 2-level scheme, the large file is rewritten every time the small file reaches a threshold, the excess disk write is called <em>write amplification</em>, and it gets worse as the large file gets larger. If we use more levels, we can keep the 2nd level small by merging it into the 3rd level, similar to how we keep the 1st level small.</p>
<p>Intuitively, levels grow exponentially, and the power of two growth (merging similarly sized levels) results in the least write amplification. But there is a trade-off between write amplification and the number of levels (query performance).</p>
</section>
<section id="lsm-tree-indexes" class="level3">
<h3>LSM-tree indexes</h3>
<p>Each level contains indexing data structures, which could simply be a sorted array, since levels are never updated (except for the 1st level). But binary search is not much better than binary tree in terms of random access, so a sensible choice is to use B-tree inside a level, that’s the “tree” part of LSM-tree. Anyway, data structures are much simpler because of the lack of updates.</p>
<p>To better understand the idea of “merge”, you can try to apply it to hashtables, a.k.a. log-structured hashtables.</p>
</section>
<section id="lsm-tree-queries" class="level3">
<h3>LSM-tree queries</h3>
<p>Keys can be in any levels, so to query an LSM-tree, the results from each level are combined (n-way merge for range queries).</p>
<p>For point queries, Bloom filters can be used as an optimization to reduce the number of searched levels.</p>
<p>Since levels are never updated, there can be old versions of keys in older levels, and deleted keys are marked with a special flag in newer levels (called tombstones). Thus, newer levels have priority in queries.</p>
<p>The merge process naturally reclaims space from old or deleted keys. Thus, it’s also called <em>compaction</em>.</p>
</section>
<section id="real-world-lsm-tree-sstable-memtable-and-log" class="level3">
<h3>Real-world LSM-tree: SSTable, MemTable and log</h3>
<p>These are jargons about LSM-tree implementation details. You don’t need to know them to build one from principles, but they do solve some real problems.</p>
<p>Levels are split into multiple non-overlapping files called SSTables, rather than one large file, so that merging can be done gradually. This reduces the free space requirement when merging large levels, and the merging process is spread out over time.</p>
<p>The 1st level is updated directly, a log becomes a viable choice because the 1st level is bounded in size. This is the “log” part of the LSM-tree, an example of combining a log with other indexing data structures.</p>
<p>But even if the log is small, a proper indexing data structure is still needed. The log data is <em>duplicated</em> in an in-memory index called MemTable, which can be a B-tree, skiplist, or whatever. It’s a small, bounded amount of in-memory data, and has the added benefit of accelerating the read-the-recent-updates scenario.</p>
</section>
</section>
<section id="summary-of-indexing-data-structures" class="level2">
<h2>2.6 Summary of indexing data structures</h2>
<p>There are 2 options: B+tree and LSM-tree.</p>
<p>LSM-tree solves many of the challenges from the last chapter, such as how to update disk-based data structures and resue space. While these challenges remain for B+tree, which will be explored later.</p>
</section>
</section>
</body>
</html>
