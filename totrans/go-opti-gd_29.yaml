- en: 'Low-Level Network Optimizations: Socket Options That Matter'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 低级网络优化：重要的套接字选项
- en: 原文：[https://goperf.dev/02-networking/low-level-optimizations/](https://goperf.dev/02-networking/low-level-optimizations/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://goperf.dev/02-networking/low-level-optimizations/](https://goperf.dev/02-networking/low-level-optimizations/)
- en: Socket settings can limit both throughput and latency when the system is under
    load. The defaults are designed for safety and compatibility, not for any particular
    workload. In practice they often become the bottleneck before CPU or memory do.
    Go lets you reach the underlying file descriptors through `syscall`, so you can
    change key socket options without giving up its concurrency model or the standard
    library.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 当系统负载时，套接字设置可以限制吞吐量和延迟。默认设置是为了安全性和兼容性，而不是针对任何特定的负载。在实践中，它们通常在CPU或内存之前成为瓶颈。Go允许你通过`syscall`访问底层文件描述符，因此你可以更改关键的套接字选项，而无需放弃其并发模型或标准库。
- en: 'Disabling Nagle’s Algorithm: `TCP_NODELAY`'
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 禁用Nagle算法：`TCP_NODELAY`
- en: Nagle’s algorithm exists to make TCP more efficient. Every tiny packet you send
    carries headers that add up to a lot of wasted bandwidth if left unchecked. Nagle
    fixes that by holding back small writes until it can batch them into a full segment,
    cutting down on overhead and network chatter. That trade-off — bandwidth at the
    expense of latency — is usually fine, which is why it’s on by default. But if
    your application sends lots of small, time-critical messages, like a game server
    or a trading system, waiting even a few milliseconds for the buffer to fill can
    hurt.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Nagle算法的存在是为了使TCP更加高效。你发送的每一个微小的数据包都携带了头部信息，如果未经检查，这些头部信息会累积成大量的浪费带宽。Nagle通过将小的写入操作延迟，直到可以将它们批量合并成一个完整的数据段，从而减少了开销和网络噪音。这种权衡——以延迟为代价换取带宽——通常是可接受的，这也是为什么它默认开启的原因。但是，如果你的应用程序发送大量的小型、时间敏感的消息，比如游戏服务器或交易系统，等待缓冲区填满甚至几毫秒都可能造成伤害。
- en: '[PRE0]'
  id: totrans-5
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Nagle’s algorithm trades latency for efficiency by holding back small packets
    until there’s more data to send or an acknowledgment comes back. That delay is
    fine for bulk transfers but a problem for anything that needs fast, small messages.
    Setting `TCP_NODELAY` turns it off so data goes out immediately. This is critical
    for workloads like gaming, trading, real-time video, and other interactive systems
    where you can’t afford to wait.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Nagle算法通过延迟小的数据包，直到有更多数据要发送或收到确认，来以延迟换取效率。这种延迟对于大量传输是可接受的，但对于需要快速、小消息的任何事物来说都是问题。设置`TCP_NODELAY`可以关闭它，使数据立即发送。这对于游戏、交易、实时视频和其他交互式系统中的工作负载至关重要，在这些系统中，等待是不可接受的。
- en: 'In Go, you can turn off Nagle’s algorithm with `TCP_NODELAY`:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在Go中，你可以使用`TCP_NODELAY`关闭Nagle算法：
- en: '[PRE1]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: SO_REUSEPORT for Scalability
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SO_REUSEPORT以提高可伸缩性
- en: '`SO_REUSEPORT` lets multiple sockets on the same machine bind to the same port
    and accept connections at the same time. Instead of funneling all incoming connections
    through one socket, the kernel distributes new connections across all of them,
    so each socket gets its own share of the load. This is useful when running several
    worker processes or threads that each accept connections independently, because
    it removes the need for user-space coordination and avoids contention on a single
    accept queue. It also makes better use of multiple CPU cores by letting each process
    or thread handle its own queue of connections directly.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '`SO_REUSEPORT`允许同一台机器上的多个套接字绑定到同一个端口，并同时接受连接。而不是将所有传入的连接通过一个套接字引导，内核将新的连接分配到所有套接字中，这样每个套接字都获得自己的一份负载。这在运行多个独立接受连接的工作进程或线程时很有用，因为它消除了用户空间协调的需求，避免了单个接受队列上的竞争。它还通过让每个进程或线程直接处理自己的连接队列，更好地利用了多个CPU核心。'
- en: 'Typical scenarios for `SO_REUSEPORT`:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '`SO_REUSEPORT`的典型场景：'
- en: High-performance web servers where multiple worker processes call bind() on
    the same port. The kernel distributes incoming connection requests across the
    accept() queues of all bound sockets, typically using a hash of the 4-tuple or
    round-robin, eliminating the need for a user-space dispatcher.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高性能的Web服务器，其中多个工作进程在同一个端口上调用bind()。内核将传入的连接请求分配到所有已绑定套接字的accept()队列中，通常使用4元组的哈希或轮询，消除了用户空间调度器的需求。
- en: Multi-threaded or multi-process servers that accept connections in parallel.
    When combined with Go’s GOMAXPROCS, each thread or process can call accept() independently
    on its own file descriptor, avoiding lock contention on a single queue and fully
    utilizing all CPU cores.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多线程或多进程服务器可以并行接受连接。当与Go的GOMAXPROCS结合使用时，每个线程或进程都可以独立在其自己的文件描述符上调用accept()，避免了单个队列上的锁竞争，并充分利用了所有CPU核心。
- en: Fault-tolerant designs where multiple processes bind to the same port to increase
    resilience. If one process exits or is killed, the others continue to service
    connections without interruption, because each maintains its own independent accept()
    queue.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有多个进程绑定到同一端口以增加弹性的容错设计。如果一个进程退出或被杀死，其他进程将继续服务连接而不会中断，因为每个进程都维护自己的独立 accept()
    队列。
- en: In Go, SO_REUSEPORT isn’t exposed in the standard library, but it can be set
    via syscall when creating the socket. This is done with `syscall.SetsockoptInt`,
    which operates on the socket’s file descriptor. You pass the protocol level (`SOL_SOCKET`),
    the option (`SO_REUSEPORT`), and the value (`1` to enable). This must happen before
    calling `bind()`, so it’s typically placed in the Control callback of a `net.ListenConfig`,
    which runs before the socket is bound.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Go 中，标准库中没有暴露 SO_REUSEPORT，但在创建套接字时可以通过 syscall 设置。这是通过 `syscall.SetsockoptInt`
    实现的，它操作套接字的文件描述符。您传递协议级别（`SOL_SOCKET`）、选项（`SO_REUSEPORT`）和值（`1` 以启用）。这必须在调用 `bind()`
    之前完成，因此通常将其放置在 `net.ListenConfig` 的控制回调中，该回调在套接字绑定之前运行。
- en: '[PRE2]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Tuning Socket Buffer Sizes: `SO_RCVBUF` and `SO_SNDBUF`'
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调整套接字缓冲区大小：`SO_RCVBUF` 用于接收，`SO_SNDBUF` 用于发送
- en: Socket buffer sizes — `SO_RCVBUF` for receiving and `SO_SNDBUF` for sending
    — directly affect throughput and the number of system calls. These buffers hold
    incoming and outgoing data in the kernel, smoothing out bursts and letting the
    application read and write at its own pace.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 套接字缓冲区大小——`SO_RCVBUF` 用于接收，`SO_SNDBUF` 用于发送——直接影响吞吐量和系统调用次数。这些缓冲区在内核中保存传入和传出的数据，平滑突增，并允许应用程序以自己的节奏进行读写。
- en: When buffers are too small, they fill up quickly and the kernel keeps waking
    the application to read or write data. That extra churn increases CPU usage and
    limits how much data you can push through the connection. If the buffers are too
    large, they just waste memory and let packets pile up in the queue longer than
    needed, which adds latency and hurts responsiveness when the system is busy. The
    point is to make the buffers big enough to keep the link busy but not so big that
    they turn into a backlog.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当缓冲区过小时，它们会很快填满，内核会不断唤醒应用程序以读取或写入数据。这种额外的轮换增加了 CPU 使用率，并限制了您可以通过连接推送的数据量。如果缓冲区过大，它们只是浪费内存，并让数据包在队列中堆积的时间比所需的时间更长，这会增加延迟，并在系统繁忙时损害响应性。关键是使缓冲区足够大，以保持链路忙碌，但不要大到它们变成回压。
- en: 'This is how you can adjust the buffer:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这是调整缓冲区的方法：
- en: '[PRE3]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Typical default sizes for `SO_RCVBUF` and `SO_SNDBUF` on Linux systems range
    from 128 KB to 256 KB, depending on the kernel version and system configuration.
    These defaults are chosen to provide a balance between minimizing latency and
    avoiding excessive memory usage. For high-bandwidth applications or connections
    with high latency (e.g., long-haul TCP connections), increasing these buffer sizes—sometimes
    to several megabytes—can significantly improve throughput by allowing more in-flight
    data without blocking.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Linux 系统上，`SO_RCVBUF` 和 `SO_SNDBUF` 的典型默认大小从 128 KB 到 256 KB 不等，这取决于内核版本和系统配置。这些默认值的选择是为了在最小化延迟和避免过度使用内存之间取得平衡。对于高带宽应用或具有高延迟的连接（例如，长途
    TCP 连接），增加这些缓冲区大小——有时达到几个兆字节——可以通过允许更多的飞行数据而不阻塞来显著提高吞吐量。
- en: The right buffer size depends on the RTT, the link bandwidth, and the size of
    the messages your application sends. A common rule of thumb is to match the buffer
    to the bandwidth–delay product (BDP), which is just `bandwidth × RTT`. That way,
    the connection can keep the pipe full without stalling.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 正确的缓冲区大小取决于 RTT、链路带宽以及应用程序发送的消息大小。一个常见的经验法则是将缓冲区匹配到带宽-延迟积（BDP），即 `带宽 × RTT`。这样，连接可以保持管道满载而不停滞。
- en: To figure out the right buffer sizes, you need to test under realistic load.
    Tools like iperf3 are good for measuring raw throughput, and app-specific profiling
    (pprof, netstat, custom metrics) helps spot where things actually get stuck. Increase
    the buffer sizes step by step during load tests and watch where the gains level
    off — that’s usually a good place to stop.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确定正确的缓冲区大小，您需要在实际负载下进行测试。iperf3 等工具适用于测量原始吞吐量，而应用程序特定的分析（pprof、netstat、自定义指标）有助于发现问题真正卡住的地方。在负载测试期间逐步增加缓冲区大小，并观察收益何时趋于平稳——这通常是一个停止的好地方。
- en: Warning
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: The optimal settings depend on how your system actually runs, so you need to
    measure them under load. Guessing or copying values from elsewhere usually doesn’t
    work — you have to test and adjust until it performs the way you need.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳设置取决于您的系统实际运行的方式，因此您需要在负载下进行测量。猜测或从其他地方复制值通常不起作用——您必须测试和调整，直到它以您需要的方式运行。
- en: TCP Keepalives for Reliability
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TCP保活用于可靠性
- en: TCP keepalive probes detect dead peer connections, freeing resources promptly.
    Keepalives prevent hanging connections in long-lived services.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: TCP保活探测检测死对等连接，及时释放资源。保活防止在长生存期服务中挂起连接。
- en: 'Default keepalive settings on most platforms are conservative—idle time of
    2 hours, 10 probes at 75-second intervals—intended for general-purpose environments.
    For server-side applications requiring faster failure detection (e.g., reverse
    proxies, microservices over unreliable links), significantly more aggressive settings
    are common: 30 seconds idle, 3 probes at 10-second intervals.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数平台上的默认保活设置都很保守——空闲时间为2小时，每75秒进行10次探测——适用于通用环境。对于需要更快故障检测的服务器端应用程序（例如，反向代理、通过不可靠链路的微服务），通常更激进的设置更为常见：30秒空闲，每10秒进行3次探测。
- en: However, aggressive tuning increases traffic and risks false positives on congested
    links. A recommended approach is to balance early detection and network conditions.
    Typical tuned values—idle 30–60s, interval 10–15s, probes 3–5—are not specified
    by any RFC or standard, but come from operational practice and vendor guidance.
    For example, PostgreSQL, Kubernetes, and AWS all recommend values in this range
    to align with cloud load balancer timeouts and service SLAs. These numbers are
    derived from empirical experience to minimize both false positives and long detection
    delays, as discussed in sources like PostgreSQL documentation, AWS networking
    blogs, and Kubernetes issue discussions.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，过于激进的调整会增加流量并增加在拥塞链路上的误报风险。一个推荐的方法是在早期检测和网络条件之间取得平衡。典型的调整值——空闲30-60秒，间隔10-15秒，探测3-5次——并非由任何RFC或标准指定，而是来自操作实践和供应商指南。例如，PostgreSQL、Kubernetes和AWS都推荐这个范围内的值，以与云负载均衡器超时和服务SLA保持一致。这些数字是从经验中得出的，旨在最大限度地减少误报和长检测延迟，正如在PostgreSQL文档、AWS网络博客和Kubernetes问题讨论中讨论的那样。
- en: These can be adjusted system-wide using `sysctl` or per-connection on some platforms
    (e.g., Linux) via platform-specific socket options or libraries that expose `TCP_KEEPIDLE`,
    `TCP_KEEPINTVL`, and `TCP_KEEPCNT`. In Go, `SetKeepAlivePeriod` only controls
    the idle time; deeper tuning may require cgo or raw syscalls.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这些可以通过`sysctl`在系统范围内进行调整，或在某些平台（例如Linux）上通过平台特定的套接字选项或公开`TCP_KEEPIDLE`、`TCP_KEEPINTVL`和`TCP_KEEPCNT`的库按连接进行调整。在Go中，`SetKeepAlivePeriod`仅控制空闲时间；更深入的调整可能需要cgo或原始系统调用。
- en: 'In Go, enabling and tuning keepalives:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在Go中启用和调整保活：
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The `SetKeepAlivePeriod` method in Go controls only the idle time before the
    first keepalive probe is sent. On Linux, this corresponds to the `TCP_KEEPIDLE`
    parameter; on macOS and BSD, it maps to `TCP_KEEPALIVE`. However, it does not
    affect the interval between subsequent probes (`TCP_KEEPINTVL`) or the number
    of allowed failed probes (`TCP_KEEPCNT`). These two must be set separately using
    raw syscalls or cgo bindings if finer-grained control is required. Without tuning
    `KEEPINTVL` and `KEEPCNT`, a dead connection may take several minutes to detect,
    even if `SetKeepAlivePeriod` is low.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Go中的`SetKeepAlivePeriod`方法仅控制发送第一个保活探测之前的空闲时间。在Linux上，这对应于`TCP_KEEPIDLE`参数；在macOS和BSD上，它映射到`TCP_KEEPALIVE`。然而，它不影响后续探测之间的间隔（`TCP_KEEPINTVL`）或允许失败的探测次数（`TCP_KEEPCNT`）。如果需要更细粒度的控制，这两个参数必须单独使用原始系统调用或cgo绑定来设置。如果没有调整`KEEPINTVL`和`KEEPCNT`，即使`SetKeepAlivePeriod`设置得较低，死连接可能需要几分钟才能检测到。
- en: Warning
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: Do not change the default values without testing them properly. The right settings
    depend entirely on your workload, the network path, and how the application behaves
    under real conditions. Different configurations can produce very different results,
    and what works elsewhere might hurt here. You’ll find plenty of blog posts and
    advice with suggested numbers, but those are just starting points. The only reliable
    way to figure out what works in your environment is to test and measure it yourself.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在没有适当测试的情况下不要更改默认值。正确的设置完全取决于您的负载、网络路径以及应用程序在真实条件下的行为。不同的配置可以产生非常不同的结果，而在其他地方有效的东西在这里可能会造成伤害。您会发现大量带有建议数字的博客文章和建议，但这些只是起点。唯一可靠的方法是自行测试和测量。
- en: 'Connection Backlog: SOMAXCONN'
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 连接队列：SOMAXCONN
- en: The connection backlog (`SOMAXCONN`) defines how many pending connections can
    queue up for acceptance. When the backlog queue fills up, additional connection
    attempts are refused by the kernel (usually resulting in `ECONNREFUSED`(1) or
    dropped SYN packets) until space becomes available, which can cause clients to
    see connection errors under heavy load.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 连接队列（`SOMAXCONN`）定义了可以排队等待接受的挂起连接的数量。当队列填满时，内核会拒绝额外的连接尝试（通常导致 `ECONNREFUSED`(1)
    或丢弃 SYN 数据包），直到空间变得可用，这可能导致在重负载下客户端看到连接错误。
- en: A `connect()` on a stream socket found no one listening on the remote address.
    See Linux [man pages](https://man7.org/linux/man-pages/man2/connect.2.html) for
    more details.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在流套接字上的 `connect()` 调用未在远程地址上找到监听者。有关更多详细信息，请参阅 Linux [手册页](https://man7.org/linux/man-pages/man2/connect.2.html)。
- en: The default value of `SOMAXCONN` on Linux is typically 128 or 4096, depending
    on the kernel version and distribution. These defaults are chosen to strike a
    balance between memory use and handling normal connection rates, but they may
    be too low for high-traffic servers or services experiencing connection bursts.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Linux 上 `SOMAXCONN` 的默认值通常是 128 或 4096，具体取决于内核版本和发行版。这些默认值是为了在内存使用和处理正常连接速率之间取得平衡而选择的，但它们对于高流量服务器或经历连接突发的服务可能太低。
- en: A bigger backlog helps when the server gets hit with a burst of connections
    — like after a failover or during peak — so clients don’t get dropped right away.
    Raising `SOMAXCONN` lets the kernel queue more connections while the app catches
    up and keeps the service reachable under load.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 当服务器受到连接突发冲击时（例如在故障转移后或高峰时段），更大的队列有助于避免客户端立即被丢弃。提高 `SOMAXCONN` 允许内核在应用程序赶上并保持服务在负载下可达的同时，排队更多的连接。
- en: Changing `SOMAXCONN` is a system-level setting. On Linux you usually adjust
    it through kernel parameters.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 修改 `SOMAXCONN` 是一个系统级设置。在 Linux 上，您通常通过内核参数进行调整。
- en: '[PRE5]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Safely Wrapping Syscalls in Go
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Go 中安全地包装系统调用
- en: Working with socket options through syscalls means dealing directly with file
    descriptors. These calls need to happen before the socket is bound or used, which
    makes the timing important and easy to get wrong. If you set an option too late,
    the kernel ignores it, or worse, you get hard-to-reproduce bugs. Since you’re
    bypassing the Go runtime, you’re also responsible for checking errors and making
    sure the file descriptor stays in a valid state. `syscall.RawConn` exists to help
    with this — it gives you a controlled hook to run your code against the socket
    at exactly the right point during setup.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 通过系统调用与套接字选项一起工作意味着直接处理文件描述符。这些调用需要在套接字绑定或使用之前发生，这使得时机很重要，并且容易出错。如果您设置选项太晚，内核会忽略它，或者更糟糕的是，您会得到难以复现的错误。由于您绕过了
    Go 运行时，您也负责检查错误并确保文件描述符保持有效状态。`syscall.RawConn` 存在就是为了帮助您——它提供了一个受控的钩子，在设置过程中可以在套接字上精确地运行您的代码。
- en: '[PRE6]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The socket options are set during creation, at the right time, and in one place.
    That keeps them from being forgotten or misapplied later.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 套接字选项在创建时设置，在正确的时间，在一个地方。这避免了它们被遗忘或后来误用。
- en: Real-World Considerations
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实际应用考虑
- en: Tuning sockets is always a trade-off — lower latency, higher throughput, better
    reliability, and reasonable resource use don’t usually come together for free.
    You need to understand your workload, change one thing at a time, and test it
    under real load. Without monitoring and instrumentation, you’re just guessing,
    so measure everything and make sure the changes actually help.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 调整套接字始终是一种权衡——较低的延迟、较高的吞吐量、更好的可靠性和合理的资源使用通常不会免费地同时出现。您需要了解自己的工作负载，一次改变一件事，并在实际负载下进行测试。没有监控和仪表，您只是在猜测，因此要测量一切，并确保更改确实有帮助。
