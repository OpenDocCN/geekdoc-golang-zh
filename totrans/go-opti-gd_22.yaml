- en: Efficient Use of net/http, net.Conn, and UDP in High-Traffic Go Services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://goperf.dev/02-networking/efficient-net-use/](https://goperf.dev/02-networking/efficient-net-use/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: When we first start building high-traffic services in Go, we often lean heavily
    on `net/http`. It’s stable, ergonomic, and remarkably capable for 80% of use cases.
    But as soon as traffic spikes or latency budgets shrink, the cracks begin to show.
  prefs: []
  type: TYPE_NORMAL
- en: It’s not that `net/http` is broken—it’s just that the defaults are tuned for
    convenience, not for performance under stress. And as we scale backend services
    to handle millions of requests per second, understanding what happens underneath
    the abstraction becomes the difference between meeting SLOs and fire-fighting
    in production.
  prefs: []
  type: TYPE_NORMAL
- en: This article is a walkthrough of how to make networked Go services truly efficient—what
    works, what breaks, and how to go beyond idiomatic usage. We’ll start with `net/http`,
    drop into raw `net.Conn`, and finish with real-world patterns for handling UDP
    in latency-sensitive systems.
  prefs: []
  type: TYPE_NORMAL
- en: The Hidden Complexity Behind a Simple HTTP Call
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s begin where most Go developers do: a simple `http.Client`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This looks harmless. It gets the job done, and in most local tests, it performs
    reasonably well. But in production, at scale, this innocent-looking code can trigger
    a surprising range of issues: leaked connections, memory spikes, blocked goroutines,
    and mysterious latency cliffs.'
  prefs: []
  type: TYPE_NORMAL
- en: One of the most common issues is forgetting to fully read `resp.Body` before
    closing it. [Go’s HTTP client won’t reuse connections unless the body is drained](https://github.com/google/go-github/pull/317).
    And under load, that means you're constantly opening new TCP connections—slamming
    the kernel with ephemeral ports, exhausting file descriptors, and triggering throttling.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the safe pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Transport Tuning: When Defaults Aren’t Enough'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It’s easy to overlook how much global state hides behind `http.DefaultTransport`.
    If you spin up multiple `http.Client` instances across your app without customizing
    the transport, you're probably reusing a shared global pool without realizing
    it.
  prefs: []
  type: TYPE_NORMAL
- en: 'This leads to unpredictable behavior under load: idle connections get evicted
    too quickly, or keep-alive connections linger longer than they should. The fix?
    Build a tuned `Transport` that matches your concurrency profile.'
  prefs: []
  type: TYPE_NORMAL
- en: Custom `http.Transport` Fields to Tune
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'All the following settings are part of the `http.Transport` struct:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: More Advanced Optimization Tricks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'These are all tied to key settings in the `http.Transport`, `http.Client`,
    and `http.Server` structs, or custom wrappers built on top of them:'
  prefs: []
  type: TYPE_NORMAL
- en: Set `ExpectContinueTimeout` Carefully
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If our clients send large POST requests and the server doesn’t support `100-continue`
    properly, we can reduce or eliminate this delay:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Constrain `MaxConnsPerHost`
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Go’s default HTTP client will open an unbounded number of connections to a host.
    That’s fine until one of your downstreams can’t handle it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This prevents stampedes during spikes and avoids exhausting resources on your
    backend services.
  prefs: []
  type: TYPE_NORMAL
- en: Use Small `http.Client.Timeout`
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A common mistake is setting a very high timeout (e.g., 30s) for safety. But
    long timeouts hold onto goroutines, buffers, and sockets under pressure. Prefer
    tighter control:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Instead of relying on big timeouts, use retries with backoff (e.g., with go-retryablehttp)
    to improve resiliency under partial failure.
  prefs: []
  type: TYPE_NORMAL
- en: Explicitly Set `ReadBufferSize` and `WriteBufferSize` in `http.Server`
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Go''s `http.Server` does not expose `ReadBufferSize` and `WriteBufferSize`
    directly, but when you need to reduce GC pressure and improve syscall efficiency
    under load, you can pre-size the buffers in custom `Conn` wrappers. 4KB–8KB is
    a balanced value for most workloads: it''s large enough to handle small headers
    and bodies efficiently without wasting memory. For example, 4KB covers almost
    all typical HTTP headers and small JSON payloads.'
  prefs: []
  type: TYPE_NORMAL
- en: You can implement this using `bufio.NewReaderSize` and `NewWriterSize` in a
    wrapped connection that plugs into a custom `net.Listener` and `http.Server.ConnContext`.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you''re using `fasthttp`, you can configure buffer sizes explicitly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This avoids dynamic allocations on each request and leads to more predictable
    memory usage and cache locality under high throughput.
  prefs: []
  type: TYPE_NORMAL
- en: Use `bufio.Reader.Peek()` for Efficient Framing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When implementing a framed protocol over TCP, like length-prefixed binary messages,
    naively calling `Read()` in a loop can lead to fragmented reads and unnecessary
    syscalls. This adds up, especially under load. Using `Peek()` gives you a look
    into the buffered data without advancing the read position, making it easier to
    detect message boundaries without triggering extra reads. It’s a practical technique
    in streaming systems or multiplexed connections where tight control over framing
    is critical.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Force Fresh DNS Lookups with Custom Dialers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Go’s built-in DNS caching lasts for the lifetime of the process. In dynamic
    environments, like Kubernetes, this can become a problem when service IPs change
    but clients keep reusing stale ones. To avoid this, you can force fresh DNS lookups
    by creating a new net.Dialer per request or rotating the HTTP client periodically.
  prefs: []
  type: TYPE_NORMAL
- en: 'But you can bypass Go’s internal DNS cache when needed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This ensures a fresh DNS lookup per request. While this adds minor overhead,
    it's necessary in failover-sensitive environments.
  prefs: []
  type: TYPE_NORMAL
- en: Use `sync.Pool` for Readers/Writers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most people use `sync.Pool` to reuse `[]byte` buffers, but for services that
    process many requests per second, allocating `bufio.Reader` and `bufio.Writer`
    objects per connection adds up. These objects also maintain their own buffers,
    so recycling them reduces pressure on both heap allocations and garbage collection.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This practice significantly reduces allocation churn and improves latency consistency,
    especially in systems processing thousands of connections concurrently.
  prefs: []
  type: TYPE_NORMAL
- en: Don’t Share `http.Client` Across Multiple Hosts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While it might seem efficient to reuse a single `http.Client`, each target host
    maintains its own internal connection pool within the underlying `http.Transport`.
    If you use the same client for multiple base URLs, you end up mixing connection
    reuse and causing head-of-line blocking across unrelated services. Worse, DNS
    caching and socket exhaustion become harder to track.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, create a dedicated `http.Client` for each upstream service you interact
    with. This improves connection reuse, avoids cross-talk between services, and
    usually makes behavior more predictable, especially in environments like service
    meshes or when dealing with multiple external APIs.
  prefs: []
  type: TYPE_NORMAL
- en: Use `ConnContext` and `ConnState` Hooks for Debugging
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: These hooks are useful for tracking the lifecycle of each connection—especially
    when debugging issues like memory leaks, stuck connections, or resource exhaustion
    in production. The `ConnState` callback gives visibility into transitions such
    as `StateNew`, `StateActive`, `StateIdle`, and `StateHijacked`, allowing you to
    log, trace, or apply custom handling per connection state.
  prefs: []
  type: TYPE_NORMAL
- en: By monitoring these events, you can detect when connections hang, fail to close,
    or unexpectedly idle out. It also helps when correlating behavior with client
    IPs or network zones.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Dropping the Abstraction: When to Use `net.Conn`'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we get closer to the limits of what the Go standard library can offer, it’s
    worth knowing that there are high-performance alternatives built specifically
    for event-driven, low-latency workloads. Projects like [`cloudwego/netpoll`](https://github.com/cloudwego/netpoll)
    and [`tidwall/evio`](https://github.com/tidwall/evio) offer powerful tools for
    maximizing performance beyond what’s achievable with `net.Conn` alone.
  prefs: []
  type: TYPE_NORMAL
- en: '**[`cloudwego/netpoll`](https://github.com/cloudwego/netpoll)** is an epoll-based
    network library designed for building massive concurrent network services with
    minimal GC overhead. It uses event-based I/O to eliminate goroutine-per-connection
    costs, ideal for scenarios like RPC proxies, internal service meshes, or high-frequency
    messaging systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**[`tidwall/evio`](https://github.com/tidwall/evio)** provides a fast, non-blocking
    event loop for Go based on the [reactor pattern](https://en.wikipedia.org/wiki/Reactor_pattern).
    It’s well-suited for protocols where latency matters more than per-connection
    state complexity, such as custom TCP, UDP protocols, or lightweight gateways.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you're building systems where throughput or connection count exceeds hundreds
    of thousands, or where tail latency is critical, it's worth exploring these libraries.
    They come with trade-offs—most notably, less standardization and more manual lifecycle
    management—but in return, they give you fine-grained control over performance-critical
    paths.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, even a tuned HTTP stack isn't enough.
  prefs: []
  type: TYPE_NORMAL
- en: In cases like internal binary protocols or services dealing with hundreds of
    thousands of requests per second, we may find we're paying for HTTP semantics
    we don't use. Dropping to `net.Conn` gives us full control—no pooling surprises,
    no hidden keep-alives, just a raw socket.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This lets us take over the connection lifecycle, buffering, and concurrency
    fully. It also opens up opportunities to reduce GC impact via buffer reuse:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Enabling TCP_NODELAY is useful in latency-sensitive systems:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Beyond TCP: Why UDP Matters'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'TCP could be too heavy for workloads like log firehose ingestion, telemetry
    beacons, or heartbeat messages. We can turn to UDP for low-latency, connectionless
    data delivery:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This skips handshakes and reuses the socket efficiently. But remember—UDP offers
    no ordering, reliability, or built-in session tracking. It works best in high-volume,
    low-consequence pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the Right Tool
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our networking strategy should reflect traffic shape and protocol expectations:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Scenario | Preferred Tool |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| REST/gRPC, general APIs | `net/http` |'
  prefs: []
  type: TYPE_TB
- en: '| HTTP under load | Tuned `http.Transport` |'
  prefs: []
  type: TYPE_TB
- en: '| Custom TCP protocol | `net.Conn` |'
  prefs: []
  type: TYPE_TB
- en: '| Framed binary data | `net.Conn` + buffer mgmt |'
  prefs: []
  type: TYPE_TB
- en: '| Fire-and-forget telemetry | `UDPConn` |'
  prefs: []
  type: TYPE_TB
- en: '| Latency-sensitive game updates | `UDP` |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: At scale, network performance is never just about the network. It's also about
    memory pressure, context lifecycles, kernel behavior, and socket hygiene. We can
    go far with the Go standard library, but when systems push back, we need to push
    deeper.
  prefs: []
  type: TYPE_NORMAL
- en: The good news? Go gives us the tools. We just need to use them wisely.
  prefs: []
  type: TYPE_NORMAL
- en: If you're experimenting with framed protocols, zero-copy parsing, or custom
    benchmarking setups, there's a lot more to explore. Let's keep going.
  prefs: []
  type: TYPE_NORMAL
