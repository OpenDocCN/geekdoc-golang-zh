- en: 'Low-Level Network Optimizations: Socket Options That Matter¶'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 低级网络优化：重要的套接字选项
- en: 原文：[https://goperf.dev/02-networking/low-level-optimizations/](https://goperf.dev/02-networking/low-level-optimizations/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://goperf.dev/02-networking/low-level-optimizations/](https://goperf.dev/02-networking/low-level-optimizations/)
- en: Socket settings can limit both throughput and latency when the system is under
    load. The defaults are designed for safety and compatibility, not for any particular
    workload. In practice they often become the bottleneck before CPU or memory do.
    Go lets you reach the underlying file descriptors through `syscall`, so you can
    change key socket options without giving up its concurrency model or the standard
    library.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 当系统负载时，套接字设置可以限制吞吐量和延迟。默认设置是为了安全性和兼容性，而不是针对任何特定的工作负载。在实践中，它们通常在 CPU 或内存成为瓶颈之前成为瓶颈。Go
    允许你通过 `syscall` 访问底层文件描述符，因此你可以更改关键的套接字选项，而无需放弃其并发模型或标准库。
- en: 'Disabling Nagle’s Algorithm: `TCP_NODELAY`[¶](#disabling-nagles-algorithm-tcp_nodelay
    "Permanent link")'
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 禁用 Nagle 算法：`TCP_NODELAY`
- en: Nagle’s algorithm exists to make TCP more efficient. Every tiny packet you send
    carries headers that add up to a lot of wasted bandwidth if left unchecked. Nagle
    fixes that by holding back small writes until it can batch them into a full segment,
    cutting down on overhead and network chatter. That trade-off — bandwidth at the
    expense of latency — is usually fine, which is why it’s on by default. But if
    your application sends lots of small, time-critical messages, like a game server
    or a trading system, waiting even a few milliseconds for the buffer to fill can
    hurt.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Nagle 算法存在是为了使 TCP 更高效。你发送的每个小数据包都携带了大量的头部信息，如果未经检查，这些头部信息会浪费很多带宽。Nagle 通过延迟小写入，直到可以将它们批量成完整的数据段，从而减少了开销和网络嘈杂。这种权衡——以延迟为代价换取带宽——通常是可接受的，这也是为什么它是默认开启的。但是，如果你的应用程序发送大量的小、时间敏感的消息，如游戏服务器或交易系统，等待缓冲区填满甚至几毫秒都可能造成伤害。
- en: '[PRE0]'
  id: totrans-5
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Nagle’s algorithm trades latency for efficiency by holding back small packets
    until there’s more data to send or an acknowledgment comes back. That delay is
    fine for bulk transfers but a problem for anything that needs fast, small messages.
    Setting `TCP_NODELAY` turns it off so data goes out immediately. This is critical
    for workloads like gaming, trading, real-time video, and other interactive systems
    where you can’t afford to wait.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Nagle 算法通过延迟发送小数据包，直到有更多数据要发送或收到确认，以换取效率。这种延迟对于大量传输来说是可接受的，但对于需要快速、小消息的任何应用来说都是一个问题。设置
    `TCP_NODELAY` 可以关闭它，使数据立即发送出去。这对于游戏、交易、实时视频和其他交互式系统来说至关重要，在这些系统中，等待是不可接受的。
- en: 'In Go, you can turn off Nagle’s algorithm with `TCP_NODELAY`:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Go 中，你可以使用 `TCP_NODELAY` 关闭 Nagle 算法：
- en: '[PRE1]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: SO_REUSEPORT for Scalability[¶](#so_reuseport-for-scalability "Permanent link")
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '`SO_REUSEPORT` 用于可伸缩性'
- en: '`SO_REUSEPORT` lets multiple sockets on the same machine bind to the same port
    and accept connections at the same time. Instead of funneling all incoming connections
    through one socket, the kernel distributes new connections across all of them,
    so each socket gets its own share of the load. This is useful when running several
    worker processes or threads that each accept connections independently, because
    it removes the need for user-space coordination and avoids contention on a single
    accept queue. It also makes better use of multiple CPU cores by letting each process
    or thread handle its own queue of connections directly.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '`SO_REUSEPORT` 允许同一台机器上的多个套接字绑定到相同的端口，并同时接受连接。内核将新连接分配到所有套接字上，而不是将所有传入连接都通过一个套接字，这样每个套接字都能获得自己的一份负载。这在运行多个独立接受连接的工作进程或线程时非常有用，因为它消除了用户空间协调的需求，避免了单个接受队列上的竞争。它还通过让每个进程或线程直接处理自己的连接队列，更好地利用了多个
    CPU 核心。'
- en: 'Typical scenarios for `SO_REUSEPORT`:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '`SO_REUSEPORT` 的典型场景：'
- en: High-performance web servers where multiple worker processes call bind() on
    the same port. The kernel distributes incoming connection requests across the
    accept() queues of all bound sockets, typically using a hash of the 4-tuple or
    round-robin, eliminating the need for a user-space dispatcher.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高性能的 Web 服务器，其中多个工作进程在相同的端口上调用 bind()。内核将传入连接请求分配到所有已绑定套接字的 accept() 队列中，通常使用
    4 元组的哈希或轮询，消除了用户空间调度器的需求。
- en: Multi-threaded or multi-process servers that accept connections in parallel.
    When combined with Go’s GOMAXPROCS, each thread or process can call accept() independently
    on its own file descriptor, avoiding lock contention on a single queue and fully
    utilizing all CPU cores.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多线程或多进程服务器可以并行接受连接。当与 Go 的 GOMAXPROCS 结合使用时，每个线程或进程都可以独立在其自己的文件描述符上调用 accept()，从而避免了单个队列上的锁定竞争，并充分利用了所有
    CPU 核心。
- en: Fault-tolerant designs where multiple processes bind to the same port to increase
    resilience. If one process exits or is killed, the others continue to service
    connections without interruption, because each maintains its own independent accept()
    queue.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有多个进程绑定到同一端口以提高弹性的容错设计。如果一个进程退出或被杀死，其他进程将继续服务连接而不会中断，因为每个进程都维护自己的独立 accept()
    队列。
- en: In Go, SO_REUSEPORT isn’t exposed in the standard library, but it can be set
    via syscall when creating the socket. This is done with `syscall.SetsockoptInt`,
    which operates on the socket’s file descriptor. You pass the protocol level (`SOL_SOCKET`),
    the option (`SO_REUSEPORT`), and the value (`1` to enable). This must happen before
    calling `bind()`, so it’s typically placed in the Control callback of a `net.ListenConfig`,
    which runs before the socket is bound.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Go 中，`SO_REUSEPORT` 并未在标准库中公开，但在创建套接字时可以通过 syscall 设置。这是通过 `syscall.SetsockoptInt`
    实现的，它操作套接字的文件描述符。您传递协议级别（`SOL_SOCKET`）、选项（`SO_REUSEPORT`）和值（`1` 以启用）。这必须在调用 `bind()`
    之前完成，因此通常放置在 `net.ListenConfig` 的控制回调中，该回调在套接字绑定之前运行。
- en: '[PRE2]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Tuning Socket Buffer Sizes: `SO_RCVBUF` and `SO_SNDBUF`[¶](#tuning-socket-buffer-sizes-so_rcvbuf-and-so_sndbuf
    "Permanent link")'
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调整套接字缓冲区大小：`SO_RCVBUF` 和 `SO_SNDBUF`[¶](#tuning-socket-buffer-sizes-so_rcvbuf-and-so_sndbuf
    "永久链接")
- en: Socket buffer sizes — `SO_RCVBUF` for receiving and `SO_SNDBUF` for sending
    — directly affect throughput and the number of system calls. These buffers hold
    incoming and outgoing data in the kernel, smoothing out bursts and letting the
    application read and write at its own pace.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 套接字缓冲区大小——`SO_RCVBUF` 用于接收和 `SO_SNDBUF` 用于发送——直接影响吞吐量和系统调用次数。这些缓冲区在内核中持有传入和传出数据，平滑突增，并允许应用程序以自己的速度读取和写入。
- en: When buffers are too small, they fill up quickly and the kernel keeps waking
    the application to read or write data. That extra churn increases CPU usage and
    limits how much data you can push through the connection. If the buffers are too
    large, they just waste memory and let packets pile up in the queue longer than
    needed, which adds latency and hurts responsiveness when the system is busy. The
    point is to make the buffers big enough to keep the link busy but not so big that
    they turn into a backlog.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当缓冲区过小时，它们会很快填满，内核会不断唤醒应用程序来读取或写入数据。这种额外的处理增加了 CPU 使用率，并限制了可以通过连接推送的数据量。如果缓冲区过大，它们只是浪费内存，并让数据包在队列中堆积的时间比所需的时间更长，这会增加延迟并损害系统繁忙时的响应性。关键是要使缓冲区足够大，以保持链路忙碌，但不要大到它们变成积压。
- en: 'This is how you can adjust the buffer:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是调整缓冲区的方法：
- en: '[PRE3]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Typical default sizes for `SO_RCVBUF` and `SO_SNDBUF` on Linux systems range
    from 128 KB to 256 KB, depending on the kernel version and system configuration.
    These defaults are chosen to provide a balance between minimizing latency and
    avoiding excessive memory usage. For high-bandwidth applications or connections
    with high latency (e.g., long-haul TCP connections), increasing these buffer sizes—sometimes
    to several megabytes—can significantly improve throughput by allowing more in-flight
    data without blocking.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Linux 系统上，`SO_RCVBUF` 和 `SO_SNDBUF` 的典型默认大小从 128 KB 到 256 KB 不等，这取决于内核版本和系统配置。这些默认值的选择是为了在最小化延迟和避免过度使用内存之间取得平衡。对于高带宽应用或具有高延迟的连接（例如，长途
    TCP 连接），增加这些缓冲区大小——有时达到几个兆字节——可以通过允许更多的飞行数据而不阻塞来显著提高吞吐量。
- en: The right buffer size depends on the RTT, the link bandwidth, and the size of
    the messages your application sends. A common rule of thumb is to match the buffer
    to the bandwidth–delay product (BDP), which is just `bandwidth × RTT`. That way,
    the connection can keep the pipe full without stalling.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 正确的缓冲区大小取决于 RTT、链路带宽以及应用程序发送的消息大小。一个常见的经验法则是将缓冲区匹配到带宽-延迟积（BDP），即 `带宽 × RTT`。这样，连接可以保持管道满载而不停滞。
- en: To figure out the right buffer sizes, you need to test under realistic load.
    Tools like iperf3 are good for measuring raw throughput, and app-specific profiling
    (pprof, netstat, custom metrics) helps spot where things actually get stuck. Increase
    the buffer sizes step by step during load tests and watch where the gains level
    off — that’s usually a good place to stop.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确定正确的缓冲区大小，您需要在实际负载下进行测试。iperf3 等工具适用于测量原始吞吐量，而特定于应用程序的剖析（pprof、netstat、自定义指标）有助于发现实际卡住的地方。在负载测试期间逐步增加缓冲区大小，并观察收益何时趋于平稳——这通常是一个停止的好地方。
- en: Warning
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: The optimal settings depend on how your system actually runs, so you need to
    measure them under load. Guessing or copying values from elsewhere usually doesn’t
    work — you have to test and adjust until it performs the way you need.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳设置取决于您的系统实际运行的方式，因此您需要在负载下进行测量。猜测或从其他地方复制值通常不起作用——您必须测试和调整，直到它以您需要的方式运行。
- en: TCP Keepalives for Reliability[¶](#tcp-keepalives-for-reliability "Permanent
    link")
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TCP Keepalives for Reliability[¶](#tcp-keepalives-for-reliability "永久链接")
- en: TCP keepalive probes detect dead peer connections, freeing resources promptly.
    Keepalives prevent hanging connections in long-lived services.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: TCP keepalive 探测可以检测死对等连接，及时释放资源。Keepalives 可以防止在长时间运行的服务中挂起连接。
- en: 'Default keepalive settings on most platforms are conservative—idle time of
    2 hours, 10 probes at 75-second intervals—intended for general-purpose environments.
    For server-side applications requiring faster failure detection (e.g., reverse
    proxies, microservices over unreliable links), significantly more aggressive settings
    are common: 30 seconds idle, 3 probes at 10-second intervals.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数平台上的默认 keepalive 设置都很保守——空闲时间为 2 小时，每 75 秒进行 10 次探测——旨在用于通用环境。对于需要更快故障检测的服务器端应用程序（例如，反向代理、通过不可靠链路的微服务），通常更激进的设置更为常见：30
    秒空闲，每 10 秒进行 3 次探测。
- en: However, aggressive tuning increases traffic and risks false positives on congested
    links. A recommended approach is to balance early detection and network conditions.
    Typical tuned values—idle 30–60s, interval 10–15s, probes 3–5—are not specified
    by any RFC or standard, but come from operational practice and vendor guidance.
    For example, PostgreSQL, Kubernetes, and AWS all recommend values in this range
    to align with cloud load balancer timeouts and service SLAs. These numbers are
    derived from empirical experience to minimize both false positives and long detection
    delays, as discussed in sources like PostgreSQL documentation, AWS networking
    blogs, and Kubernetes issue discussions.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，激进的调整会增加流量并增加在拥塞链路上的误报风险。一种推荐的方法是平衡早期检测和网络条件。典型的调整值——空闲 30-60 秒，间隔 10-15
    秒，探测 3-5 次——并非由任何 RFC 或标准指定，而是来自操作实践和供应商指南。例如，PostgreSQL、Kubernetes 和 AWS 都推荐这个范围内的值，以与云负载均衡器超时和服务
    SLA 保持一致。这些数字是从经验中得出的，旨在最大限度地减少误报和长检测延迟，正如在 PostgreSQL 文档、AWS 网络博客和 Kubernetes
    问题讨论中讨论的那样。
- en: These can be adjusted system-wide using `sysctl` or per-connection on some platforms
    (e.g., Linux) via platform-specific socket options or libraries that expose `TCP_KEEPIDLE`,
    `TCP_KEEPINTVL`, and `TCP_KEEPCNT`. In Go, `SetKeepAlivePeriod` only controls
    the idle time; deeper tuning may require cgo or raw syscalls.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这些设置可以通过 `sysctl` 在系统范围内调整，或在某些平台（例如 Linux）上通过平台特定的套接字选项或暴露 `TCP_KEEPIDLE`、`TCP_KEEPINTVL`
    和 `TCP_KEEPCNT` 的库按连接进行调整。在 Go 中，`SetKeepAlivePeriod` 只控制空闲时间；更深入的调整可能需要 cgo 或原始系统调用。
- en: 'In Go, enabling and tuning keepalives:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Go 中启用和调整 keepalives：
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The `SetKeepAlivePeriod` method in Go controls only the idle time before the
    first keepalive probe is sent. On Linux, this corresponds to the `TCP_KEEPIDLE`
    parameter; on macOS and BSD, it maps to `TCP_KEEPALIVE`. However, it does not
    affect the interval between subsequent probes (`TCP_KEEPINTVL`) or the number
    of allowed failed probes (`TCP_KEEPCNT`). These two must be set separately using
    raw syscalls or cgo bindings if finer-grained control is required. Without tuning
    `KEEPINTVL` and `KEEPCNT`, a dead connection may take several minutes to detect,
    even if `SetKeepAlivePeriod` is low.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Go 中的 `SetKeepAlivePeriod` 方法仅控制发送第一个 keepalive 探测之前的空闲时间。在 Linux 上，这对应于 `TCP_KEEPIDLE`
    参数；在 macOS 和 BSD 上，它映射到 `TCP_KEEPALIVE`。然而，它不影响后续探测之间的间隔（`TCP_KEEPINTVL`）或允许的失败探测次数（`TCP_KEEPCNT`）。如果需要更细粒度的控制，必须单独使用原始系统调用或
    cgo 绑定来设置这两个值。如果没有调整 `KEEPINTVL` 和 `KEEPCNT`，即使 `SetKeepAlivePeriod` 设置较低，死连接也可能需要几分钟才能检测到。
- en: Warning
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: Do not change the default values without testing them properly. The right settings
    depend entirely on your workload, the network path, and how the application behaves
    under real conditions. Different configurations can produce very different results,
    and what works elsewhere might hurt here. You’ll find plenty of blog posts and
    advice with suggested numbers, but those are just starting points. The only reliable
    way to figure out what works in your environment is to test and measure it yourself.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在没有适当测试的情况下不要更改默认值。正确的设置完全取决于你的工作负载、网络路径以及应用程序在实际条件下的行为。不同的配置可以产生非常不同的结果，而在其他地方有效的东西在这里可能会造成伤害。你将找到许多带有建议数字的博客文章和建议，但那些只是起点。确定你环境中什么有效唯一可靠的方法是亲自测试和测量。
- en: 'Connection Backlog: SOMAXCONN[¶](#connection-backlog-somaxconn "Permanent link")'
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 连接队列：SOMAXCONN[¶](#connection-backlog-somaxconn "永久链接")
- en: The connection backlog (`SOMAXCONN`) defines how many pending connections can
    queue up for acceptance. When the backlog queue fills up, additional connection
    attempts are refused by the kernel (usually resulting in `ECONNREFUSED`(1) or
    dropped SYN packets) until space becomes available, which can cause clients to
    see connection errors under heavy load.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 连接队列（`SOMAXCONN`）定义了可以排队等待接受多少个挂起的连接。当队列填满时，内核会拒绝额外的连接尝试（通常导致`ECONNREFUSED`(1)或丢弃SYN数据包），直到空间变得可用，这可能导致在高负载下客户端看到连接错误。
- en: A `connect()` on a stream socket found no one listening on the remote address.
    See Linux [man pages](https://man7.org/linux/man-pages/man2/connect.2.html) for
    more details.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在流套接字上的`connect()`调用没有在远程地址上找到监听者。有关更多详细信息，请参阅Linux [手册页](https://man7.org/linux/man-pages/man2/connect.2.html)。
- en: The default value of `SOMAXCONN` on Linux is typically 128 or 4096, depending
    on the kernel version and distribution. These defaults are chosen to strike a
    balance between memory use and handling normal connection rates, but they may
    be too low for high-traffic servers or services experiencing connection bursts.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Linux上`SOMAXCONN`的默认值通常是128或4096，具体取决于内核版本和发行版。这些默认值是为了在内存使用和处理正常连接速率之间取得平衡而选择的，但它们对于高流量服务器或经历连接突发的服务可能太低。
- en: A bigger backlog helps when the server gets hit with a burst of connections
    — like after a failover or during peak — so clients don’t get dropped right away.
    Raising `SOMAXCONN` lets the kernel queue more connections while the app catches
    up and keeps the service reachable under load.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 当服务器受到连接突发（如故障转移后或高峰期间）的冲击时，更大的队列有助于避免客户端立即掉线。提高`SOMAXCONN`允许内核在应用程序赶上并保持服务在负载下可达的同时排队更多连接。
- en: Changing `SOMAXCONN` is a system-level setting. On Linux you usually adjust
    it through kernel parameters.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 修改`SOMAXCONN`是一个系统级设置。在Linux上，你通常通过内核参数进行调整。
- en: '[PRE5]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Safely Wrapping Syscalls in Go[¶](#safely-wrapping-syscalls-in-go "Permanent
    link")
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Go中安全地包装系统调用[¶](#safely-wrapping-syscalls-in-go "永久链接")
- en: Working with socket options through syscalls means dealing directly with file
    descriptors. These calls need to happen before the socket is bound or used, which
    makes the timing important and easy to get wrong. If you set an option too late,
    the kernel ignores it, or worse, you get hard-to-reproduce bugs. Since you’re
    bypassing the Go runtime, you’re also responsible for checking errors and making
    sure the file descriptor stays in a valid state. `syscall.RawConn` exists to help
    with this — it gives you a controlled hook to run your code against the socket
    at exactly the right point during setup.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 通过系统调用与套接字选项一起工作意味着直接处理文件描述符。这些调用必须在套接字绑定或使用之前发生，这使得时机很重要，并且很容易出错。如果你设置选项太晚，内核会忽略它，或者更糟糕的是，你会得到难以复现的错误。由于你绕过了Go运行时，你也负责检查错误并确保文件描述符保持有效状态。`syscall.RawConn`存在就是为了帮助解决这个问题——它为你提供了一个受控的钩子，可以在设置过程中在精确的点上运行你的代码。
- en: '[PRE6]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The socket options are set during creation, at the right time, and in one place.
    That keeps them from being forgotten or misapplied later.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '网络套接字选项在创建时、在正确的时间和位置设置。这避免了它们被遗忘或错误应用。 '
- en: Real-World Considerations[¶](#real-world-considerations "Permanent link")
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实际考虑因素[¶](#real-world-considerations "永久链接")
- en: Tuning sockets is always a trade-off — lower latency, higher throughput, better
    reliability, and reasonable resource use don’t usually come together for free.
    You need to understand your workload, change one thing at a time, and test it
    under real load. Without monitoring and instrumentation, you’re just guessing,
    so measure everything and make sure the changes actually help.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 调整套接字总是一种权衡——较低的延迟、较高的吞吐量、更好的可靠性和合理的资源使用通常不会免费地同时出现。你需要了解你的工作负载，一次改变一件事，并在实际负载下进行测试。没有监控和仪表，你只是在猜测，所以测量一切并确保这些更改确实有帮助。
