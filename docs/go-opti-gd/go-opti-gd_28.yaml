- en: Go Networking Internals¶
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Go 网络内部结构
- en: 原文：[https://goperf.dev/02-networking/networking-internals/](https://goperf.dev/02-networking/networking-internals/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://goperf.dev/02-networking/networking-internals/](https://goperf.dev/02-networking/networking-internals/)
- en: 'Go’s networking model is deceptively simple on the surface—spawn a goroutine,
    accept a connection, read from it, and write a response. But behind this apparent
    ease is a highly optimized and finely tuned runtime that handles tens or hundreds
    of thousands of connections with minimal OS overhead. In this deep dive, we’ll
    walk through the mechanisms that make this possible: from goroutines and the scheduler
    to how Go interacts with OS-level pollers like `epoll`, `kqueue`, and IOCP.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Go 的网络模型表面上看似简单——启动一个协程，接受连接，从中读取，并写入响应。但在这看似简单的背后，是一个高度优化和精细调校的运行时，它以最小的操作系统开销处理成千上万的连接。在这篇深入探讨中，我们将了解使这一切成为可能的各种机制：从协程和调度器到
    Go 如何与操作系统级别的轮询器（如 `epoll`、`kqueue` 和 IOCP）交互。
- en: Goroutines and the Runtime Scheduler[¶](#goroutines-and-the-runtime-scheduler
    "Permanent link")
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 协程和运行时调度器[¶](#goroutines-and-the-runtime-scheduler "永久链接")
- en: Goroutines are lightweight user-space threads managed by the Go runtime. They’re
    cheap to create (a few kilobytes of stack) and can scale to millions. But they’re
    not magic—they rely on the runtime scheduler to multiplex execution across a limited
    number of OS threads.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 协程是由 Go 运行时管理的轻量级用户空间线程。它们易于创建（只有几KB的堆栈），可以扩展到数百万。但它们并非魔法——它们依赖于运行时调度器来在有限的操作系统线程之间多路复用执行。
- en: 'Go’s scheduler is based on an M:N model:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Go 的调度器基于 M:N 模型：
- en: '**M (Machine)**: Represents an OS thread.'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**M (机器)**: 代表操作系统线程。'
- en: '**G (Goroutine)**: Represents the actual task or coroutine.'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**G (协程)**: 代表实际的任务或协程。'
- en: '**P (Processor)**: Represents the context for scheduling (holding run queues,
    caches).'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**P (处理器)**: 代表调度上下文（持有运行队列、缓存）。'
- en: Each P can execute one G at a time using an M. There are as many Ps as GOMAXPROCS.
    If a goroutine blocks on I/O, another runnable G may park and reuse the thread.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 P 可以使用一个 M 同时执行一个 G。P 的数量与 GOMAXPROCS 相等。如果一个协程在 I/O 上阻塞，另一个可运行的 G 可能会挂起并重用线程。
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Blocking I/O in Goroutines: What Really Happens?[¶](#blocking-io-in-goroutines-what-really-happens
    "Permanent link")'
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 协程中的阻塞 I/O：实际发生了什么？[¶](#blocking-io-in-goroutines-what-really-happens "永久链接")
- en: Suppose a goroutine calls `conn.Read()`. This *looks* blocking—but only from
    the goroutine's perspective. Internally, Go’s runtime intercepts the call and
    uses a mechanism known as the [netpoller](https://go.dev/src/runtime/netpoll.go).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一个协程调用 `conn.Read()`。这看起来是阻塞的——但仅从协程的角度来看。内部，Go 的运行时拦截调用并使用一种称为 [netpoller](https://go.dev/src/runtime/netpoll.go)
    的机制。
- en: 'On Unix-based systems, Go uses readiness-based polling (`epoll` on Linux, `kqueue`
    on macOS/BSD). When a goroutine performs a syscall like `read(fd)`, the runtime
    checks whether the file descriptor is ready. If not:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于 Unix 的系统上，Go 使用基于就绪的轮询（Linux 上的 `epoll`，macOS/BSD 上的 `kqueue`）。当一个协程执行 `read(fd)`
    这样的系统调用时，运行时会检查文件描述符是否就绪。如果不是：
- en: The goroutine is parked.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 协程被挂起。
- en: The file descriptor is registered with the poller.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 文件描述符注册到轮询器。
- en: The OS thread is released to run other work.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 操作系统线程被释放以运行其他工作。
- en: When the fd becomes ready, the poller wakes up, and the runtime marks the goroutine
    as runnable.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当文件描述符准备好时，轮询器唤醒，运行时将协程标记为可运行。
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This system enables Go to serve a massive number of clients concurrently, using
    a small number of threads, avoiding the overhead of traditional thread-per-connection
    models.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这个系统使 Go 能够使用少量线程并发地服务大量客户端，避免了传统线程-连接模型的开销。
- en: Internals of the `net` Package[¶](#internals-of-the-net-package "Permanent link")
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '`net` 包的内部结构[¶](#internals-of-the-net-package "永久链接")'
- en: Let’s take a look at what happens behind `net.Listen("tcp", ":8080")` and `conn.Read()`.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看在 `net.Listen("tcp", ":8080")` 和 `conn.Read()` 后面发生了什么。
- en: '`net.Listen` calls into `net.ListenTCP`, which constructs a `netFD` struct
    wrapping the socket.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`net.Listen` 调用 `net.ListenTCP`，它构建一个包装套接字的 `netFD` 结构体。'
- en: The socket is marked non-blocking via `syscall.SetNonblock(fd, true)`.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过 `syscall.SetNonblock(fd, true)` 将套接字标记为非阻塞。
- en: '`Accept` and `Read` methods on `netFD` are layered on top of syscalls, but
    routed through internal pollers and wrapped with logic to yield and resume goroutines.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`netFD` 上的 `Accept` 和 `Read` 方法是在系统调用之上分层，但通过内部轮询器路由，并用逻辑包装以产生和恢复协程。'
- en: 'Here’s a rough diagram of the call chain:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个调用链的大致图：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This architecture makes the blocking calls from the developer’s perspective
    translate into non-blocking interactions with the kernel.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这种架构使得从开发者的角度来看，阻塞调用转换为与内核的非阻塞交互。
- en: 'The Netpoller: Polling with Epoll/Kqueue/IOCP[¶](#the-netpoller-polling-with-epollkqueueiocp
    "Permanent link")'
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Netpoller：使用 Epoll/Kqueue/IOCP 进行轮询[¶](#the-netpoller-polling-with-epollkqueueiocp
    "永久链接")
- en: The **netpoller** is a runtime subsystem that integrates low-level polling mechanisms
    with Go’s scheduling system. Each fd has an associated `pollDesc`, which helps
    coordinate goroutine suspension and resumption.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**netpoller** 是一个运行时子系统，它将低级轮询机制与 Go 的调度系统集成。每个 fd 都有一个相关的 `pollDesc`，这有助于协调
    goroutine 的挂起和恢复。'
- en: 'The poller operates in a dedicated thread (or threads) that loop over OS wait
    primitives:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 轮询器在一个专用的线程（或线程）中运行，该线程循环遍历 OS 等待原语：
- en: '[epoll_wait](https://man7.org/linux/man-pages/man2/epoll_wait.2.html) (Linux)'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[epoll_wait](https://man7.org/linux/man-pages/man2/epoll_wait.2.html)（Linux）'
- en: '[kqueue](https://en.wikipedia.org/wiki/Kqueue) (macOS/BSD)'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[kqueue](https://en.wikipedia.org/wiki/Kqueue)（macOS/BSD）'
- en: '[IOCP](https://learn.microsoft.com/en-gb/windows/win32/fileio/i-o-completion-ports)
    (Windows)'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[IOCP](https://learn.microsoft.com/en-gb/windows/win32/fileio/i-o-completion-ports)（Windows）'
- en: When an I/O event fires, the poller finds the associated `pollDesc`, identifies
    the parked goroutine, and puts it back into the run queue.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个 I/O 事件触发时，轮询器找到相关的 `pollDesc`，识别出挂起的 goroutine，并将其放回运行队列中。
- en: 'In the Go source, relevant files include:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Go 源代码中，相关文件包括：
- en: '[runtime/netpoll_epoll.go](https://go.dev/src/runtime/netpoll_epoll.go)'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[runtime/netpoll_epoll.go](https://go.dev/src/runtime/netpoll_epoll.go)'
- en: '[runtime/netpoll_kqueue.go](https://go.dev/src/runtime/netpoll_kqueue.go)'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[runtime/netpoll_kqueue.go](https://go.dev/src/runtime/netpoll_kqueue.go)'
- en: '[runtime/netpoll_windows.go](https://go.dev/src/runtime/netpoll_windows.go)'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[runtime/netpoll_windows.go](https://go.dev/src/runtime/netpoll_windows.go)'
- en: 'The Go poller is readiness-based (not completion-based, except for Windows
    IOCP). It handles:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Go 轮询器基于就绪状态（不是基于完成状态，除了 Windows IOCP）。它处理：
- en: fd registration
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: fd 注册
- en: waking goroutines on readiness
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在就绪时唤醒 goroutine
- en: integration with the run queue (P-local or global)
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与运行队列的集成（P-局部或全局）
- en: 'Example: High-Performance TCP Echo Server[¶](#example-high-performance-tcp-echo-server
    "Permanent link")'
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例：高性能 TCP Echo 服务器[¶](#example-high-performance-tcp-echo-server "永久链接")
- en: Let's break down a simple Go TCP echo server and map each part to Go’s internal
    networking and scheduling mechanisms — including `netFD`, `poll.FD`, and goroutines.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分解一个简单的 Go TCP Echo 服务器，并将每个部分映射到 Go 的内部网络和调度机制——包括 `netFD`、`poll.FD` 和 goroutine。
- en: <details class="example"><summary>Simple Echo server source code</summary>
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: <details class="example"><summary>简单的 Echo 服务器源代码</summary>
- en: '[PRE3]</details>'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE3]</details>'
- en: Imports and Setup[¶](#imports-and-setup "Permanent link")
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 导入和设置[¶](#imports-and-setup "永久链接")
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**Internals Involved**:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**涉及的内部机制**：'
- en: The `net` package abstracts system-level networking.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`net` 包抽象了系统级网络。'
- en: 'Under the hood:'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在底层：
- en: Uses `netFD` (internal, private struct)
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `netFD`（内部，私有结构）
- en: Wraps `poll.FD` for non-blocking I/O
  id: totrans-53
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `poll.FD` 包装非阻塞 I/O
- en: Uses OS features like `epoll`, `kqueue`, or `IOCP` for event notification
  id: totrans-54
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 OS 功能，如 `epoll`、`kqueue` 或 `IOCP` 进行事件通知
- en: Listener Setup[¶](#listener-setup "Permanent link")
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 监听器设置[¶](#listener-setup "永久链接")
- en: '[PRE5]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '**Internals Involved**:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**涉及的内部机制**：'
- en: '`net.Listen()` returns a `TCPListener`'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`net.Listen()` 返回一个 `TCPListener`'
- en: Internally calls `syscall.socket`, `bind`, `listen`
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内部调用 `syscall.socket`、`bind`、`listen`
- en: Associates a `netFD` with the socket
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 `netFD` 与套接字关联
- en: The listener uses Go’s internal poller to enable non-blocking `Accept`
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监听器使用 Go 的内部轮询器来启用非阻塞 `Accept`
- en: Accept Loop and Goroutine Scheduling[¶](#accept-loop-and-goroutine-scheduling
    "Permanent link")
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 接受循环和 goroutine 调度[¶](#accept-loop-and-goroutine-scheduling "永久链接")
- en: '[PRE6]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**Internals Involved**:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**涉及的内部机制**：'
- en: '`listener.Accept()` → `netFD.Accept()` → `poll.FD.Accept()` → `syscall.accept`'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`listener.Accept()` → `netFD.Accept()` → `poll.FD.Accept()` → `syscall.accept`'
- en: Non-blocking, waits via Go's poller (`runtime_pollWait`)
  id: totrans-66
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非阻塞，通过 Go 的轮询器 (`runtime_pollWait`) 等待
- en: '`go handle(conn)` spawns a **goroutine (G)**'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`go handle(conn)` 启动一个 **goroutine (G**)'
- en: Scheduled onto a **P** (Processor)
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调度到 **P**（处理器）
- en: '`P` is part of Go’s M:N scheduler governed by `GOMAXPROCS`'
  id: totrans-69
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`P` 是 Go 的 M:N 调度器的一部分，由 `GOMAXPROCS` 管理'
- en: Connection Handler[¶](#connection-handler "Permanent link")
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 连接处理器[¶](#connection-handler "永久链接")
- en: '[PRE7]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**Internals Involved**:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**涉及的内部机制**：'
- en: '`bufio.NewReader(conn)` wraps the `net.Conn`, which is backed by `*TCPConn`
    and `netFD`.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bufio.NewReader(conn)` 包装 `net.Conn`，它由 `*TCPConn` 和 `netFD` 支持。'
- en: '`ReadString()` calls `conn.Read()` under the hood:'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ReadString()` 在底层调用 `conn.Read()`'
- en: '`netFD.Read()` → `poll.FD.Read()` → `syscall.Read()`'
  id: totrans-75
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`netFD.Read()` → `poll.FD.Read()` → `syscall.Read()`'
- en: Uses `runtime_pollWait` to yield the goroutine if data isn't ready
  id: totrans-76
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `runtime_pollWait` 在数据未准备好时让goroutine让出
- en: '`SetReadDeadline` sets a timeout by integrating with the runtime''s network
    poller to prevent indefinite blocking.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SetReadDeadline` 通过与运行时的网络轮询器集成来设置超时，以防止无限期阻塞。'
- en: '`conn.Write()` → `netFD.Write()` → `poll.FD.Write()` → `syscall.write`'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conn.Write()` → `netFD.Write()` → `poll.FD.Write()` → `syscall.write`'
- en: Internal Flow Diagram[¶](#internal-flow-diagram "Permanent link")
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 内部流程图[¶](#internal-flow-diagram "永久链接")
- en: '[PRE8]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This model scales well as long as you:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 只要你这样做，这个模型就能很好地扩展：
- en: Ensure your `ulimit -n` is high enough
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保你的 `ulimit -n` 足够高
- en: Avoid shared state and contention
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 避免共享状态和竞争
- en: Tune your GOMAXPROCS for your workload
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为你的工作负载调整 `GOMAXPROCS`
- en: Observations at Scale[¶](#observations-at-scale "Permanent link")
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 规模观察[¶](#observations-at-scale "永久链接")
- en: 'As connections scale up ([see how it may look like here](../gc-endpoint-profiling/)):'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 随着连接的扩展（[在此处查看可能的样子](../gc-endpoint-profiling/))）：
- en: Per-connection memory and GC pressure grow
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个连接的内存和GC压力增加
- en: Frequent goroutine context switching may introduce latency
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 频繁的goroutine上下文切换可能会引入延迟
- en: Coordinating channels, timeouts, and backpressure adds complexity
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 协调通道、超时和背压增加了复杂性
- en: 'Some mitigation strategies:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 一些缓解策略：
- en: Use `sync.Pool` for buffer reuse
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `sync.Pool` 进行缓冲区重用
- en: Minimize GC pauses (avoid per-request allocations)
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最小化GC暂停（避免每个请求的分配）
- en: Prefer `netpoll`-friendly designs (avoid long CPU-bound goroutines)
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优先选择 `netpoll` 友好的设计（避免长时间CPU密集型goroutine）
- en: '* * *'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Go’s model trades OS-level multiplexing for user-space scheduling and event-driven
    I/O coordination. It’s not a silver bullet—but when used correctly, it offers
    a robust platform for building scalable network services. Understanding these
    internals helps you avoid common traps, optimize at the right layer, and build
    systems that behave predictably under load.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Go的模型以操作系统级别的多路复用交换为用户空间调度和事件驱动的I/O协调。它不是万能的银弹——但使用得当，它提供了一个健壮的平台，用于构建可扩展的网络服务。理解这些内部机制有助于你避免常见陷阱，在正确的层进行优化，并构建在负载下表现可预测的系统。
