- en: Efficient Use of net/http, net.Conn, and UDP in High-Traffic Go Services¶
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高流量 Go 服务中 net/http、net.Conn 和 UDP 的有效使用[¶]
- en: 原文：[https://goperf.dev/02-networking/efficient-net-use/](https://goperf.dev/02-networking/efficient-net-use/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://goperf.dev/02-networking/efficient-net-use/](https://goperf.dev/02-networking/efficient-net-use/)
- en: When we first start building high-traffic services in Go, we often lean heavily
    on `net/http`. It’s stable, ergonomic, and remarkably capable for 80% of use cases.
    But as soon as traffic spikes or latency budgets shrink, the cracks begin to show.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们最初在 Go 中构建高流量服务时，我们通常会大量依赖 `net/http`。它稳定、易用，对于 80% 的用例来说非常出色。但是，一旦流量激增或延迟预算减少，问题就开始显现。
- en: It’s not that `net/http` is broken—it’s just that the defaults are tuned for
    convenience, not for performance under stress. And as we scale backend services
    to handle millions of requests per second, understanding what happens underneath
    the abstraction becomes the difference between meeting SLOs and fire-fighting
    in production.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 并非 `net/http` 出了问题——只是默认设置是为了方便而调整的，而不是在压力下的性能。随着我们将后端服务扩展到每秒处理数百万个请求，了解抽象之下的发生情况成为满足
    SLO 和生产中的救火之间的区别。
- en: This article is a walkthrough of how to make networked Go services truly efficient—what
    works, what breaks, and how to go beyond idiomatic usage. We’ll start with `net/http`,
    drop into raw `net.Conn`, and finish with real-world patterns for handling UDP
    in latency-sensitive systems.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本文将指导如何使网络化的 Go 服务真正高效——什么有效，什么无效，以及如何超越惯用用法。我们将从 `net/http` 开始，深入到原始的 `net.Conn`，并以处理延迟敏感系统中
    UDP 的实际模式结束。
- en: The Hidden Complexity Behind a Simple HTTP Call[¶](#the-hidden-complexity-behind-a-simple-http-call
    "Permanent link")
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简单 HTTP 调用背后的隐藏复杂性[¶](#the-hidden-complexity-behind-a-simple-http-call "永久链接")
- en: 'Let’s begin where most Go developers do: a simple `http.Client`.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从大多数 Go 开发者开始的地方开始：一个简单的 `http.Client`。
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This looks harmless. It gets the job done, and in most local tests, it performs
    reasonably well. But in production, at scale, this innocent-looking code can trigger
    a surprising range of issues: leaked connections, memory spikes, blocked goroutines,
    and mysterious latency cliffs.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来无害。它完成了工作，并且在大多数本地测试中表现合理。但在生产环境中，在规模上，这种看似无辜的代码可能会触发一系列令人惊讶的问题：泄漏的连接、内存激增、阻塞的
    goroutine 和神秘的延迟悬崖。
- en: One of the most common issues is forgetting to fully read `resp.Body` before
    closing it. [Go’s HTTP client won’t reuse connections unless the body is drained](https://github.com/google/go-github/pull/317).
    And under load, that means you're constantly opening new TCP connections—slamming
    the kernel with ephemeral ports, exhausting file descriptors, and triggering throttling.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的问题之一是关闭前忘记完全读取 `resp.Body`。[Go 的 HTTP 客户端只有在内容被完全读取后才会重用连接](https://github.com/google/go-github/pull/317)。在负载下，这意味着您会不断打开新的
    TCP 连接——对内核的临时端口进行冲击，耗尽文件描述符，并触发节流。
- en: 'Here’s the safe pattern:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个安全的模式：
- en: '[PRE1]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Transport Tuning: When Defaults Aren’t Enough[¶](#transport-tuning-when-defaults-arent-enough
    "Permanent link")'
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调整传输：当默认设置不足时[¶](#transport-tuning-when-defaults-arent-enough "永久链接")
- en: It’s easy to overlook how much global state hides behind `http.DefaultTransport`.
    If you spin up multiple `http.Client` instances across your app without customizing
    the transport, you're probably reusing a shared global pool without realizing
    it.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 很容易忽视 `http.DefaultTransport` 后面隐藏了多少全局状态。如果您在应用程序中启动多个未自定义传输的 `http.Client`
    实例，您可能在不意识到的情况下正在重用共享的全局池。
- en: 'This leads to unpredictable behavior under load: idle connections get evicted
    too quickly, or keep-alive connections linger longer than they should. The fix?
    Build a tuned `Transport` that matches your concurrency profile.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这会导致在负载下的不可预测行为：空闲连接被过快地移除，或者保持连接的时间比应有的要长。解决方案？构建一个与您的并发配置相匹配的调整过的 `Transport`。
- en: Custom `http.Transport` Fields to Tune[¶](#custom-httptransport-fields-to-tune
    "Permanent link")
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可调整的自定义 `http.Transport` 字段[¶](#custom-httptransport-fields-to-tune "永久链接")
- en: 'All the following settings are part of the `http.Transport` struct:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 以下所有设置都是 `http.Transport` 结构体的一部分：
- en: '[PRE2]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: More Advanced Optimization Tricks[¶](#more-advanced-optimization-tricks "Permanent
    link")
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更高级的优化技巧[¶](#more-advanced-optimization-tricks "永久链接")
- en: 'These are all tied to key settings in the `http.Transport`, `http.Client`,
    and `http.Server` structs, or custom wrappers built on top of them:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这些都与 `http.Transport`、`http.Client` 和 `http.Server` 结构体中的关键设置相关联，或者是在它们之上构建的自定义包装器：
- en: Set `ExpectContinueTimeout` Carefully[¶](#set-expectcontinuetimeout-carefully
    "Permanent link")
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 谨慎设置 `ExpectContinueTimeout`[¶](#set-expectcontinuetimeout-carefully "永久链接")
- en: 'If our clients send large POST requests and the server doesn’t support `100-continue`
    properly, we can reduce or eliminate this delay:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的客户端发送大的 POST 请求，而服务器没有正确支持 `100-continue`，我们可以减少或消除这种延迟：
- en: '[PRE3]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Constrain `MaxConnsPerHost`[¶](#constrain-maxconnsperhost "Permanent link")
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 限制 `MaxConnsPerHost`[¶](#constrain-maxconnsperhost "永久链接")
- en: Go’s default HTTP client will open an unbounded number of connections to a host.
    That’s fine until one of your downstreams can’t handle it.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Go 的默认 HTTP 客户端会为一个主机打开无限数量的连接。这很好，直到您的下游之一无法处理。
- en: '[PRE4]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This prevents stampedes during spikes and avoids exhausting resources on your
    backend services.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这防止了高峰期间的冲击，并避免了耗尽后端服务的资源。
- en: Use Small `http.Client.Timeout`[¶](#use-small-httpclienttimeout "Permanent link")
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用小的 `http.Client.Timeout`[¶](#use-small-httpclienttimeout "永久链接")
- en: 'A common mistake is setting a very high timeout (e.g., 30s) for safety. But
    long timeouts hold onto goroutines, buffers, and sockets under pressure. Prefer
    tighter control:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见的错误是设置非常高的超时时间（例如，30 秒）以确保安全。但是，长时间的超时会在压力下保持 goroutines、缓冲区和套接字。更喜欢更紧密的控制：
- en: '[PRE5]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Instead of relying on big timeouts, use retries with backoff (e.g., with go-retryablehttp)
    to improve resiliency under partial failure.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是依赖于大的超时，使用带有退避的重试（例如，使用 go-retryablehttp）来提高部分失败情况下的弹性。
- en: Explicitly Set `ReadBufferSize` and `WriteBufferSize` in `http.Server`[¶](#explicitly-set-readbuffersize-and-writebuffersize-in-httpserver
    "Permanent link")
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在 `http.Server` 中显式设置 `ReadBufferSize` 和 `WriteBufferSize`[¶](#explicitly-set-readbuffersize-and-writebuffersize-in-httpserver
    "永久链接")
- en: 'Go''s `http.Server` does not expose `ReadBufferSize` and `WriteBufferSize`
    directly, but when you need to reduce GC pressure and improve syscall efficiency
    under load, you can pre-size the buffers in custom `Conn` wrappers. 4KB–8KB is
    a balanced value for most workloads: it''s large enough to handle small headers
    and bodies efficiently without wasting memory. For example, 4KB covers almost
    all typical HTTP headers and small JSON payloads.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Go 的 `http.Server` 没有直接暴露 `ReadBufferSize` 和 `WriteBufferSize`，但在您需要减少 GC 压力和提高负载下的系统调用效率时，您可以在自定义的
    `Conn` 包装器中预先设置缓冲区的大小。对于大多数工作负载来说，4KB–8KB 是一个平衡的值：它足够大，可以有效地处理小的头和体，而不会浪费内存。例如，4KB
    可以覆盖几乎所有典型的 HTTP 头和小的 JSON 有效负载。
- en: You can implement this using `bufio.NewReaderSize` and `NewWriterSize` in a
    wrapped connection that plugs into a custom `net.Listener` and `http.Server.ConnContext`.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过在包装连接中使用 `bufio.NewReaderSize` 和 `NewWriterSize` 来实现这一点，该连接连接到自定义的 `net.Listener`
    和 `http.Server.ConnContext`。
- en: 'If you''re using `fasthttp`, you can configure buffer sizes explicitly:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用 `fasthttp`，您可以显式配置缓冲区大小：
- en: '[PRE6]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This avoids dynamic allocations on each request and leads to more predictable
    memory usage and cache locality under high throughput.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这避免了每次请求时的动态分配，并在高吞吐量下导致更可预测的内存使用和缓存局部性。
- en: Use `bufio.Reader.Peek()` for Efficient Framing[¶](#use-bufioreaderpeek-for-efficient-framing
    "Permanent link")
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 `bufio.Reader.Peek()` 进行高效的帧化[¶](#use-bufioreaderpeek-for-efficient-framing
    "永久链接")
- en: When implementing a framed protocol over TCP, like length-prefixed binary messages,
    naively calling `Read()` in a loop can lead to fragmented reads and unnecessary
    syscalls. This adds up, especially under load. Using `Peek()` gives you a look
    into the buffered data without advancing the read position, making it easier to
    detect message boundaries without triggering extra reads. It’s a practical technique
    in streaming systems or multiplexed connections where tight control over framing
    is critical.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 当在 TCP 上实现帧协议时，如长度前缀的二进制消息，在循环中天真地调用 `Read()` 可能会导致读取碎片化和不必要的系统调用。这会累积起来，尤其是在负载下。使用
    `Peek()` 允许您查看缓冲区数据而不前进读取位置，这使得在没有触发额外读取的情况下更容易检测消息边界。这在需要严格控制帧化的流式系统或复用连接中是一种实用的技术。
- en: '[PRE7]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Force Fresh DNS Lookups with Custom Dialers[¶](#force-fresh-dns-lookups-with-custom-dialers
    "Permanent link")
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用自定义拨号器强制进行新的 DNS 查询[¶](#force-fresh-dns-lookups-with-custom-dialers "永久链接")
- en: Go’s built-in DNS caching lasts for the lifetime of the process. In dynamic
    environments, like Kubernetes, this can become a problem when service IPs change
    but clients keep reusing stale ones. To avoid this, you can force fresh DNS lookups
    by creating a new net.Dialer per request or rotating the HTTP client periodically.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Go 的内置 DNS 缓存持续整个进程的生命周期。在动态环境中，如 Kubernetes，当服务 IP 发生变化但客户端继续重用过时的 IP 时，这可能会成为一个问题。为了避免这种情况，您可以通过为每个请求创建一个新的
    `net.Dialer` 或定期轮换 HTTP 客户端来强制进行新的 DNS 查询。
- en: 'But you can bypass Go’s internal DNS cache when needed:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 但在需要时，您可以绕过 Go 的内部 DNS 缓存：
- en: '[PRE8]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This ensures a fresh DNS lookup per request. While this adds minor overhead,
    it's necessary in failover-sensitive environments.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这确保了每个请求都进行一次新的 DNS 查询。虽然这会增加一些开销，但在故障转移敏感的环境中这是必要的。
- en: Use `sync.Pool` for Readers/Writers[¶](#use-syncpool-for-readerswriters "Permanent
    link")
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用`sync.Pool`为读取器/写入器[¶](#use-syncpool-for-readerswriters "永久链接")
- en: Most people use `sync.Pool` to reuse `[]byte` buffers, but for services that
    process many requests per second, allocating `bufio.Reader` and `bufio.Writer`
    objects per connection adds up. These objects also maintain their own buffers,
    so recycling them reduces pressure on both heap allocations and garbage collection.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数人使用`sync.Pool`来重用`[]byte`缓冲区，但对于每秒处理许多请求的服务，为每个连接分配`bufio.Reader`和`bufio.Writer`对象会累积起来。这些对象还维护自己的缓冲区，因此回收它们可以减轻堆分配和垃圾回收的压力。
- en: '[PRE9]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This practice significantly reduces allocation churn and improves latency consistency,
    especially in systems processing thousands of connections concurrently.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这种做法显著减少了分配的波动并提高了延迟一致性，尤其是在处理数千个并发连接的系统。
- en: Don’t Share `http.Client` Across Multiple Hosts[¶](#dont-share-httpclient-across-multiple-hosts
    "Permanent link")
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 不要在多个主机之间共享`http.Client`[¶](#dont-share-httpclient-across-multiple-hosts "永久链接")
- en: While it might seem efficient to reuse a single `http.Client`, each target host
    maintains its own internal connection pool within the underlying `http.Transport`.
    If you use the same client for multiple base URLs, you end up mixing connection
    reuse and causing head-of-line blocking across unrelated services. Worse, DNS
    caching and socket exhaustion become harder to track.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然重用单个`http.Client`可能看起来效率更高，但每个目标主机在底层的`http.Transport`中维护自己的内部连接池。如果你为多个基本
    URL 使用相同的客户端，你最终会混合连接重用，导致无关服务之间的头阻塞。更糟糕的是，DNS 缓存和套接字耗尽变得难以追踪。
- en: Instead, create a dedicated `http.Client` for each upstream service you interact
    with. This improves connection reuse, avoids cross-talk between services, and
    usually makes behavior more predictable, especially in environments like service
    meshes or when dealing with multiple external APIs.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，为每个你与之交互的上游服务创建一个专门的`http.Client`。这提高了连接的重用性，避免了服务之间的串扰，并且通常使行为更加可预测，尤其是在服务网格或处理多个外部
    API 的环境中。
- en: Use `ConnContext` and `ConnState` Hooks for Debugging[¶](#use-conncontext-and-connstate-hooks-for-debugging
    "Permanent link")
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用`ConnContext`和`ConnState`钩子进行调试[¶](#use-conncontext-and-connstate-hooks-for-debugging
    "永久链接")
- en: These hooks are useful for tracking the lifecycle of each connection—especially
    when debugging issues like memory leaks, stuck connections, or resource exhaustion
    in production. The `ConnState` callback gives visibility into transitions such
    as `StateNew`, `StateActive`, `StateIdle`, and `StateHijacked`, allowing you to
    log, trace, or apply custom handling per connection state.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这些钩子对于跟踪每个连接的生命周期非常有用——尤其是在调试生产环境中的内存泄漏、连接停滞或资源耗尽等问题时。`ConnState`回调提供了对`StateNew`、`StateActive`、`StateIdle`和`StateHijacked`等转换的可见性，允许你根据连接状态进行日志记录、跟踪或应用自定义处理。
- en: By monitoring these events, you can detect when connections hang, fail to close,
    or unexpectedly idle out. It also helps when correlating behavior with client
    IPs or network zones.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 通过监控这些事件，你可以检测到连接挂起、无法关闭或意外空闲的情况。这也有助于将行为与客户端 IP 或网络区域相关联。
- en: '[PRE10]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Dropping the Abstraction: When to Use `net.Conn`[¶](#dropping-the-abstraction-when-to-use-netconn
    "Permanent link")'
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 放弃抽象：何时使用`net.Conn`[¶](#dropping-the-abstraction-when-to-use-netconn "永久链接")
- en: As we get closer to the limits of what the Go standard library can offer, it’s
    worth knowing that there are high-performance alternatives built specifically
    for event-driven, low-latency workloads. Projects like [`cloudwego/netpoll`](https://github.com/cloudwego/netpoll)
    and [`tidwall/evio`](https://github.com/tidwall/evio) offer powerful tools for
    maximizing performance beyond what’s achievable with `net.Conn` alone.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们接近 Go 标准库所能提供的极限，了解存在专门为事件驱动、低延迟工作负载构建的高性能替代品是很有价值的。例如，`cloudwego/netpoll`（[GitHub链接](https://github.com/cloudwego/netpoll)）和`tidwall/evio`（[GitHub链接](https://github.com/tidwall/evio)）等项目提供了超越仅使用`net.Conn`所能实现性能的强大工具。
- en: '**[`cloudwego/netpoll`](https://github.com/cloudwego/netpoll)** is an epoll-based
    network library designed for building massive concurrent network services with
    minimal GC overhead. It uses event-based I/O to eliminate goroutine-per-connection
    costs, ideal for scenarios like RPC proxies, internal service meshes, or high-frequency
    messaging systems.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**[`cloudwego/netpoll`](https://github.com/cloudwego/netpoll)** 是一个基于 epoll
    的网络库，旨在以最小的 GC 开销构建大规模并发网络服务。它使用基于事件的 I/O 来消除每个连接的 goroutine 成本，非常适合像 RPC 代理、内部服务网格或高频消息系统这样的场景。'
- en: '**[`tidwall/evio`](https://github.com/tidwall/evio)** provides a fast, non-blocking
    event loop for Go based on the [reactor pattern](https://en.wikipedia.org/wiki/Reactor_pattern).
    It’s well-suited for protocols where latency matters more than per-connection
    state complexity, such as custom TCP, UDP protocols, or lightweight gateways.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**[`tidwall/evio`](https://github.com/tidwall/evio)** 提供了一个基于 [reactor 模式](https://en.wikipedia.org/wiki/Reactor_pattern)
    的快速、非阻塞的事件循环，适用于对延迟敏感而连接状态复杂性较低的协议，例如自定义 TCP、UDP 协议或轻量级网关。'
- en: If you're building systems where throughput or connection count exceeds hundreds
    of thousands, or where tail latency is critical, it's worth exploring these libraries.
    They come with trade-offs—most notably, less standardization and more manual lifecycle
    management—but in return, they give you fine-grained control over performance-critical
    paths.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在构建吞吐量或连接数超过数十万，或尾延迟至关重要的系统，探索这些库是值得的。它们有权衡——最明显的是标准化程度较低和更多手动生命周期管理——但作为回报，它们让你能够对性能关键路径进行精细控制。
- en: Sometimes, even a tuned HTTP stack isn't enough.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，即使是调优过的 HTTP 栈也不够。
- en: In cases like internal binary protocols or services dealing with hundreds of
    thousands of requests per second, we may find we're paying for HTTP semantics
    we don't use. Dropping to `net.Conn` gives us full control—no pooling surprises,
    no hidden keep-alives, just a raw socket.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理内部二进制协议或每秒处理数十万请求的服务等情况下，我们可能会发现我们正在为不使用的 HTTP 语义付费。降低到 `net.Conn` 给我们提供了完全的控制权——没有池化惊喜，没有隐藏的保活，只是一个原始套接字。
- en: '[PRE11]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This lets us take over the connection lifecycle, buffering, and concurrency
    fully. It also opens up opportunities to reduce GC impact via buffer reuse:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这使我们能够完全接管连接生命周期、缓冲和并发。它还开辟了通过缓冲区重用减少 GC 影响的机会：
- en: '[PRE12]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Enabling TCP_NODELAY is useful in latency-sensitive systems:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在延迟敏感的系统中启用 TCP_NODELAY 很有用：
- en: '[PRE13]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Beyond TCP: Why UDP Matters[¶](#beyond-tcp-why-udp-matters "Permanent link")'
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超越 TCP：为什么 UDP 很重要[¶](#beyond-tcp-why-udp-matters "永久链接")
- en: 'TCP could be too heavy for workloads like log firehose ingestion, telemetry
    beacons, or heartbeat messages. We can turn to UDP for low-latency, connectionless
    data delivery:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 对于像日志火流摄取、遥测信标或心跳消息这样的工作负载，TCP 可能太重了。我们可以转向 UDP 以实现低延迟、无连接的数据传输：
- en: '[PRE14]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This skips handshakes and reuses the socket efficiently. But remember—UDP offers
    no ordering, reliability, or built-in session tracking. It works best in high-volume,
    low-consequence pipelines.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这跳过了握手并有效地重用了套接字。但请记住——UDP 不提供排序、可靠性或内置会话跟踪。它在高流量、低后果的管道中表现最佳。
- en: Choosing the Right Tool[¶](#choosing-the-right-tool "Permanent link")
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择合适的工具[¶](#choosing-the-right-tool "永久链接")
- en: 'Our networking strategy should reflect traffic shape and protocol expectations:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的网络策略应该反映流量形状和协议预期：
- en: '| Scenario | Preferred Tool |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 场景 | 推荐工具 |'
- en: '| --- | --- |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| REST/gRPC, general APIs | `net/http` |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| REST/gRPC，通用 API | `net/http` |'
- en: '| HTTP under load | Tuned `http.Transport` |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 高负载下的 HTTP | 调优的 `http.Transport` |'
- en: '| Custom TCP protocol | `net.Conn` |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 自定义 TCP 协议 | `net.Conn` |'
- en: '| Framed binary data | `net.Conn` + buffer mgmt |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 帧化二进制数据 | `net.Conn` + 缓冲区管理 |'
- en: '| Fire-and-forget telemetry | `UDPConn` |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 一触即发的遥测 | `UDPConn` |'
- en: '| Latency-sensitive game updates | `UDP` |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 延迟敏感的游戏更新 | `UDP` |'
- en: '* * *'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: At scale, network performance is never just about the network. It's also about
    memory pressure, context lifecycles, kernel behavior, and socket hygiene. We can
    go far with the Go standard library, but when systems push back, we need to push
    deeper.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在规模上，网络性能永远不仅仅是关于网络。它还关乎内存压力、上下文生命周期、内核行为和套接字卫生。我们可以利用 Go 标准库走得很远，但当系统反推时，我们需要深入挖掘。
- en: The good news? Go gives us the tools. We just need to use them wisely.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息？Go 给我们提供了工具。我们只需要明智地使用它们。
- en: If you're experimenting with framed protocols, zero-copy parsing, or custom
    benchmarking setups, there's a lot more to explore. Let's keep going.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在尝试帧协议、零拷贝解析或自定义基准测试设置，还有很多东西可以探索。让我们继续前进。
