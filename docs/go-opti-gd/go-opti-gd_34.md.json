["```go\nstateDiagram-v2\n    [*] --> Closed\n    Closed --> Open : errorRate > threshold\n    Open --> HalfOpen : resetTimeout expires\n    HalfOpen --> Closed : testSuccess >= threshold\n    HalfOpen --> Open : testFailure\n```", "```go\nflowchart TD\nsubgraph SlidingWindow [\"Sliding Window (last N intervals)\"]\n    B0((Bucket 0))\n    B1((Bucket 1))\n    B2((Bucket 2))\n    B3((Bucket 3))\n    B4((Bucket 4))\nend\n\nB0 -.-> Tick1[\"Tick(): move idx + reset bucket\"]\nTick1 --> B1\nB1 -.-> Tick2[\"Tick()\"]\nTick2 --> B2\nB2 -.-> Tick3[\"Tick()\"]\nTick3 --> B3\nB3 -.-> Tick4[\"Tick()\"]\nTick4 --> B4\nB4 -.-> Tick5[\"Tick()\"]\nTick5 --> B0\n\nB0 -.-> SumFailures[\"Sum all failures\"]\n\nSumFailures -->|Failures >= errorThreshold| OpenCircuit[\"Circuit Opens\"]\n\nOpenCircuit --> WaitReset[\"Wait resetTimeout\"]\nWaitReset --> HalfOpen[\"Move to Half-Open state\"]\n\nsubgraph HalfOpenPhase [\"Half-Open Phase\"]\n    TryCall1(\"Try Call 1\")\n    TryCall2(\"Try Call 2\")\n    TryCall3(\"Try Call 3\")\nend\n\nHalfOpen --> SuccessCheck[\"Check Successes\"]\nSuccessCheck -->|Enough successes| CloseCircuit[\"Circuit Closes\"]\nSuccessCheck -->|Failure during trial| ReopenCircuit[\"Circuit Re-Opens\"]\n\nReopenCircuit --> WaitReset\n```", "```go\ntype slidingWindow struct {\n    buckets []int32\n    size    int\n    idx     int\n    mu      sync.Mutex\n} \n```", "```go\nfunc (w *slidingWindow) Tick() {\n    w.mu.Lock()\n    defer w.mu.Unlock()\n    w.idx = (w.idx + 1) % w.size\n    atomic.StoreInt32(&w.buckets[w.idx], 0)\n} \n```", "```go\ntype CircuitState int32\n\nconst (\n    StateClosed CircuitState = iota\n    StateOpen\n    StateHalfOpen\n) \n```", "```go\ntype CircuitBreaker struct {\n    failures              *slidingWindow\n    errorThresh           int\n    successThresh         int32\n    interval              time.Duration\n    resetTimeout          time.Duration\n    halfOpenMaxConcurrent int32\n\n    state          CircuitState\n    lastOpen       time.Time\n    successes      int32\n    inFlightTrials int32\n} \n```", "```go\nfunc NewCircuitBreaker(errThresh int, succThresh int, interval, reset time.Duration, halfOpenMax int32) *CircuitBreaker {\n    cb := &CircuitBreaker{\n        failures:              newWindow(60),\n        errorThresh:           errThresh,\n        successThresh:         int32(succThresh),\n        interval:              interval,\n        resetTimeout:          reset,\n        halfOpenMaxConcurrent: halfOpenMax,\n    }\n    go func() {\n        ticker := time.NewTicker(interval)\n        for range ticker.C {\n            cb.failures.Tick()\n        }\n    }()\n    return cb\n} \n```", "```go\nfunc (cb *CircuitBreaker) Allow() bool {\n    switch atomic.LoadInt32((*int32)(&cb.state)) {\n    case int32(StateClosed):\n        return true\n    case int32(StateOpen):\n        if time.Since(cb.lastOpen) >= cb.resetTimeout {\n            atomic.StoreInt32((*int32)(&cb.state), int32(StateHalfOpen))\n            atomic.StoreInt32(&cb.successes, 0)\n            atomic.StoreInt32(&cb.inFlightTrials, 0)\n            return true\n        }\n        return false\n    case int32(StateHalfOpen):\n        if atomic.LoadInt32(&cb.inFlightTrials) >= cb.halfOpenMaxConcurrent {\n            return false\n        }\n        atomic.AddInt32(&cb.inFlightTrials, 1)\n        return true\n    }\n    return true\n} \n```", "```go\nfunc (cb *CircuitBreaker) Report(success bool) {\n    if !cb.Allow() {\n        return\n    }\n    defer func() {\n        if atomic.LoadInt32((*int32)(&cb.state)) == int32(StateHalfOpen) {\n            atomic.AddInt32(&cb.inFlightTrials, -1)\n        }\n    }()\n\n    switch atomic.LoadInt32((*int32)(&cb.state)) {\n    case int32(StateClosed):\n        if !success {\n            cb.failures.Inc()\n            if int(cb.failures.Sum()) >= cb.errorThresh {\n                atomic.StoreInt32((*int32)(&cb.state), int32(StateOpen))\n                cb.lastOpen = time.Now()\n            }\n        }\n\n    case int32(StateHalfOpen):\n        if success {\n            if atomic.AddInt32(&cb.successes, 1) >= cb.successThresh {\n                atomic.StoreInt32((*int32)(&cb.state), int32(StateClosed))\n            }\n        } else {\n            atomic.StoreInt32((*int32)(&cb.state), int32(StateOpen))\n            cb.lastOpen = time.Now()\n        }\n    }\n} \n```", "```go\nbreaker := NewCircuitBreaker(\n    10,              // open after 10 failures\n    5,               // close after 5 half-open successes\n    time.Second,     // tick every second\n    10*time.Second,  // remain open for 10 seconds\n    3,               // allow up to 3 trial calls\n)\n\nif breaker.Allow() {\n    success := callRemoteService()\n    breaker.Report(success)\n} \n```", "```go\nflowchart TD\n    A[Incoming Connection] --> B{Channel Full?}\n    B -- No --> C[Enqueue Request]\n    B -- Yes --> D[Drop Connection]\n```", "```go\n// A buffered channel of size N implements passive load shedding.\n// When full, new requests are silently dropped (connection closed).\nrequests := make(chan *Request, 1000)\n\n// acceptLoop continuously accepts new connections and enqueues them\n// if there is capacity; otherwise, it drops excess load immediately.\nfunc acceptLoop(ln net.Listener) {\n    for {\n        conn, err := ln.Accept()\n        if err != nil {\n            continue // transient accept error, skip\n        }\n        req := &Request{conn: conn}\n\n        select {\n        case requests <- req:\n            // Request accepted and queued for processing.\n        default:\n            // Channel full: drop request immediately to avoid overload.\n            conn.Close()\n        }\n    }\n} \n```", "```go\nflowchart TD\n    A[Incoming Request] --> B{CPU Load > Threshold?}\n    B -- Yes --> C[Reject Request]\n    B -- No --> D[Accept and Process]\n```", "```go\n// shedder monitors system CPU load and decides whether to shed incoming requests.\ntype shedder struct {\n    maxCPU    float64        // CPU usage threshold to start shedding\n    checkFreq time.Duration  // frequency to check CPU load\n}\n\n// ShouldShed checks current CPU usage against the configured maximum.\nfunc (s *shedder) ShouldShed() bool {\n    cpu := getCPULoad()\n    return cpu > s.maxCPU\n}\n\n// startMonitor periodically evaluates CPU load and updates the global shedding flag.\nfunc (s *shedder) startMonitor() {\n    ticker := time.NewTicker(s.checkFreq)\n    for range ticker.C {\n        if s.ShouldShed() {\n            atomic.StoreInt32(&shedding, 1) // enter shedding mode\n        } else {\n            atomic.StoreInt32(&shedding, 0) // exit shedding mode\n        }\n    }\n}\n\n// During request acceptance, the shedding flag is checked to actively reject overload.\nif atomic.LoadInt32(&shedding) == 1 {\n    conn.Close() // actively reject new connection\n} else {\n    enqueue(conn) // accept and process normally\n} \n```", "```go\nsequenceDiagram\n    participant Producer\n    participant Buffer\n    participant Consumer\n    Producer->>Buffer: Send Request\n    Buffer-->>Producer: Blocks if full\n    Buffer->>Consumer: Process Request\n```", "```go\n// requests is a buffered channel that provides natural backpressure.\n// When full, producers block until space becomes available.\nrequests := make(chan *Request, 500)\n\n// Producer loop reads incoming connections and enqueues them.\n// Blocks automatically when the channel is full, applying backpressure upstream.\nfor conn := range incomingConns {\n    req := &Request{conn: conn}\n    requests <- req // blocks when buffer reaches 500\n} \n```", "```go\nflowchart TD\n    A[Send Request] --> B{Timeout Exceeded?}\n    B -- No --> C[Enqueue in Channel]\n    B -- Yes --> D[Cancel or Drop Request]\n```", "```go\n// Set up a context with a strict timeout to bound enqueue latency.\nctx, cancel := context.WithTimeout(context.Background(), 50*time.Millisecond)\ndefer cancel()\n\n// Attempt to enqueue the request with timeout protection.\nselect {\ncase requests <- req:\n    // Request accepted into the processing queue.\ncase <-ctx.Done():\n    // Timeout exceeded before enqueue succeeded; drop or fallback.\n    req.conn.Close()\n} \n```", "```go\nflowchart TD\n    A[Incoming Requests] --> B{Buffer Usage High?}\n    B -- Yes --> C[Increase Buffer Size]\n    C --> D[Reconfigure Channel or Queue]\n    D --> E[Continue Processing]\n\n    B -- No --> F{Buffer Usage Low?}\n    F -- Yes --> G[Decrease Buffer Size]\n    G --> D\n\n    F -- No --> E\n```", "```go\n// DynamicBuffer wraps a buffered channel and automatically resizes it\n// based on usage thresholds. This enables better elasticity under varying load.\ntype DynamicBuffer struct {\n    mu        sync.Mutex\n    ch        chan Request     // underlying buffered channel\n    minSize   int              // minimum buffer capacity\n    maxSize   int              // maximum buffer capacity\n    growPct   float64          // grow if usage exceeds this fraction\n    shrinkPct float64          // shrink if usage falls below this fraction\n}\n\n// NewDynamicBuffer initializes a dynamic buffer with initial capacity and growth rules.\n// It also starts a background monitor that periodically evaluates whether resizing is needed.\nfunc NewDynamicBuffer(initial, min, max int, growPct, shrinkPct float64) *DynamicBuffer {\n    db := &DynamicBuffer{\n        ch:        make(chan Request, initial),\n        minSize:   min,\n        maxSize:   max,\n        growPct:   growPct,\n        shrinkPct: shrinkPct,\n    }\n    go db.monitor()\n    return db\n}\n\n// Enqueue adds a request into the channel.\n// If the channel is full, this call blocks until space is available.\nfunc (db *DynamicBuffer) Enqueue(req Request) {\n    db.mu.Lock()\n    ch := db.ch\n    db.mu.Unlock()\n\n    ch <- req\n}\n\n// Dequeue retrieves a request from the channel or aborts if the context expires.\n// This ensures consumers can cancel work if needed without hanging indefinitely.\nfunc (db *DynamicBuffer) Dequeue(ctx context.Context) (Request, bool) {\n    select {\n    case req := <-db.ch:\n        return req, true\n    case <-ctx.Done():\n        return Request{}, false\n    }\n}\n\n// monitor runs periodically, evaluating the channel's fill ratio,\n// and triggers resizing if usage crosses configured thresholds.\nfunc (db *DynamicBuffer) monitor() {\n    ticker := time.NewTicker(1 * time.Second)\n    for range ticker.C {\n        db.mu.Lock()\n\n        oldCh := db.ch\n        cap := cap(oldCh)\n        length := len(oldCh)\n        usage := float64(length) / float64(cap)\n\n        var newSize int\n        if usage > db.growPct && cap < db.maxSize {\n            // If heavily loaded, double the buffer size, but cap it at maxSize\n            newSize = min(db.maxSize, cap*2)\n        } else if usage < db.shrinkPct && cap > db.minSize {\n            // If lightly loaded, shrink the buffer to half, but not below minSize\n            newSize = max(db.minSize, cap/2)\n        }\n\n        if newSize != 0 {\n            // Create a new channel with the updated size and drain old requests into it\n            newCh := make(chan Request, newSize)\n            for len(oldCh) > 0 {\n                newCh <- <-oldCh\n            }\n            db.ch = newCh\n        }\n\n        db.mu.Unlock()\n    }\n} \n```", "```go\nflowchart TD\n    A[Request Received] --> B{Overloaded?}\n    B -- Yes --> C[Return 503 + Retry-After]\n    B -- No --> D[Process Request]\n```", "```go\n// This HTTP handler implements basic overload protection at the protocol level.\n// When the system is under pressure, it responds with a 503 and Retry-After header,\n// signaling clients to back off temporarily rather than retry aggressively.\n\nhttp.HandleFunc(\"/\", func(w http.ResponseWriter, r *http.Request) {\n    if isOverloaded() {\n        // Inform the client that the server is temporarily unavailable.\n        w.Header().Set(\"Retry-After\", \"5\") // suggest waiting 5 seconds before retrying\n        w.WriteHeader(http.StatusServiceUnavailable)\n        _, _ = w.Write([]byte(\"Service is temporarily overloaded. Please try again later.\"))\n        return\n    }\n\n    // Otherwise, proceed with request handling\n    process(r.Context(), w)\n}) \n```", "```go\nflowchart TD\n    A[Request Received] --> B{High Load?}\n    B -- Yes --> C[Return Degraded Response]\n    B -- No --> D[Return Full Response]\n```", "```go\nif highLoad() {\n    // degrade: return minimal response\n    w.Header().Set(\"Content-Type\", \"application/json\")\n    w.Write([]byte(`{\"data\":\"partial\"}`))\n    return\n} \n```"]