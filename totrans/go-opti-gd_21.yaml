- en: Go Networking Internals
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Go 网络内部结构
- en: 原文：[https://goperf.dev/02-networking/networking-internals/](https://goperf.dev/02-networking/networking-internals/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://goperf.dev/02-networking/networking-internals/](https://goperf.dev/02-networking/networking-internals/)
- en: 'Go’s networking model is deceptively simple on the surface—spawn a goroutine,
    accept a connection, read from it, and write a response. But behind this apparent
    ease is a highly optimized and finely tuned runtime that handles tens or hundreds
    of thousands of connections with minimal OS overhead. In this deep dive, we’ll
    walk through the mechanisms that make this possible: from goroutines and the scheduler
    to how Go interacts with OS-level pollers like `epoll`, `kqueue`, and IOCP.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Go 的网络模型表面上看似简单——启动一个 goroutine，接受连接，从中读取，并写入响应。但在这看似简单的背后，是一个高度优化和精细调校的运行时，它以最小的操作系统开销处理成千上万的连接。在这篇深入探讨中，我们将了解使这一切成为可能机制：从
    goroutine 和调度器到 Go 如何与操作系统级别的轮询器（如 `epoll`、`kqueue` 和 IOCP）交互。
- en: Goroutines and the Runtime Scheduler
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Goroutines 和运行时调度器
- en: Goroutines are lightweight user-space threads managed by the Go runtime. They’re
    cheap to create (a few kilobytes of stack) and can scale to millions. But they’re
    not magic—they rely on the runtime scheduler to multiplex execution across a limited
    number of OS threads.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Goroutines 是由 Go 运行时管理的轻量级用户空间线程。它们易于创建（只有几KB的堆栈），可以扩展到数百万。但它们并非魔法——它们依赖于运行时调度器来在有限的操作系统线程之间多路复用执行。
- en: 'Go’s scheduler is based on an M:N model:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Go 的调度器基于 M:N 模型：
- en: '**M (Machine)**: Represents an OS thread.'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**M (Machine)**：表示一个操作系统线程。'
- en: '**G (Goroutine)**: Represents the actual task or coroutine.'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**G (Goroutine)**：表示实际的任务或协程。'
- en: '**P (Processor)**: Represents the context for scheduling (holding run queues,
    caches).'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**P (Processor)**：表示调度上下文（持有运行队列、缓存）。'
- en: Each P can execute one G at a time using an M. There are as many Ps as GOMAXPROCS.
    If a goroutine blocks on I/O, another runnable G may park and reuse the thread.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 P 可以使用一个 M 来执行一个 G。P 的数量与 GOMAXPROCS 相同。如果一个 goroutine 在 I/O 上阻塞，另一个可运行的
    G 可以暂停并重用线程。
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Blocking I/O in Goroutines: What Really Happens?'
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Goroutine 中的阻塞 I/O：实际上发生了什么？
- en: Suppose a goroutine calls `conn.Read()`. This *looks* blocking—but only from
    the goroutine's perspective. Internally, Go’s runtime intercepts the call and
    uses a mechanism known as the [netpoller](https://go.dev/src/runtime/netpoll.go).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一个 goroutine 调用 `conn.Read()`。这看起来像是阻塞的——但仅从 goroutine 的角度来看。内部，Go 的运行时拦截调用并使用一种称为
    [netpoller](https://go.dev/src/runtime/netpoll.go) 的机制。
- en: 'On Unix-based systems, Go uses readiness-based polling (`epoll` on Linux, `kqueue`
    on macOS/BSD). When a goroutine performs a syscall like `read(fd)`, the runtime
    checks whether the file descriptor is ready. If not:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于 Unix 的系统上，Go 使用基于就绪的轮询（Linux 上的 `epoll`，macOS/BSD 上的 `kqueue`）。当一个 goroutine
    执行 `read(fd)` 这样的系统调用时，运行时会检查文件描述符是否就绪。如果不是：
- en: The goroutine is parked.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: goroutine 被挂起。
- en: The file descriptor is registered with the poller.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 文件描述符已注册到轮询器。
- en: The OS thread is released to run other work.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 操作系统线程被释放以运行其他工作。
- en: When the fd becomes ready, the poller wakes up, and the runtime marks the goroutine
    as runnable.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当 fd 准备就绪时，轮询器唤醒，运行时将 goroutine 标记为可运行。
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This system enables Go to serve a massive number of clients concurrently, using
    a small number of threads, avoiding the overhead of traditional thread-per-connection
    models.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 该系统使 Go 能够使用少量线程并发地为大量客户端提供服务，避免了传统线程-连接模型的开销。
- en: Internals of the `net` Package
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '`net` 包的内部结构'
- en: Let’s take a look at what happens behind `net.Listen("tcp", ":8080")` and `conn.Read()`.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 `net.Listen("tcp", ":8080")` 和 `conn.Read()` 背后发生了什么。
- en: '`net.Listen` calls into `net.ListenTCP`, which constructs a `netFD` struct
    wrapping the socket.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`net.Listen` 调用 `net.ListenTCP`，它构建了一个包含套接字的 `netFD` 结构体。'
- en: The socket is marked non-blocking via `syscall.SetNonblock(fd, true)`.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过 `syscall.SetNonblock(fd, true)` 将套接字标记为非阻塞。
- en: '`Accept` and `Read` methods on `netFD` are layered on top of syscalls, but
    routed through internal pollers and wrapped with logic to yield and resume goroutines.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`netFD` 上的 `Accept` 和 `Read` 方法建立在系统调用之上，但通过内部轮询器路由，并用逻辑包装以产生和恢复 goroutine。'
- en: 'Here’s a rough diagram of the call chain:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一张调用链的大致图：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This architecture makes the blocking calls from the developer’s perspective
    translate into non-blocking interactions with the kernel.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这种架构使得从开发者的角度来看，阻塞调用转换为与内核的非阻塞交互。
- en: 'The Netpoller: Polling with Epoll/Kqueue/IOCP'
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Netpoller：使用 Epoll/Kqueue/IOCP 进行轮询
- en: The **netpoller** is a runtime subsystem that integrates low-level polling mechanisms
    with Go’s scheduling system. Each fd has an associated `pollDesc`, which helps
    coordinate goroutine suspension and resumption.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**netpoller** 是一个运行时子系统，它将低级轮询机制与 Go 的调度系统集成。每个 fd 都有一个关联的 `pollDesc`，这有助于协调
    goroutine 的挂起和恢复。'
- en: 'The poller operates in a dedicated thread (or threads) that loop over OS wait
    primitives:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: poller 在一个专用线程（或线程）中运行，该线程循环遍历 OS 等待原语：
- en: '[epoll_wait](https://man7.org/linux/man-pages/man2/epoll_wait.2.html) (Linux)'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[epoll_wait](https://man7.org/linux/man-pages/man2/epoll_wait.2.html) (Linux)'
- en: '[kqueue](https://en.wikipedia.org/wiki/Kqueue) (macOS/BSD)'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[kqueue](https://en.wikipedia.org/wiki/Kqueue) (macOS/BSD)'
- en: '[IOCP](https://learn.microsoft.com/en-gb/windows/win32/fileio/i-o-completion-ports)
    (Windows)'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[IOCP](https://learn.microsoft.com/en-gb/windows/win32/fileio/i-o-completion-ports)
    (Windows)'
- en: When an I/O event fires, the poller finds the associated `pollDesc`, identifies
    the parked goroutine, and puts it back into the run queue.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 当 I/O 事件触发时，poller 找到相关的 `pollDesc`，识别已停泊的 goroutine，并将其放回运行队列。
- en: 'In the Go source, relevant files include:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Go 源代码中，相关的文件包括：
- en: '[runtime/netpoll_epoll.go](https://go.dev/src/runtime/netpoll_epoll.go)'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[runtime/netpoll_epoll.go](https://go.dev/src/runtime/netpoll_epoll.go)'
- en: '[runtime/netpoll_kqueue.go](https://go.dev/src/runtime/netpoll_kqueue.go)'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[runtime/netpoll_kqueue.go](https://go.dev/src/runtime/netpoll_kqueue.go)'
- en: '[runtime/netpoll_windows.go](https://go.dev/src/runtime/netpoll_windows.go)'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[runtime/netpoll_windows.go](https://go.dev/src/runtime/netpoll_windows.go)'
- en: 'The Go poller is readiness-based (not completion-based, except for Windows
    IOCP). It handles:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Go 的 poller 是基于就绪状态的（不是基于完成状态的，除了 Windows IOCP）。它处理：
- en: fd registration
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: fd 注册
- en: waking goroutines on readiness
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在就绪时唤醒 goroutine
- en: integration with the run queue (P-local or global)
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与运行队列（P-局部或全局）集成
- en: 'Example: High-Performance TCP Echo Server'
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例：高性能 TCP Echo 服务器
- en: Let's break down a simple Go TCP echo server and map each part to Go’s internal
    networking and scheduling mechanisms — including `netFD`, `poll.FD`, and goroutines.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分解一个简单的 Go TCP echo 服务器，并将每个部分映射到 Go 的内部网络和调度机制——包括 `netFD`、`poll.FD` 和 goroutines。
- en: <details class="example"><summary>Simple Echo server source code</summary>
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: <details class="example"><summary>简单的 Echo 服务器源代码</summary>
- en: '[PRE3]</details>'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE3]</details>'
- en: Imports and Setup
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 导入和设置
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**Internals Involved**:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**涉及内部**:'
- en: The `net` package abstracts system-level networking.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`net` 包抽象了系统级网络。'
- en: 'Under the hood:'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在底层：
- en: Uses `netFD` (internal, private struct)
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `netFD`（内部，私有结构）
- en: Wraps `poll.FD` for non-blocking I/O
  id: totrans-53
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包装 `poll.FD` 以实现非阻塞 I/O
- en: Uses OS features like `epoll`, `kqueue`, or `IOCP` for event notification
  id: totrans-54
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用操作系统功能如 `epoll`、`kqueue` 或 `IOCP` 进行事件通知
- en: Listener Setup
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 监听器设置
- en: '[PRE5]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '**Internals Involved**:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**涉及内部**:'
- en: '`net.Listen()` returns a `TCPListener`'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`net.Listen()` 返回一个 `TCPListener`'
- en: Internally calls `syscall.socket`, `bind`, `listen`
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内部调用 `syscall.socket`、`bind`、`listen`
- en: Associates a `netFD` with the socket
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 `netFD` 与套接字关联
- en: The listener uses Go’s internal poller to enable non-blocking `Accept`
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监听器使用 Go 的内部 poller 来启用非阻塞 `Accept`
- en: Accept Loop and Goroutine Scheduling
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 接受循环和 goroutine 调度
- en: '[PRE6]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**Internals Involved**:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**涉及内部**:'
- en: '`listener.Accept()` → `netFD.Accept()` → `poll.FD.Accept()` → `syscall.accept`'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`listener.Accept()` → `netFD.Accept()` → `poll.FD.Accept()` → `syscall.accept`'
- en: Non-blocking, waits via Go's poller (`runtime_pollWait`)
  id: totrans-66
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非阻塞，通过 Go 的 poller (`runtime_pollWait`) 等待
- en: '`go handle(conn)` spawns a **goroutine (G)**'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`go handle(conn)` 启动一个 **goroutine (G**)'
- en: Scheduled onto a **P** (Processor)
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调度到 **P**（处理器）
- en: '`P` is part of Go’s M:N scheduler governed by `GOMAXPROCS`'
  id: totrans-69
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`P` 是 Go 的 M:N 调度器的一部分，由 `GOMAXPROCS` 管理'
- en: Connection Handler
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 连接处理器
- en: '[PRE7]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**Internals Involved**:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**涉及内部**:'
- en: '`bufio.NewReader(conn)` wraps the `net.Conn`, which is backed by `*TCPConn`
    and `netFD`.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bufio.NewReader(conn)` 包装 `net.Conn`，它由 `*TCPConn` 和 `netFD` 支持。'
- en: '`ReadString()` calls `conn.Read()` under the hood:'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ReadString()` 在底层调用 `conn.Read()`：'
- en: '`netFD.Read()` → `poll.FD.Read()` → `syscall.Read()`'
  id: totrans-75
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`netFD.Read()` → `poll.FD.Read()` → `syscall.Read()`'
- en: Uses `runtime_pollWait` to yield the goroutine if data isn't ready
  id: totrans-76
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `runtime_pollWait` 在数据未就绪时让出 goroutine
- en: '`SetReadDeadline` sets a timeout by integrating with the runtime''s network
    poller to prevent indefinite blocking.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SetReadDeadline` 通过与运行时的网络 poller 集成来设置超时，以防止无限期阻塞。'
- en: '`conn.Write()` → `netFD.Write()` → `poll.FD.Write()` → `syscall.write`'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conn.Write()` → `netFD.Write()` → `poll.FD.Write()` → `syscall.write`'
- en: Internal Flow Diagram
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 内部流程图
- en: '[PRE8]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This model scales well as long as you:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 只要满足以下条件，此模型就可以很好地扩展：
- en: Ensure your `ulimit -n` is high enough
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保你的 `ulimit -n` 足够高
- en: Avoid shared state and contention
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 避免共享状态和竞争
- en: Tune your GOMAXPROCS for your workload
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为你的工作负载调整 GOMAXPROCS
- en: Observations at Scale
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 规模观察
- en: 'As connections scale up ([see how it may look like here](../gc-endpoint-profiling/)):'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 随着连接数的增加（[在此处查看可能的样子](../gc-endpoint-profiling/))：
- en: Per-connection memory and GC pressure grow
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个连接的内存和垃圾回收压力增加
- en: Frequent goroutine context switching may introduce latency
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 频繁的goroutine上下文切换可能会引入延迟
- en: Coordinating channels, timeouts, and backpressure adds complexity
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 协调通道、超时和背压增加了复杂性
- en: 'Some mitigation strategies:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 一些缓解策略：
- en: Use `sync.Pool` for buffer reuse
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `sync.Pool` 进行缓冲区重用
- en: Minimize GC pauses (avoid per-request allocations)
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最小化垃圾回收暂停（避免每个请求的分配）
- en: Prefer `netpoll`-friendly designs (avoid long CPU-bound goroutines)
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优先选择对 `netpoll` 友好的设计（避免长时间占用CPU的goroutine）
- en: '* * *'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Go’s model trades OS-level multiplexing for user-space scheduling and event-driven
    I/O coordination. It’s not a silver bullet—but when used correctly, it offers
    a robust platform for building scalable network services. Understanding these
    internals helps you avoid common traps, optimize at the right layer, and build
    systems that behave predictably under load.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Go的模型以用户空间调度和事件驱动I/O协调来交换OS级别的多路复用。它不是万能的银弹——但使用得当，它提供了一个构建可扩展网络服务的强大平台。理解这些内部机制有助于你避免常见陷阱，在正确的层级进行优化，并构建在负载下表现可预测的系统。
