<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Managing 10K+ Concurrent Connections in Go</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Managing 10K+ Concurrent Connections in Go</h1>
<blockquote>原文：<a href="https://goperf.dev/02-networking/10k-connections/">https://goperf.dev/02-networking/10k-connections/</a></blockquote>
                
                  


  
  



<details class="info">
<summary>Why not 100K+ or 1 Mill connection?</summary>
<p>While framing the challenge in terms of “100K concurrent connections” is tempting, practical engineering often begins with a more grounded target: 10K to 20K stable, performant connections. This isn’t a limitation of Go itself but a reflection of real-world constraints: ulimit settings, ephemeral port availability, TCP stack configuration, and the nature of the application workload all set hard boundaries.</p>
<p>Cloud environments introduce their own considerations. For instance, AWS Fargate explicitly sets both the soft and hard nofile (number of open files) limit to 65,535, which provides more headroom for socket-intensive applications but still falls short of the 100K+ threshold. On EC2 instances, the practical limits depend on the base operating system and user configuration. By default, many Linux distributions impose a soft limit of 1024 and a hard limit of 65535 for nofile. Even this hard cap is lower than required to handle 100,000 open connections in a single process. Reaching higher limits requires kernel-level tuning, container runtime overrides, and multi-process strategies to distribute file descriptor load.</p>
<p>A server handling simple echo logic behaves very differently from one performing CPU-bound processing, structured logging, or real-time transformation. Additionally, platform-level tunability varies—Linux exposes granular control through sysctl, epoll, and reuseport, while macOS lacks many of these mechanisms. In that context, achieving and sustaining 10K+ concurrent connections with real workloads is a demanding, yet practical, benchmark.</p>
</details>
<p>Handling massive concurrency in Go is often romanticized—<em>"goroutines are cheap, just spawn them!"</em>—but reality gets harsher as we push towards six-digit concurrency levels. Serving over 10,000 concurrent sockets isn’t something you solve by scaling hardware alone—it requires an architecture that works with the OS, the Go runtime, and the network stack, not against them.</p>
<h2 id="embracing-gos-concurrency-model">Embracing Go’s Concurrency Model</h2>
<p>Go’s lightweight goroutines and its powerful runtime scheduler make it an excellent choice for scaling network applications. Goroutines consume only a few kilobytes of stack space, which, in theory, makes them ideal for handling tens of thousands of concurrent connections. However, reality forces us to think beyond just spinning up goroutines. While the language’s abstraction makes concurrency almost “magical,” achieving true efficiency at this scale demands intentional design.</p>
<p>Running a server that spawns one goroutine per connection means you’re leaning heavily on the runtime scheduler to juggle thousands of concurrent execution paths. While goroutines are lightweight, they’re not free—each one adds to memory consumption and introduces scheduling overhead that scales with concurrency. Thus, the first design pattern that should be adopted is to ensure that each connection follows a clearly defined lifecycle and that every goroutine performs its task as efficiently as possible.</p>
<p>Let’s consider a basic model where we accept connections and delegate their handling to separate goroutines:</p>
<div class="highlight"><pre><span/><code><span class="kn">package</span><span class="w"> </span><span class="nx">main</span>

<span class="kn">import</span><span class="w"> </span><span class="p">(</span>
<span class="w">    </span><span class="s">"log"</span>
<span class="w">    </span><span class="s">"net"</span>
<span class="w">    </span><span class="s">"sync/atomic"</span>
<span class="w">    </span><span class="s">"time"</span>
<span class="p">)</span>

<span class="kd">var</span><span class="w"> </span><span class="nx">activeConnections</span><span class="w"> </span><span class="kt">uint64</span>

<span class="kd">func</span><span class="w"> </span><span class="nx">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="nx">listener</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">net</span><span class="p">.</span><span class="nx">Listen</span><span class="p">(</span><span class="s">"tcp"</span><span class="p">,</span><span class="w"> </span><span class="s">":8080"</span><span class="p">)</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nx">log</span><span class="p">.</span><span class="nx">Fatalf</span><span class="p">(</span><span class="s">"Error starting TCP listener: %v"</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="p">)</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="k">defer</span><span class="w"> </span><span class="nx">listener</span><span class="p">.</span><span class="nx">Close</span><span class="p">()</span>

<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nx">conn</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">listener</span><span class="p">.</span><span class="nx">Accept</span><span class="p">()</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="nx">log</span><span class="p">.</span><span class="nx">Printf</span><span class="p">(</span><span class="s">"Error accepting connection: %v"</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="p">)</span>
<span class="w">            </span><span class="k">continue</span>
<span class="w">        </span><span class="p">}</span>

<span class="w">        </span><span class="nx">atomic</span><span class="p">.</span><span class="nx">AddUint64</span><span class="p">(</span><span class="o">&amp;</span><span class="nx">activeConnections</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span>
<span class="w">        </span><span class="k">go</span><span class="w"> </span><span class="nx">handleConnection</span><span class="p">(</span><span class="nx">conn</span><span class="p">)</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>

<span class="kd">func</span><span class="w"> </span><span class="nx">handleConnection</span><span class="p">(</span><span class="nx">conn</span><span class="w"> </span><span class="nx">net</span><span class="p">.</span><span class="nx">Conn</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">defer</span><span class="w"> </span><span class="kd">func</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nx">conn</span><span class="p">.</span><span class="nx">Close</span><span class="p">()</span>
<span class="w">        </span><span class="nx">atomic</span><span class="p">.</span><span class="nx">AddUint64</span><span class="p">(</span><span class="o">&amp;</span><span class="nx">activeConnections</span><span class="p">,</span><span class="w"> </span><span class="p">^</span><span class="nb">uint64</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span><span class="w"> </span><span class="c1">// effectively decrements the counter</span>
<span class="w">    </span><span class="p">}()</span>

<span class="w">    </span><span class="c1">// Imagine complex processing here—an echo server example:</span>
<span class="w">    </span><span class="nx">buffer</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nb">make</span><span class="p">([]</span><span class="kt">byte</span><span class="p">,</span><span class="w"> </span><span class="mi">1024</span><span class="p">)</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nx">conn</span><span class="p">.</span><span class="nx">SetDeadline</span><span class="p">(</span><span class="nx">time</span><span class="p">.</span><span class="nx">Now</span><span class="p">().</span><span class="nx">Add</span><span class="p">(</span><span class="mi">30</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nx">time</span><span class="p">.</span><span class="nx">Second</span><span class="p">))</span><span class="w"> </span><span class="c1">// prevent idle hangs</span>
<span class="w">        </span><span class="nx">n</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">conn</span><span class="p">.</span><span class="nx">Read</span><span class="p">(</span><span class="nx">buffer</span><span class="p">)</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="nx">log</span><span class="p">.</span><span class="nx">Printf</span><span class="p">(</span><span class="s">"Connection read error: %v"</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="p">)</span>
<span class="w">            </span><span class="k">return</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">        </span><span class="nx">_</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">conn</span><span class="p">.</span><span class="nx">Write</span><span class="p">(</span><span class="nx">buffer</span><span class="p">[:</span><span class="nx">n</span><span class="p">])</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="nx">log</span><span class="p">.</span><span class="nx">Printf</span><span class="p">(</span><span class="s">"Connection write error: %v"</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="p">)</span>
<span class="w">            </span><span class="k">return</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div>
<p>Each connection is assigned its own goroutine. That approach works fine at low concurrency and fits Go’s model well. But once you’re dealing with tens of thousands of connections, the design has to account for system limits. Goroutines are cheap—but not free.</p>
<h3 id="managing-concurrency-at-scale">Managing Concurrency at Scale</h3>
<p>It’s not enough to just accept connections; you need to control what happens after. Unbounded goroutine creation leads to memory growth and increased scheduler load. To keep the system stable, concurrency must be capped—typically using a semaphore or similar construct to limit how many goroutines handle active work at any given time.</p>
<p>For example, you might limit the number of simultaneous active connections before spinning up a new goroutine for each incoming connection. This strategy might involve a buffered channel acting as a semaphore:</p>
<div class="highlight"><pre><span/><code><span class="kn">package</span><span class="w"> </span><span class="nx">main</span>

<span class="kn">import</span><span class="w"> </span><span class="p">(</span>
<span class="w">    </span><span class="s">"net"</span>
<span class="p">)</span>

<span class="kd">var</span><span class="w"> </span><span class="nx">connLimiter</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nb">make</span><span class="p">(</span><span class="kd">chan</span><span class="w"> </span><span class="kd">struct</span><span class="p">{},</span><span class="w"> </span><span class="mi">10000</span><span class="p">)</span><span class="w"> </span><span class="c1">// Max 10K concurrent conns</span>

<span class="kd">func</span><span class="w"> </span><span class="nx">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="nx">ln</span><span class="p">,</span><span class="w"> </span><span class="nx">_</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">net</span><span class="p">.</span><span class="nx">Listen</span><span class="p">(</span><span class="s">"tcp"</span><span class="p">,</span><span class="w"> </span><span class="s">":8080"</span><span class="p">)</span>
<span class="w">    </span><span class="k">defer</span><span class="w"> </span><span class="nx">ln</span><span class="p">.</span><span class="nx">Close</span><span class="p">()</span>

<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nx">conn</span><span class="p">,</span><span class="w"> </span><span class="nx">_</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">ln</span><span class="p">.</span><span class="nx">Accept</span><span class="p">()</span>

<span class="w">        </span><span class="nx">connLimiter</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="kd">struct</span><span class="p">{}{}</span><span class="w"> </span><span class="c1">// Acquire slot</span>
<span class="w">        </span><span class="k">go</span><span class="w"> </span><span class="kd">func</span><span class="p">(</span><span class="nx">c</span><span class="w"> </span><span class="nx">net</span><span class="p">.</span><span class="nx">Conn</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="k">defer</span><span class="w"> </span><span class="kd">func</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">                </span><span class="nx">c</span><span class="p">.</span><span class="nx">Close</span><span class="p">()</span>
<span class="w">                </span><span class="o">&lt;-</span><span class="nx">connLimiter</span><span class="w"> </span><span class="c1">// Release slot</span>
<span class="w">            </span><span class="p">}()</span>
<span class="w">            </span><span class="c1">// Dummy echo logic</span>
<span class="w">            </span><span class="nx">buf</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nb">make</span><span class="p">([]</span><span class="kt">byte</span><span class="p">,</span><span class="w"> </span><span class="mi">1024</span><span class="p">)</span>
<span class="w">            </span><span class="nx">c</span><span class="p">.</span><span class="nx">Read</span><span class="p">(</span><span class="nx">buf</span><span class="p">)</span>
<span class="w">            </span><span class="nx">c</span><span class="p">.</span><span class="nx">Write</span><span class="p">(</span><span class="nx">buf</span><span class="p">)</span>
<span class="w">        </span><span class="p">}(</span><span class="nx">conn</span><span class="p">)</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div>
<p>This pattern not only helps prevent resource exhaustion but also gracefully degrades service under high load. Adjusting these limits according to your hardware and workload characteristics is a continuous tuning process.</p>
<div class="admonition info">
<p class="admonition-title">Info</p>
<p>We use the <code>connLimiter</code> approach here for purely illustrative purposes, as it clarifies the idea. In real life, you will most likely use <a href="https://pkg.go.dev/golang.org/x/sync/errgroup">errgroup</a> to manage the goroutines amount and some <code>SIGINT,</code> and <code>SIGTERM</code> signal handling for graceful process termination.</p>
</div>
<h3 id="os-level-and-socket-tuning">OS-Level and Socket Tuning</h3>
<p>Before your Go application can handle more than 10,000 simultaneous connections, the operating system has to be prepared for that scale. On Linux, this usually starts with raising the limit on open file descriptors. The TCP stack also needs tuning—default settings often aren’t designed for high-connection workloads. Without these adjustments, the application will hit OS-level ceilings long before Go becomes the bottleneck.</p>
<div class="highlight"><pre><span/><code><span class="err">#</span><span class="w"> </span><span class="nx">Increase</span><span class="w"> </span><span class="nx">file</span><span class="w"> </span><span class="nx">descriptor</span><span class="w"> </span><span class="nx">limit</span>
<span class="nx">ulimit</span><span class="w"> </span><span class="o">-</span><span class="nx">n</span><span class="w"> </span><span class="mi">200000</span>
</code></pre></div>
<p>But it doesn’t stop there. You’ll also need:</p>
<div class="highlight"><pre><span/><code>sysctl<span class="w"> </span>-w<span class="w"> </span>net.core.somaxconn<span class="o">=</span><span class="m">65535</span>
sysctl<span class="w"> </span>-w<span class="w"> </span>net.ipv4.ip_local_port_range<span class="o">=</span><span class="s2">"10000 65535"</span>
sysctl<span class="w"> </span>-w<span class="w"> </span>net.ipv4.tcp_tw_reuse<span class="o">=</span><span class="m">1</span>
sysctl<span class="w"> </span>-w<span class="w"> </span>net.ipv4.tcp_fin_timeout<span class="o">=</span><span class="m">15</span>
</code></pre></div>
<ul>
<li><code>net.core.somaxconn=65535</code>: This controls the size of the pending connection queue (the backlog) for listening sockets. A small value here will cause connection drops when many clients attempt to connect simultaneously.</li>
<li><code>net.ipv4.ip_local_port_range="10000 65535"</code>: Defines the ephemeral port range used for outbound connections. A wider range prevents port exhaustion when you’re making many outbound connections from the same machine.</li>
<li><code>net.ipv4.tcp_tw_reuse=1</code>: Allows reuse of sockets in <code>TIME_WAIT</code> state for new connections if safe. Helps reduce socket exhaustion, especially in short-lived TCP connections.</li>
<li><code>net.ipv4.tcp_fin_timeout=15</code>: Reduces the time the kernel holds sockets in <code>FIN_WAIT2</code> after a connection is closed. Shorter timeout means faster resource reclamation, crucial when thousands of sockets churn per minute.</li>
</ul>
<p>Tuning these parameters helps prevent the OS from becoming the bottleneck as connection counts grow. On top of that, setting socket options like <code>TCP_NODELAY</code> can reduce latency by disabling <a href="https://en.wikipedia.org/wiki/Nagle%27s_algorithm">Nagle’s algorithm</a>, which buffers small packets by default. In Go, these options can be applied through the net package, or more directly via the syscall package if lower-level control is needed.</p>
<p>In some cases, using Go’s <code>net.ListenConfig</code> allows you to inject custom control over socket creation. This is particularly useful when you need to set options at the time of listener creation:</p>
<div class="highlight"><pre><span/><code><span class="kd">func</span><span class="w"> </span><span class="nx">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="nx">lc</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">net</span><span class="p">.</span><span class="nx">ListenConfig</span><span class="p">{</span>
<span class="w">        </span><span class="nx">Control</span><span class="p">:</span><span class="w"> </span><span class="kd">func</span><span class="p">(</span><span class="nx">network</span><span class="p">,</span><span class="w"> </span><span class="nx">address</span><span class="w"> </span><span class="kt">string</span><span class="p">,</span><span class="w"> </span><span class="nx">c</span><span class="w"> </span><span class="nx">syscall</span><span class="p">.</span><span class="nx">RawConn</span><span class="p">)</span><span class="w"> </span><span class="kt">error</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="kd">var</span><span class="w"> </span><span class="nx">controlErr</span><span class="w"> </span><span class="kt">error</span>
<span class="w">            </span><span class="nx">err</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">c</span><span class="p">.</span><span class="nx">Control</span><span class="p">(</span><span class="kd">func</span><span class="p">(</span><span class="nx">fd</span><span class="w"> </span><span class="kt">uintptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">                </span><span class="c1">// Enable TCP_NODELAY on the socket</span>
<span class="w">                </span><span class="nx">controlErr</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">syscall</span><span class="p">.</span><span class="nx">SetsockoptInt</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="nx">fd</span><span class="p">),</span><span class="w"> </span><span class="nx">syscall</span><span class="p">.</span><span class="nx">IPPROTO_TCP</span><span class="p">,</span><span class="w"> </span><span class="nx">syscall</span><span class="p">.</span><span class="nx">TCP_NODELAY</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span>
<span class="w">            </span><span class="p">})</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span>
<span class="w">                </span><span class="k">return</span><span class="w"> </span><span class="nx">err</span>
<span class="w">            </span><span class="p">}</span>
<span class="w">            </span><span class="k">return</span><span class="w"> </span><span class="nx">controlErr</span>
<span class="w">        </span><span class="p">},</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="nx">listener</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">lc</span><span class="p">.</span><span class="nx">Listen</span><span class="p">(</span><span class="nx">context</span><span class="p">.</span><span class="nx">Background</span><span class="p">(),</span><span class="w"> </span><span class="s">"tcp"</span><span class="p">,</span><span class="w"> </span><span class="s">":8080"</span><span class="p">)</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nx">log</span><span class="p">.</span><span class="nx">Fatalf</span><span class="p">(</span><span class="s">"Error creating listener: %v"</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="p">)</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="k">defer</span><span class="w"> </span><span class="nx">listener</span><span class="p">.</span><span class="nx">Close</span><span class="p">()</span>
<span class="w">    </span><span class="c1">// Accept connections in a loop…</span>
<span class="p">}</span>
</code></pre></div>
<h3 id="go-scheduler-and-memory-pressure">Go Scheduler and Memory Pressure</h3>
<p>Spawning 10,000 goroutines might look impressive on paper, but what matters is how those goroutines behave. If they’re mostly idle—blocked on I/O like network or disk—Go’s scheduler handles them efficiently, parking and resuming with little overhead. But when goroutines actively allocate memory, spin in tight loops, or constantly contend on channels and mutexes, things get expensive. You’ll start to see increased garbage collection pressure and scheduler thrashing, both of which erode performance.</p>
<p>Go’s garbage collector handles short-lived allocations well, but it doesn’t come for free. If you’re spawning goroutines that churn through memory—allocating per request, per message, or worse, per loop—GC pressure builds fast. The result isn’t just more frequent collections, but higher latency and lost CPU cycles. Throughput drops, and the system spends more time cleaning up than doing real work.</p>
<p>To manage this, you can explicitly tune the GC aggressiveness:</p>
<div class="highlight"><pre><span/><code><span class="nv">GOGC</span><span class="o">=</span><span class="m">50</span>
</code></pre></div>
<p>Or directly within your codebase:</p>
<div class="highlight"><pre><span/><code><span class="kn">import</span><span class="w"> </span><span class="s">"runtime/debug"</span>

<span class="kd">func</span><span class="w"> </span><span class="nx">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="nx">debug</span><span class="p">.</span><span class="nx">SetGCPercent</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span>
<span class="w">    </span><span class="c1">// rest of your application logic</span>
<span class="p">}</span>
</code></pre></div>
<p>The default value for <code>GOGC</code> is 100, meaning the GC triggers when the heap size doubles compared to the previous GC cycle. Lower values (like 50) mean more frequent but shorter GC cycles, helping control memory growth at the cost of increased CPU overhead.</p>
<div class="admonition info">
<p class="admonition-title">Info</p>
<p>In some cases, you may need an opposite – <a href="../../01-common-patterns/gc/#gc-tuning-gogc">to increase the <code>GOGC</code> value, turn the GC off completely</a>, or prefer <a href="../../01-common-patterns/gc/#gomemlimitx-and-gogcoff-configuration">GOMEMLIMIT=X and GOGC=off</a> configuration. <strong>Do not make a decision before careful profiling!</strong></p>
</div>
<h3 id="optimizing-goroutine-behavior">Optimizing Goroutine Behavior</h3>
<p>Consider structuring your application so that goroutines block naturally rather than actively waiting or spinning. For example, instead of polling channels in tight loops, use select statements efficiently:</p>
<div class="highlight"><pre><span/><code><span class="k">for</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">select</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">case</span><span class="w"> </span><span class="nx">msg</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="o">&lt;-</span><span class="nx">msgChan</span><span class="p">:</span>
<span class="w">        </span><span class="nx">handleMsg</span><span class="p">(</span><span class="nx">msg</span><span class="p">)</span>
<span class="w">    </span><span class="k">case</span><span class="w"> </span><span class="o">&lt;-</span><span class="nx">ctx</span><span class="p">.</span><span class="nx">Done</span><span class="p">():</span>
<span class="w">        </span><span class="k">return</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div>
<p>If your goroutines must wait, prefer blocking on channels or synchronization primitives provided by Go, like mutexes or condition variables, instead of actively polling.</p>
<h3 id="pooling-and-reusing-objects">Pooling and Reusing Objects</h3>
<p>Another crucial technique to reduce memory allocations and GC overhead <a href="../../01-common-patterns/object-pooling/">is using <code>sync.Pool</code></a>:</p>
<div class="highlight"><pre><span/><code><span class="kd">var</span><span class="w"> </span><span class="nx">bufPool</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">sync</span><span class="p">.</span><span class="nx">Pool</span><span class="p">{</span>
<span class="w">    </span><span class="nx">New</span><span class="p">:</span><span class="w"> </span><span class="kd">func</span><span class="p">()</span><span class="w"> </span><span class="kt">any</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="nb">make</span><span class="p">([]</span><span class="kt">byte</span><span class="p">,</span><span class="w"> </span><span class="mi">1024</span><span class="p">)</span><span class="w"> </span><span class="p">},</span>
<span class="p">}</span>

<span class="kd">func</span><span class="w"> </span><span class="nx">handleRequest</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="nx">buf</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">bufPool</span><span class="p">.</span><span class="nx">Get</span><span class="p">().([]</span><span class="kt">byte</span><span class="p">)</span>
<span class="w">    </span><span class="k">defer</span><span class="w"> </span><span class="nx">bufPool</span><span class="p">.</span><span class="nx">Put</span><span class="p">(</span><span class="nx">buf</span><span class="p">)</span><span class="w">  </span><span class="c1">// (1)</span>

<span class="w">    </span><span class="c1">// use buffer for request handling</span>
<span class="p">}</span>
</code></pre></div>
<ol>
<li>Be careful here! It's strictly workflow-dependant, when you must return an object to the pool!</li>
</ol>
<p>Reusing objects through pools reduces memory churn. With fewer allocations, the garbage collector runs less often and with less impact. This translates directly into lower latency and more predictable performance under load.</p>
<h3 id="connection-lifecycle-management">Connection Lifecycle Management</h3>
<p>A connection isn’t just accepted and forgotten—it moves through a full lifecycle: setup, data exchange, teardown. Problems usually show up in the quiet phases. Idle connections that aren’t cleaned up can tie up memory and block goroutines indefinitely. Enforcing read and write deadlines is essential. Heartbeat messages help too—they give you a way to detect dead peers without waiting for the OS to time out.</p>
<p>In one production case, slow client responses left goroutines blocked in reads. Over time, they built up until the system started degrading. Adding deadlines and lightweight health checks fixed the leak. Goroutines no longer lingered, and resource usage stayed flat under load.</p>
<p>Each connection still runs in its own goroutine—but with proper lifecycle management in place, scale doesn’t come at the cost of stability.</p>
<div class="highlight"><pre><span/><code><span class="k">for</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="nx">conn</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">ln</span><span class="p">.</span><span class="nx">Accept</span><span class="p">()</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="c1">// handle error</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="k">go</span><span class="w"> </span><span class="nx">handle</span><span class="p">(</span><span class="nx">conn</span><span class="p">)</span>
<span class="p">}</span>
</code></pre></div>
<p>Inside the handler, a ticker is used to fire every few seconds, triggering a periodic heartbeat that keeps the connection active and responsive:</p>
<div class="highlight"><pre><span/><code><span class="nx">ticker</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">time</span><span class="p">.</span><span class="nx">NewTicker</span><span class="p">(</span><span class="mi">5</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nx">time</span><span class="p">.</span><span class="nx">Second</span><span class="p">)</span>
<span class="k">defer</span><span class="w"> </span><span class="nx">ticker</span><span class="p">.</span><span class="nx">Stop</span><span class="p">()</span>
</code></pre></div>
<p>Before reading from the client, the server sets a read deadline—if no data is received within that time, the operation fails, and the connection is cleaned up. This prevents a blocked read from stalling the goroutine indefinitely:</p>
<div class="highlight"><pre><span/><code><span class="nx">conn</span><span class="p">.</span><span class="nx">SetReadDeadline</span><span class="p">(</span><span class="nx">time</span><span class="p">.</span><span class="nx">Now</span><span class="p">().</span><span class="nx">Add</span><span class="p">(</span><span class="mi">10</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nx">time</span><span class="p">.</span><span class="nx">Second</span><span class="p">))</span>
<span class="nx">_</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">reader</span><span class="p">.</span><span class="nx">ReadString</span><span class="p">(</span><span class="sc">'</span>
<span class="sc">'</span><span class="p">)</span>
<span class="k">if</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="c1">// read timeout or client gone</span>
<span class="p">}</span>
</code></pre></div>
<p>Likewise, before sending the heartbeat, the server sets a write deadline. If the client is unresponsive or the network is slow, the write will fail promptly, avoiding resource leakage:</p>
<div class="highlight"><pre><span/><code><span class="k">select</span><span class="w"> </span><span class="p">{</span>
<span class="k">case</span><span class="w"> </span><span class="o">&lt;-</span><span class="nx">ticker</span><span class="p">.</span><span class="nx">C</span><span class="p">:</span>
<span class="w">    </span><span class="nx">conn</span><span class="p">.</span><span class="nx">SetWriteDeadline</span><span class="p">(</span><span class="nx">time</span><span class="p">.</span><span class="nx">Now</span><span class="p">().</span><span class="nx">Add</span><span class="p">(</span><span class="mi">10</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nx">time</span><span class="p">.</span><span class="nx">Second</span><span class="p">))</span>
<span class="w">    </span><span class="nx">conn</span><span class="p">.</span><span class="nx">Write</span><span class="p">([]</span><span class="nb">byte</span><span class="p">(</span><span class="s">"ping"</span><span class="p">))</span>
<span class="k">default</span><span class="p">:</span>
<span class="w">    </span><span class="c1">// skip heartbeat if not due</span>
<span class="p">}</span>
</code></pre></div>
<p>The loop handles incoming messages and sends periodic heartbeats, with read and write deadlines enforcing boundaries on both sides. This setup keeps each connection under active supervision. Silent failures don’t linger, and the system avoids trading stability for performance.</p>
<h2 id="real-world-tuning-and-scaling-pitfalls">Real-World Tuning and Scaling Pitfalls</h2>
<p>Scaling to 10K+ connections is not just a matter of code—it requires anticipating and mitigating potential pitfalls across many layers of the stack. Beyond addressing memory footprint, file descriptor limits, and blocking I/O, a series of high-concurrency echo server tests revealed additional performance considerations under real load.</p>
<p>One experiment began with a simple line-based echo server. The baseline handler was straightforward:</p>
<div class="highlight"><pre><span/><code><span class="kd">func</span><span class="w"> </span><span class="nx">handle</span><span class="p">(</span><span class="nx">conn</span><span class="w"> </span><span class="nx">net</span><span class="p">.</span><span class="nx">Conn</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">defer</span><span class="w"> </span><span class="nx">conn</span><span class="p">.</span><span class="nx">Close</span><span class="p">()</span>
<span class="w">    </span><span class="nx">reader</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">bufio</span><span class="p">.</span><span class="nx">NewReader</span><span class="p">(</span><span class="nx">conn</span><span class="p">)</span>

<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nx">line</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">reader</span><span class="p">.</span><span class="nx">ReadString</span><span class="p">(</span><span class="sc">'\n'</span><span class="p">)</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="nx">fmt</span><span class="p">.</span><span class="nx">Printf</span><span class="p">(</span><span class="s">"Connection closed: %v\n"</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="p">)</span>
<span class="w">            </span><span class="k">return</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">        </span><span class="nx">conn</span><span class="p">.</span><span class="nx">Write</span><span class="p">([]</span><span class="nb">byte</span><span class="p">(</span><span class="nx">line</span><span class="p">))</span><span class="w"> </span><span class="c1">// echo</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div>
<p>Using a tool like <code>tcpkali</code>:</p>
<div class="highlight"><pre><span/><code>tcpkali<span class="w"> </span>-m<span class="w"> </span><span class="s1">$'ping\n'</span><span class="w"> </span>-c<span class="w"> </span><span class="m">10000</span><span class="w"> </span>--connect-rate<span class="o">=</span><span class="m">2000</span><span class="w"> </span>--duration<span class="o">=</span>60s<span class="w"> </span><span class="m">127</span>.0.0.1:9000
</code></pre></div>
<p>The test ramped up to 10'000 concurrent connections. Over the 60-second run, it sent 2.4 MiB and received 210.3 MiB of data. Each connection averaged around 0.4 kBps, with an aggregate throughput of 29.40 Mbps downstream and 0.33 Mbps upstream. This result highlighted the server’s limited responsiveness to outgoing data under sustained high concurrency, with substantial backpressure on <code>fd.Read</code>.</p>
<h3 id="instrumenting-and-benchmarking-the-server">Instrumenting and Benchmarking the Server</h3>
<div class="admonition info">
<p class="admonition-title">Info</p>
<p>We use <code>c5.2xlarge</code> (8 CPU, 16 GiB) AWS instance for all these tests.</p>
</div>
<p>To better understand system behavior under high load, Go’s built-in tracing facilities were enabled:</p>
<div class="highlight"><pre><span/><code><span class="kn">import</span><span class="w"> </span><span class="p">(</span>
<span class="w">    </span><span class="s">"runtime/trace"</span>
<span class="w">    </span><span class="s">"os"</span>
<span class="w">    </span><span class="s">"log"</span>
<span class="p">)</span>

<span class="kd">func</span><span class="w"> </span><span class="nx">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="nx">f</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">os</span><span class="p">.</span><span class="nx">Create</span><span class="p">(</span><span class="s">"trace.out"</span><span class="p">)</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="nx">log</span><span class="p">.</span><span class="nx">Fatal</span><span class="p">(</span><span class="nx">err</span><span class="p">)</span><span class="w"> </span><span class="p">}</span>
<span class="w">    </span><span class="k">defer</span><span class="w"> </span><span class="nx">f</span><span class="p">.</span><span class="nx">Close</span><span class="p">()</span>

<span class="w">    </span><span class="nx">trace</span><span class="p">.</span><span class="nx">Start</span><span class="p">(</span><span class="nx">f</span><span class="p">)</span>
<span class="w">    </span><span class="k">defer</span><span class="w"> </span><span class="nx">trace</span><span class="p">.</span><span class="nx">Stop</span><span class="p">()</span>

<span class="w">    </span><span class="c1">// server logic ...</span>
<span class="p">}</span>
</code></pre></div>
<p>After running the server and collecting traces, the command</p>
<div class="highlight"><pre><span/><code>go<span class="w"> </span>tool<span class="w"> </span>trace<span class="w"> </span>trace.out
</code></pre></div>
<p>revealed that a significant portion of runtime was spent blocked in <code>fd.Read</code> and <code>fd.Write</code>, suggesting an opportunity to balance I/O operations more effectively. Trace analysis revealed that <code>fd.Read</code> accounted for 23% of runtime, while <code>fd.Write</code> consumed 75%, indicating significant write-side backpressure during echoing. Although <code>ulimit -n</code> was set to 65535 (AWS EC2 instance's hard limit), the system still encountered bottlenecks due to I/O blocking and ephemeral port range limitations.</p>
<h3 id="reducing-write-blocking-with-buffered-writes">Reducing Write Blocking with Buffered Writes</h3>
<p>Connection writes were wrapped in a <code>bufio.Writer</code> with periodic flushing instead of flushing after each write. The updated snippet:</p>
<div class="highlight"><pre><span/><code><span class="nx">reader</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">bufio</span><span class="p">.</span><span class="nx">NewReader</span><span class="p">(</span><span class="nx">conn</span><span class="p">)</span>
<span class="nx">writer</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">bufio</span><span class="p">.</span><span class="nx">NewWriter</span><span class="p">(</span><span class="nx">conn</span><span class="p">)</span>
<span class="nx">count</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="mi">0</span>
<span class="kd">const</span><span class="w"> </span><span class="nx">flushInterval</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="mi">10</span>

<span class="k">for</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="nx">line</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">reader</span><span class="p">.</span><span class="nx">ReadString</span><span class="p">(</span><span class="sc">'\n'</span><span class="p">)</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="k">return</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="nx">writer</span><span class="p">.</span><span class="nx">WriteString</span><span class="p">(</span><span class="nx">line</span><span class="p">)</span>
<span class="w">    </span><span class="nx">count</span><span class="o">++</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nx">count</span><span class="w"> </span><span class="o">&gt;=</span><span class="w"> </span><span class="nx">flushInterval</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nx">writer</span><span class="p">.</span><span class="nx">Flush</span><span class="p">()</span>
<span class="w">        </span><span class="nx">count</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="mi">0</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div>
<p>Benchmarking with:</p>
<div class="highlight"><pre><span/><code>tcpkali<span class="w"> </span>-m<span class="w"> </span><span class="s1">$'ping\n'</span><span class="w"> </span>-c<span class="w"> </span><span class="m">10000</span><span class="w"> </span>--connect-rate<span class="o">=</span><span class="m">2000</span><span class="w"> </span>--duration<span class="o">=</span>60s<span class="w"> </span><span class="m">127</span>.0.0.1:9000
</code></pre></div>
<p>showed dramatic improvements—throughput increased from about 33.8 MiB to over 1661 MiB received and 1369 MiB sent across 10,000 connections, with per-connection bandwidth reaching 5.3 kBps. Aggregate throughput rose to 232.28 Mbps downstream and 191.41 Mbps upstream. The tracing profile confirmed more balanced I/O wait times, even under a much heavier concurrent load.</p>
<h3 id="handling-burst-loads-and-cpu-bound-workloads">Handling Burst Loads and CPU-Bound Workloads</h3>
<p>To evaluate the server's behavior under extreme connection pressure, a burst test was executed with 30,000 connections ramping up at 5,000 per second:</p>
<div class="highlight"><pre><span/><code>tcpkali<span class="w"> </span>-m<span class="w"> </span><span class="s1">$'ping\n'</span><span class="w"> </span>-c<span class="w"> </span><span class="m">30000</span><span class="w"> </span>--connect-rate<span class="o">=</span><span class="m">5000</span><span class="w"> </span>--duration<span class="o">=</span>60s<span class="w"> </span><span class="m">127</span>.0.0.1:9000
</code></pre></div>
<p>The server ramped up cleanly to 30,000 concurrent connections and sustained them for the full 60 seconds. It handled a total of 2580.3 MiB sent and 1250.9 MiB received, maintaining an aggregate throughput of 360.75 Mbps upstream and 174.89 Mbps downstream. Per-channel bandwidth naturally decreased to about 1.2 kBps, but the stability across all channels and the lack of dropped connections pointed to effective load distribution and solid I/O handling even at scale.</p>
<p>To simulate CPU-bound workloads, the server was modified to compute a SHA256 hash for each incoming line:</p>
<div class="highlight"><pre><span/><code><span class="kd">func</span><span class="w"> </span><span class="nx">hash</span><span class="p">(</span><span class="nx">s</span><span class="w"> </span><span class="kt">string</span><span class="p">)</span><span class="w"> </span><span class="kt">string</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="nx">h</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">sha256</span><span class="p">.</span><span class="nx">Sum256</span><span class="p">([]</span><span class="nb">byte</span><span class="p">(</span><span class="nx">s</span><span class="p">))</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="nx">hex</span><span class="p">.</span><span class="nx">EncodeToString</span><span class="p">(</span><span class="nx">h</span><span class="p">[:])</span>
<span class="p">}</span>

<span class="o">...</span>

<span class="k">for</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="nx">line</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">reader</span><span class="p">.</span><span class="nx">ReadString</span><span class="p">(</span><span class="sc">'\n'</span><span class="p">)</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="k">return</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="nx">_</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">hash</span><span class="p">(</span><span class="nx">line</span><span class="p">)</span><span class="w"> </span><span class="c1">// simulate CPU-intensive processing</span>
<span class="w">    </span><span class="nx">writer</span><span class="p">.</span><span class="nx">WriteString</span><span class="p">(</span><span class="nx">line</span><span class="p">)</span>
<span class="w">    </span><span class="nx">count</span><span class="o">++</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nx">count</span><span class="w"> </span><span class="o">&gt;=</span><span class="w"> </span><span class="nx">flushInterval</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nx">writer</span><span class="p">.</span><span class="nx">Flush</span><span class="p">()</span>
<span class="w">        </span><span class="nx">count</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="mi">0</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div>
<p>In this configuration, using the same 30,000-connection setup, throughput dropped to 1068.3 MiB sent and 799.3 MiB received. Aggregate bandwidth fell to 149.35 Mbps upstream and 111.74 Mbps downstream, and per-connection bandwidth declined to around 0.7 kBps. While the server maintained full connection count and uptime, trace analysis revealed increased time spent in runtime.systemstack_switch and GC-related functions. This clearly demonstrated the impact of compute-heavy tasks on overall throughput and reinforced the need for careful balance between I/O and CPU workload when operating at high concurrency.</p>
<h3 id="summarizing-the-technical-gains">Summarizing the Technical Gains</h3>
<p>Benchmarking across four distinct server configurations revealed how buffering, concurrency scaling, and CPU-bound tasks influence performance under load:</p>
<table>
<thead>
<tr>
<th>Feature</th>
<th>Baseline (10K, no buffer)</th>
<th>10K Buffered Connections</th>
<th>30K Buffered Connections</th>
<th>30K + CPU Load (SHA256)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Connections handled</td>
<td>10,000</td>
<td>10,000</td>
<td>30,000</td>
<td>30,000</td>
</tr>
<tr>
<td>Data sent (60s)</td>
<td>2.4 MiB</td>
<td>1369.1 MiB</td>
<td>2580.3 MiB</td>
<td>1068.3 MiB</td>
</tr>
<tr>
<td>Data received (60s)</td>
<td>210.3 MiB</td>
<td>1661.4 MiB</td>
<td>1250.9 MiB</td>
<td>799.3 MiB</td>
</tr>
<tr>
<td>Per-channel bandwidth</td>
<td>~0.4 kBps</td>
<td>~5.3 kBps</td>
<td>~1.2 kBps</td>
<td>~0.7 kBps</td>
</tr>
<tr>
<td>Aggregate bandwidth (↓/↑)</td>
<td>29.40 / 0.33 Mbps</td>
<td>232.28 / 191.41 Mbps</td>
<td>174.89 / 360.75 Mbps</td>
<td>111.74 / 149.35 Mbps</td>
</tr>
<tr>
<td>Packet rate estimate (↓/↑)</td>
<td>329K / 29 pkt/s</td>
<td>278K / 16K pkt/s</td>
<td>135K / 32K pkt/s</td>
<td>136K / 13K pkt/s</td>
</tr>
<tr>
<td>I/O characteristics</td>
<td>Severe write backpressure</td>
<td>Balanced read/write</td>
<td>Efficient under scale</td>
<td>Latency from CPU contention</td>
</tr>
<tr>
<td>CPU and GC pressure</td>
<td>Low</td>
<td>Low</td>
<td>Moderate</td>
<td>High (GC + hash compute)</td>
</tr>
</tbody>
</table>
<p>Starting from the baseline of 10,000 unbuffered connections, the server showed limited throughput—just 2.4 MiB sent and 210.3 MiB received over 60 seconds—with clear signs of write-side backpressure. Introducing buffered writes with the same connection count unlocked over 1369 MiB sent and 1661 MiB received, improving throughput by more than an order of magnitude and balancing I/O wait times. Scaling further to 30,000 connections maintained stability and increased overall throughput, albeit with reduced per-connection bandwidth. When SHA256 hashing was added per message, total throughput dropped significantly, confirming the expected CPU bottleneck and reinforcing the need to factor in compute latency when designing high-concurrency, I/O-heavy services.</p>
<p>These profiles serve as a concrete reference for performance-aware development, where transport, memory, and compute must be co-optimized for real-world scalability.</p>
<p>As you can see, achieving even 30,000 concurrent connections with reliable performance is a non-trivial task. The test results demonstrated that once a workload deviates from a trivial echo server—for example, by adding logging, CPU-bound processing, or more complex read/write logic—throughput and stability can degrade rapidly. Performance at scale is highly dependent on workflow characteristics, such as I/O patterns, synchronization frequency, and memory pressure.</p>
<p>Taken together, these tests reinforce the need for workload-aware tuning and platform-specific adjustments when building high-performance, scalable networking systems. </p>









  




                
                  
</body>
</html>