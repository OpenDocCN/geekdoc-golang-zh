<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Memory Efficiency: Mastering Go’s Garbage Collector</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Memory Efficiency: Mastering Go’s Garbage Collector</h1>
<blockquote>原文：<a href="https://goperf.dev/01-common-patterns/gc/">https://goperf.dev/01-common-patterns/gc/</a></blockquote>
                
                  


  
  



<p>Memory management in Go is automated—but it’s not invisible. Every allocation you make contributes to GC workload. The more frequently objects are created and discarded, the more work the runtime has to do reclaiming memory.</p>
<p>This becomes especially relevant in systems prioritizing low latency, predictable resource usage, or high throughput. Tuning your allocation patterns and leveraging newer features like weak references can help reduce pressure on the GC without adding complexity to your code.</p>
<h2 id="how-gos-garbage-collector-works">How Go's Garbage Collector Works</h2>
<div class="admonition info">
<p class="admonition-title">Info</p>
<p>Highly encourage you to read the official <a href="https://go.dev/doc/gc-guide">A Guide to the Go Garbage Collector</a>! The document provides a detailed description of multiple Go's GC internals.</p>
</div>
<p>Go uses a <strong>non-generational, concurrent, tri-color mark-and-sweep</strong> garbage collector. Here's what that means in practice and how it's implemented.</p>
<h3 id="non-generational">Non-generational</h3>
<p>Many modern GCs, like those in the JVM or .NET CLR, divide memory into <em>generations</em> (young and old) under the assumption that most objects die young. These collectors focus on the young generation, which leads to shorter collection cycles.</p>
<p>Go’s GC takes a different approach. It treats all objects equally—no generational segmentation—not because generational GC conflicts with short pause times or concurrent scanning, but because it hasn’t shown clear, consistent benefits in real-world Go programs with the designs tried so far. This choice avoids the complexity of promotion logic and specialized memory regions. While it can mean scanning more objects overall, this cost is mitigated by concurrent execution and efficient write barriers.</p>
<h3 id="concurrent">Concurrent</h3>
<p>Go’s GC runs concurrently with your application, which means it does most of its work without stopping the world. Concurrency is implemented using multiple phases that interleave with normal program execution:</p>
<p>Even though Go’s garbage collector is mostly concurrent, it still requires brief Stop-The-World (STW) pauses at several points to maintain correctness. These pauses are kept extremely short—typically under 100 microseconds—even with large heaps and hundreds of goroutines.</p>
<p>STW is essential for ensuring that memory structures are not mutated while the GC analyzes them. In most applications, these pauses are imperceptible. However, even sub-millisecond pauses in latency-sensitive systems can be significant—so understanding and monitoring STW behavior becomes important when optimizing for tail latencies or jitter.</p>
<ul>
<li><strong>STW Start Phase:</strong> The application is briefly paused to initiate GC. The runtime scans stacks, globals, and root objects.</li>
<li><strong>Concurrent Mark Phase:</strong> The garbage collector traverses the heap, marking all reachable objects while the program continues running. This is the heaviest phase in terms of work but runs concurrently to avoid long stop-the-world pauses.</li>
<li><strong>STW Mark Termination:</strong> Once marking is mostly complete, the GC briefly pauses the program to finish any remaining work and ensure the heap is in a consistent state before sweeping begins. This pause is typically very short—measured in microseconds.</li>
<li><strong>Concurrent Sweep Phase:</strong> The GC reclaims memory from unreachable (white) objects and returns it to the heap for reuse, all while your program continues running.</li>
</ul>
<p>Write barriers ensure correctness while the application mutates objects during concurrent marking. These barriers help track references created or modified mid-scan so the GC doesn’t miss them.</p>
<h3 id="tri-color-mark-and-sweep">Tri-color Mark and Sweep</h3>
<p>The tri-color algorithm breaks the heap into three working sets during garbage collection:</p>
<ul>
<li><strong>White:</strong> Objects that haven’t been reached—if they stay white, they’ll be collected.</li>
<li><strong>Grey:</strong> Objects that have been discovered (i.e., marked as reachable) but haven’t had their references scanned yet.</li>
<li><strong>Black:</strong> Objects that are both reachable and fully scanned—they’re retained and don’t need further processing.</li>
</ul>
<p>Garbage collection starts by marking all root objects (stack, globals, etc.) grey. It then walks the grey set: for each object, it scans its fields. Any referenced objects that are still white are added to the grey set. Once an object’s references are fully processed, it’s marked black.</p>
<p>When no grey objects remain, anything still white is unreachable and gets cleaned up during the sweep phase. This model ensures that no live object is accidentally collected—even if references change mid-scan—thanks to Go’s write barriers that maintain the algorithm’s core invariants.</p>
<p>A key optimization is <strong>incremental marking</strong>: Go spreads out GC work to avoid long pauses, supported by precise stack scanning and conservative write barriers. The use of concurrent sweeping further reduces latency, allowing memory to be reclaimed without halting execution.</p>
<p>This design gives Go a GC that’s safe, fast, and friendly to server workloads with large heaps and many cores.</p>
<h2 id="gc-tuning-gogc">GC Tuning: GOGC</h2>
<p>Go’s garbage collector is tuned to deliver good performance without manual configuration. The default <code>GOGC</code> setting typically strikes the right balance between memory consumption and CPU effort, adapting well across a wide range of workloads. In most cases, manually tweaking it offers little benefit—and in many, it actually makes things worse by increasing either pause times or memory pressure. Unless you’ve profiled a specific bottleneck and understand the trade-offs, it’s usually best to leave <code>GOGC</code> alone.</p>
<p>That said, there are specific cases where tuning <code>GOGC</code> can yield significant gains. For example, <a href="https://www.uber.com/en-GB/blog/how-we-saved-70k-cores-across-30-mission-critical-services/">Uber implemented dynamic GC tuning</a> across their Go services to reduce CPU usage and saved tens of thousands of cores in the process. Their approach relied on profiling, metric collection, and automation to safely adjust GC behavior based on actual memory pressure and workload characteristics.</p>
<p>Another unusual case is from Cloudflare. They <a href="https://blog.cloudflare.com/go-dont-collect-my-garbage/">profiled a high-concurrency cryptographic workload</a> and found that Go’s GC became a bottleneck as goroutines increased. Their application produced minimal garbage, yet GC overhead grew with concurrency. By tuning GOGC to a much higher value—specifically 11300—they significantly reduced GC frequency and improved throughput, achieving over 22× performance gains compared to the single-core baseline. This case highlights how allowing more heap growth in CPU-bound and low-allocation scenarios can yield major improvements.</p>
<p>So, if you decide to tune the garbage collector, be methodical:</p>
<ul>
<li>Always profile first. Use tools like <code>pprof</code> to confirm that GC activity is a bottleneck.</li>
<li>Change settings incrementally. For example, increasing <code>GOGC</code> from 100 to 150 means the GC will run less frequently, using less CPU but more memory.</li>
<li>Verify impact. After tuning, validate with profiling data that the change had a positive effect. Without that confirmation, it's easy to make things worse.</li>
</ul>
<div class="highlight"><pre><span/><code><span class="nv">GOGC</span><span class="o">=</span><span class="m">100</span><span class="w">  </span><span class="c1"># Default: GC runs when heap grows 100% since last collection</span>
<span class="nv">GOGC</span><span class="o">=</span>off<span class="w">  </span><span class="c1"># Disables GC (use only in special cases like short-lived CLI tools)</span>
</code></pre></div>
<h3 id="memory-limiting-with-gomemlimit">Memory Limiting with <code>GOMEMLIMIT</code></h3>
<p>In addition to <code>GOGC</code>, Go provides <code>GOMEMLIMIT</code>—a soft memory limit that caps the total heap size the runtime will try to stay under. This allows you to explicitly control memory growth, especially useful in environments like containers or systems with strict memory budgets.</p>
<p>Why is this helpful? In containerized environments (like Kubernetes), memory limits are typically enforced at the OS or orchestrator level. If your application exceeds its memory quota, the OOM killer may abruptly terminate the container. Go's GC isn't aware of those limits by default.</p>
<p>Setting a <code>GOMEMLIMIT</code> helps prevent this. For example, if your container has a 512MiB memory limit, you might set:</p>
<div class="highlight"><pre><span/><code><span class="nv">GOMEMLIMIT</span><span class="o">=</span>400MiB
</code></pre></div>
<p>This buffer gives the Go runtime room to act before reaching the hard system-imposed memory cap. It allows the garbage collector to become more aggressive as total memory usage grows, reducing the chances of the process being killed due to an out-of-memory condition. It also leaves space for non-heap allocations—like goroutine stacks, OS threads, and other internal runtime structures—which don’t count toward heap size but still consume real memory.</p>
<p>You can also set the limit programmatically:</p>
<div class="highlight"><pre><span/><code><span class="kn">import</span><span class="w"> </span><span class="s">"runtime/debug"</span>

<span class="nx">debug</span><span class="p">.</span><span class="nx">SetMemoryLimit</span><span class="p">(</span><span class="mi">2</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="mi">30</span><span class="p">)</span><span class="w"> </span><span class="c1">// 2 GiB</span>
</code></pre></div>
<p>The GC will become more aggressive as heap usage nears the limit, which can increase CPU load. Be careful not to set the limit too low—especially if your application maintains a large live set of objects—or you may trigger excessive GC cycles.</p>
<p>While <code>GOGC</code> controls how frequently the GC runs based on heap growth, <code>GOMEMLIMIT</code> constrains the heap size itself. The two can be combined for more precise control:</p>
<div class="highlight"><pre><span/><code><span class="nv">GOGC</span><span class="o">=</span><span class="m">100</span><span class="w"> </span><span class="nv">GOMEMLIMIT</span><span class="o">=</span>4GiB<span class="w"> </span>./your-service
</code></pre></div>
<p>This tells the GC to operate with the default growth ratio and to start collecting sooner if heap usage nears 4 GiB.</p>
<h3 id="gomemlimitx-and-gogcoff-configuration">GOMEMLIMIT=X and GOGC=off configuration</h3>
<p>In scenarios where memory availability is fixed and predictable—such as within containers or VMs, you can use these two variables together:</p>
<ul>
<li><code>GOMEMLIMIT=X</code> tells the runtime to aim for a specific memory ceiling. For example, <code>GOMEMLIMIT=2GiB</code> will trigger garbage collection when total memory usage nears 2 GiB.</li>
<li><code>GOGC=off</code> disables the default GC pacing algorithm, so garbage collection only runs when the memory limit is hit.</li>
</ul>
<p>This configuration maximizes memory usage efficiency and avoids the overhead of frequent GC cycles. It's especially effective in high-throughput or latency-sensitive systems where predictable memory usage matters.</p>
<p><strong>Example:</strong></p>
<div class="highlight"><pre><span/><code><span class="nv">GOMEMLIMIT</span><span class="o">=</span>2GiB<span class="w"> </span><span class="nv">GOGC</span><span class="o">=</span>off<span class="w"> </span>./my-app
</code></pre></div>
<p>With this setup, memory usage grows freely until the 2 GiB threshold is reached. At that point, Go performs a full garbage collection pass.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<ul>
<li>Always benchmark with your real workload. Disabling automatic GC can backfire if your application produces a lot of short-lived allocations.</li>
<li>Monitor memory pressure and GC pause times using <code>runtime.ReadMemStats</code> or <code>pprof</code>.</li>
<li>This approach works best when your memory usage patterns are well understood and stable.</li>
</ul>
</div>
<h2 id="practical-strategies-for-reducing-gc-pressure">Practical Strategies for Reducing GC Pressure</h2>
<h3 id="prefer-stack-allocation">Prefer Stack Allocation</h3>
<p>Go allocates variables on the stack whenever possible. Avoid escaping variables to the heap:</p>
<div class="highlight"><pre><span/><code><span class="c1">// BAD: returns pointer to heap-allocated struct</span>
<span class="kd">func</span><span class="w"> </span><span class="nx">newUser</span><span class="p">(</span><span class="nx">name</span><span class="w"> </span><span class="kt">string</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="nx">User</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="o">&amp;</span><span class="nx">User</span><span class="p">{</span><span class="nx">Name</span><span class="p">:</span><span class="w"> </span><span class="nx">name</span><span class="p">}</span><span class="w">  </span><span class="c1">// escapes to heap</span>
<span class="p">}</span>

<span class="c1">// BETTER: use value types if pointer is unnecessary</span>
<span class="kd">func</span><span class="w"> </span><span class="nx">printUser</span><span class="p">(</span><span class="nx">u</span><span class="w"> </span><span class="nx">User</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="nx">fmt</span><span class="p">.</span><span class="nx">Println</span><span class="p">(</span><span class="nx">u</span><span class="p">.</span><span class="nx">Name</span><span class="p">)</span>
<span class="p">}</span>
</code></pre></div>
<p>Use <code>go build -gcflags="-m"</code> to view escape analysis diagnostics. See <a href="../stack-alloc/">Stack Allocations and Escape Analysis</a> for more details.</p>
<h3 id="use-syncpool-for-short-lived-objects">Use sync.Pool for Short-Lived Objects</h3>
<p><code>sync.Pool</code> is ideal for temporary, reusable allocations that are expensive to GC.</p>
<div class="highlight"><pre><span/><code><span class="kd">var</span><span class="w"> </span><span class="nx">bufPool</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">sync</span><span class="p">.</span><span class="nx">Pool</span><span class="p">{</span>
<span class="w">    </span><span class="nx">New</span><span class="p">:</span><span class="w"> </span><span class="kd">func</span><span class="p">()</span><span class="w"> </span><span class="kt">any</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="nb">new</span><span class="p">(</span><span class="nx">bytes</span><span class="p">.</span><span class="nx">Buffer</span><span class="p">)</span><span class="w"> </span><span class="p">},</span>
<span class="p">}</span>

<span class="kd">func</span><span class="w"> </span><span class="nx">handler</span><span class="p">(</span><span class="nx">w</span><span class="w"> </span><span class="nx">http</span><span class="p">.</span><span class="nx">ResponseWriter</span><span class="p">,</span><span class="w"> </span><span class="nx">r</span><span class="w"> </span><span class="o">*</span><span class="nx">http</span><span class="p">.</span><span class="nx">Request</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="nx">buf</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">bufPool</span><span class="p">.</span><span class="nx">Get</span><span class="p">().(</span><span class="o">*</span><span class="nx">bytes</span><span class="p">.</span><span class="nx">Buffer</span><span class="p">)</span>
<span class="w">    </span><span class="nx">buf</span><span class="p">.</span><span class="nx">Reset</span><span class="p">()</span>
<span class="w">    </span><span class="k">defer</span><span class="w"> </span><span class="nx">bufPool</span><span class="p">.</span><span class="nx">Put</span><span class="p">(</span><span class="nx">buf</span><span class="p">)</span>

<span class="w">    </span><span class="c1">// Use buf...</span>
<span class="p">}</span>
</code></pre></div>
<p>See <a href="../object-pooling/">Object Pooling</a> for more details.</p>
<h3 id="batch-allocations">Batch Allocations</h3>
<p>Group allocations into fewer objects to reduce GC pressure.</p>
<div class="highlight"><pre><span/><code><span class="c1">// Instead of allocating many small structs, allocate a slice of structs</span>
<span class="nx">users</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nb">make</span><span class="p">([]</span><span class="nx">User</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">1000</span><span class="p">)</span><span class="w">  </span><span class="c1">// single large allocation</span>
</code></pre></div>
<p>See <a href="../mem-prealloc/">Memory Preallocation</a> for more details.</p>
<h2 id="weak-references-in-go">Weak References in Go</h2>
<p>Go 1.24 added the <code>weak</code> package, providing a standardized way to create weak references—pointers that don’t keep their target objects alive. In garbage-collected systems like Go, strong references extend an object’s lifetime: as long as something points to it, it won’t be collected. That’s usually what you want, but in structures like caches, deduplication maps, or object graphs, this can lead to memory staying alive much longer than intended. Weak references solve that by allowing you to refer to an object without blocking the GC from reclaiming it when nothing else is using it.</p>
<p>A weak reference, by contrast, tells the garbage collector: “you can collect this object if nothing else is strongly referencing it.” This pattern is important for building memory-sensitive data structures that should not interfere with garbage collection.</p>
<div class="highlight"><pre><span/><code><span class="kn">package</span><span class="w"> </span><span class="nx">main</span>

<span class="kn">import</span><span class="w"> </span><span class="p">(</span>
<span class="w">    </span><span class="s">"fmt"</span>
<span class="w">    </span><span class="s">"runtime"</span>
<span class="w">    </span><span class="s">"weak"</span>
<span class="p">)</span>

<span class="kd">type</span><span class="w"> </span><span class="nx">Data</span><span class="w"> </span><span class="kd">struct</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="nx">Value</span><span class="w"> </span><span class="kt">string</span>
<span class="p">}</span>

<span class="kd">func</span><span class="w"> </span><span class="nx">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="nx">data</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="o">&amp;</span><span class="nx">Data</span><span class="p">{</span><span class="nx">Value</span><span class="p">:</span><span class="w"> </span><span class="s">"Important"</span><span class="p">}</span>
<span class="w">    </span><span class="nx">wp</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">weak</span><span class="p">.</span><span class="nx">Make</span><span class="p">(</span><span class="nx">data</span><span class="p">)</span><span class="w"> </span><span class="c1">// create weak pointer</span>

<span class="w">    </span><span class="nx">fmt</span><span class="p">.</span><span class="nx">Println</span><span class="p">(</span><span class="s">"Original:"</span><span class="p">,</span><span class="w"> </span><span class="nx">wp</span><span class="p">.</span><span class="nx">Value</span><span class="p">().</span><span class="nx">Value</span><span class="p">)</span>

<span class="w">    </span><span class="nx">data</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="c1">// remove strong reference</span>
<span class="w">    </span><span class="nx">runtime</span><span class="p">.</span><span class="nx">GC</span><span class="p">()</span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nx">v</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">wp</span><span class="p">.</span><span class="nx">Value</span><span class="p">();</span><span class="w"> </span><span class="nx">v</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nx">fmt</span><span class="p">.</span><span class="nx">Println</span><span class="p">(</span><span class="s">"Still alive:"</span><span class="p">,</span><span class="w"> </span><span class="nx">v</span><span class="p">.</span><span class="nx">Value</span><span class="p">)</span>
<span class="w">    </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nx">fmt</span><span class="p">.</span><span class="nx">Println</span><span class="p">(</span><span class="s">"Data has been collected"</span><span class="p">)</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div>
<div class="highlight"><pre><span/><code>Original: Important
Data has been collected
</code></pre></div>
<p>In this example, <code>wp</code> holds a weak reference to a <code>Data</code> object. After the strong reference (<code>data</code>) goes out of scope and the garbage collector runs, the <code>Data</code> may be collected—at which point <code>wp.Value()</code> will return nil. This pattern is especially useful in memory-sensitive contexts like caches or canonicalization maps, where you want to avoid artificially extending object lifetimes. Always check the result of <code>Value()</code> before using it, since the target may have been reclaimed.</p>
<h2 id="benchmarking-impact">Benchmarking Impact</h2>
<p>It's tempting to rely on synthetic benchmarks to evaluate the performance of Go's garbage collector, but generic benchmarks rarely capture the nuances of real-world workloads. Memory behavior is highly dependent on allocation patterns, object lifetimes, concurrency, and how frequently short-lived versus long-lived data structures are used.</p>
<p>For example, the impact of GC in a CPU-bound microservice that maintains large in-memory indexes will differ dramatically from an I/O-heavy API server with minimal heap usage. As such, tuning decisions should always be informed by your application's profiling data.</p>
<p>We cover targeted use cases and their GC performance trade-offs in more focused articles:</p>
<ul>
<li><a href="../object-pooling/">Object Pooling</a>: Reducing allocation churn using <code>sync.Pool</code></li>
<li><a href="../stack-alloc/">Stack Allocations and Escape Analysis</a>: Minimizing heap usage by keeping values on the stack</li>
<li><a href="../mem-prealloc/">Memory Preallocation</a>: Avoiding unnecessary growth of slices and maps</li>
</ul>
<p>When applied to the right context, these techniques can make a measurable difference, but they don’t lend themselves to one-size-fits-all benchmarks.</p>









  




                
                  
</body>
</html>