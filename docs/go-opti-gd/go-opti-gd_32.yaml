- en: GOMAXPROCS, epoll/kqueue, and Scheduler-Level Tuning¶
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GOMAXPROCS, epoll/kqueue和调度器级别的调整[¶]
- en: 原文：[https://goperf.dev/02-networking/a-bit-more-tuning/](https://goperf.dev/02-networking/a-bit-more-tuning/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[https://goperf.dev/02-networking/a-bit-more-tuning/](https://goperf.dev/02-networking/a-bit-more-tuning/)'
- en: 'Go applications operating at high concurrency levels frequently encounter performance
    ceilings that are not attributable to CPU saturation. These limitations often
    stem from runtime-level mechanics: how goroutines (G) are scheduled onto logical
    processors (P) via operating system threads (M), how blocking operations affect
    thread availability, and how the runtime interacts with kernel facilities like
    `epoll` or `kqueue` for I/O readiness.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在高并发级别运行的Go应用程序经常遇到性能瓶颈，这些瓶颈并非由CPU饱和引起。这些限制通常源于运行时级别的机制：goroutines（G）如何通过操作系统线程（M）调度到逻辑处理器（P），阻塞操作如何影响线程可用性，以及运行时如何与内核设施（如`epoll`或`kqueue`）交互以进行I/O就绪。
- en: Unlike surface-level code optimization, resolving these issues requires awareness
    of the Go scheduler’s internal design, particularly how GOMAXPROCS governs execution
    parallelism and how thread contention, cache locality, and syscall latency emerge
    under load. Misconfigured runtime settings can lead to excessive context switching,
    stalled P’s, and degraded throughput despite available cores.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 与表面代码优化不同，解决这些问题需要了解Go调度器的内部设计，特别是GOMAXPROCS如何控制执行并行性，以及线程竞争、缓存局部性和系统调用延迟如何在负载下出现。错误的运行时设置可能导致上下文切换过多、P停滞和吞吐量下降，尽管有可用的核心。
- en: System-level tuning—through CPU affinity, thread pinning, and scheduler introspection—provides
    a critical path to improving latency and throughput in multicore environments.
    When paired with precise benchmarking and observability, these adjustments allow
    Go services to scale more predictably and fully take advantage of modern hardware
    architectures.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 系统级别的调整——通过CPU亲和性、线程固定和调度器内省——是提高多核环境中延迟和吞吐量的关键路径。当与精确基准测试和可观察性相结合时，这些调整使Go服务能够更可预测地扩展，并充分利用现代硬件架构。
- en: Understanding GOMAXPROCS[¶](#understanding-gomaxprocs "Permanent link")
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解GOMAXPROCS[¶](#understanding-gomaxprocs "永久链接")
- en: In Go, `GOMAXPROCS` defines the maximum number of operating system threads (M’s)
    simultaneously executing user‑level Go code (G’s). It’s set to the developer's
    machine’s logical CPU count by default. Under the hood, the scheduler exposes
    P’s (processors) equal to `GOMAXPROCS`. Each P hosts a run queue of G’s and binds
    to a single M to execute Go code.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在Go中，`GOMAXPROCS`定义了同时执行用户级Go代码（G）的最大操作系统线程数（M）。默认情况下，它设置为开发者的机器的逻辑CPU数量。在底层，调度器暴露了等于`GOMAXPROCS`的P（处理器）。每个P托管一个G的运行队列，并绑定到一个M来执行Go代码。
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: When developers increase `GOMAXPROCS`, developers allow more P’s—and therefore
    more OS threads—to run Go‑routines in parallel. That often boosts performance
    for CPU‑bound workloads. However, more P’s also incur more context switches, more
    cache thrashing, and potentially more contention in shared data structures (e.g.,
    the garbage collector’s work queues). It's important to understand that blindly
    scaling past the sweet spot can actually degrade latency.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 当开发者增加`GOMAXPROCS`时，开发者允许更多的P（因此更多的OS线程）并行运行Go-routines。这通常能提升CPU密集型工作的性能。然而，更多的P也会带来更多的上下文切换、更多的缓存冲突，以及共享数据结构（例如，垃圾收集器的工作队列）的潜在竞争。重要的是要理解，盲目地超过最佳点实际上可能会降低延迟。
- en: Diving into Go’s Scheduler Internals[¶](#diving-into-gos-scheduler-internals
    "Permanent link")
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深入Go的调度器内部[¶](#diving-into-gos-scheduler-internals "永久链接")
- en: 'Go’s scheduler organizes three core actors: G (goroutine), M (OS thread), and
    P (logical processor), [see more details here](../networking-internals/#goroutines-and-the-runtime-scheduler).
    When a goroutine makes a blocking syscall, its M detaches from its P, returning
    the P to the global scheduler so another M can pick it up. This design prevents
    syscalls from starving CPU‑bound goroutines.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Go的调度器组织了三个核心参与者：G（goroutine）、M（OS线程）和P（逻辑处理器），[更多信息请见此处](../networking-internals/#goroutines-and-the-runtime-scheduler)。当一个goroutine执行阻塞系统调用时，它的M会从它的P上分离，将P返回给全局调度器，以便另一个M可以取走。这种设计防止了系统调用使CPU密集型goroutine饿死。
- en: 'The scheduler uses work stealing: each P maintains a local run queue, and idle
    P’s will steal work from busier peers. If developers set GOMAXPROCS too high,
    developers will see diminishing returns in stolen work versus the overhead of
    balancing those run queues.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 调度器使用工作窃取：每个P维护一个本地运行队列，空闲的P将从忙碌的同伴那里窃取工作。如果开发者将GOMAXPROCS设置得太高，开发者将看到窃取工作与平衡这些运行队列的开销之间的回报递减。
- en: 'Enabling scheduler tracing via `GODEBUG` can reveal fine grained metrics:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 通过`GODEBUG`启用调度器跟踪可以揭示细粒度的指标：
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '`schedtrace=1000` instructs the runtime to print scheduler state every 1000
    milliseconds (1 second).'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`schedtrace=1000` 指示运行时每1000毫秒（1秒）打印一次调度器状态。'
- en: '`scheddetail=1` enables additional information per logical processor (P), such
    as individual run queue lengths.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scheddetail=1` 启用每个逻辑处理器（P）的附加信息，例如单个运行队列长度。'
- en: 'Each printed trace includes statistics like:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 每个打印的跟踪包括如下统计信息：
- en: '[PRE2]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The first line reports global scheduler state including whether garbage collection
    is blocking (gcwaiting), if spinning threads are needed, and idle thread counts.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行报告全局调度器状态，包括垃圾收集是否阻塞（gcwaiting）、是否需要自旋线程以及空闲线程计数。
- en: Each P line details the logical processor's scheduler activity, including the
    number of times it's scheduled (schedtick), system call activity (syscalltick),
    timers, and free goroutine slots.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 每个P行详细说明了逻辑处理器的调度活动，包括其被调度的次数（schedtick）、系统调用活动（syscalltick）、计时器和空闲goroutine槽位。
- en: The M lines correspond to OS threads. Each line shows which goroutine—if any—is
    running on that thread, whether the thread is idle, spinning, or blocked, along
    with memory allocation activity and lock states.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: M行对应于操作系统线程。每一行显示哪个goroutine（如果有）正在该线程上运行，线程是否空闲、自旋或阻塞，以及内存分配活动和锁状态。
- en: 'This view makes it easier to spot not only classic concurrency bottlenecks
    but also deeper issues: scheduler delays, blocking syscalls, threads that spin
    without doing useful work, or CPU cores that sit idle when they shouldn’t. The
    output reveals patterns that aren’t visible from logs or metrics alone.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这种视图不仅有助于发现经典的并发瓶颈，还能发现更深层次的问题：调度延迟、阻塞的系统调用、不进行有用工作的自旋线程，或不应空闲的CPU核心。输出揭示了仅从日志或指标中无法看到的模式。
- en: '`gomaxprocs=14`: Number of logical processors (P’s).'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gomaxprocs=14`: 逻辑处理器（P）的数量。'
- en: '`idleprocs=14`: All processors are idle, indicating no runnable goroutines.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`idleprocs=14`: 所有处理器都空闲，表示没有可运行的goroutines。'
- en: '`threads=26`: Number of M’s (OS threads) created.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`threads=26`: 创建的M的数量（操作系统线程）。'
- en: '`spinningthreads=0`: No threads are actively searching for work.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spinningthreads=0`: 没有线程正在积极寻找工作。'
- en: '`needspinning=0`: No additional spinning threads are requested by the scheduler.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`needspinning=0`: 调度器不需要额外的自旋线程。'
- en: '`idlethreads=20`: Number of OS threads currently idle.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`idlethreads=20`: 当前空闲的操作系统线程数量。'
- en: '`runqueue=0`: Global run queue is empty.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`runqueue=0`: 全局运行队列为空。'
- en: '`gcwaiting=false`: Garbage collector is not blocking execution.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gcwaiting=false`: 垃圾收集器不会阻塞执行。'
- en: '`nmidlelocked=1`: One P is locked to a thread that is currently idle.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nmidlelocked=1`: 一个P被锁定到一个当前空闲的线程上。'
- en: '`stopwait=0`: No goroutines waiting to stop the world.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stopwait=0`: 没有goroutines正在等待停止世界。'
- en: '`sysmonwait=false`: The system monitor is actively running, not sleeping.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sysmonwait=false`: 系统监控器正在积极运行，而不是休眠。'
- en: 'The global run queue holds goroutines that are not bound to any specific P
    or that overflowed local queues. In contrast, each logical processor (P) maintains
    a local run queue of goroutines it is responsible for scheduling. Goroutines are
    preferentially enqueued locally for performance: local queues avoid lock contention
    and improve cache locality. It may be placed on the global queue only when a P''s
    local queue is full, or a goroutine originates from outside a P (e.g., from a
    syscall).'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 全局运行队列包含未绑定到任何特定P或超出本地队列的goroutines。相比之下，每个逻辑处理器（P）维护一个它负责调度的goroutines的本地运行队列。为了提高性能，goroutines优先在本地队列中入队：本地队列避免了锁竞争并提高了缓存局部性。只有在P的本地队列已满或goroutine来自P外部（例如，来自系统调用）时，才将其放置在全局队列上。
- en: This dual-queue strategy reduces synchronization overhead across P’s and enables
    efficient scheduling under high concurrency. Understanding the ratio of local
    vs global queue activity helps diagnose whether the system is under-provisioned,
    improperly balanced, or suffering from excessive cross-P migrations.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这种双队列策略降低了P之间的同步开销，并在高并发下实现了高效的调度。了解本地与全局队列活动比率有助于诊断系统是否配置不足、不平衡或遭受过多的跨P迁移。
- en: These insights help quantify how efficiently goroutines are scheduled, how much
    parallelism is actually utilized, and whether the system is under- or over-provisioned
    in terms of logical processors. Observing these patterns under load is crucial
    when adjusting `GOMAXPROCS`, diagnosing tail latency, or identifying scheduler
    contention.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这些见解有助于量化goroutines的调度效率，实际利用了多少并行性，以及从逻辑处理器的角度来看，系统是否配置不足或过剩。在调整`GOMAXPROCS`、诊断尾部延迟或识别调度竞争时，在负载下观察这些模式至关重要。
- en: 'Netpoller: Deep Dive into epoll on Linux and kqueue on BSD[¶](#netpoller-deep-dive-into-epoll-on-linux-and-kqueue-on-bsd
    "Permanent link")'
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Netpoller：深入Linux上的epoll和BSD上的kqueue[¶](#netpoller-deep-dive-into-epoll-on-linux-and-kqueue-on-bsd
    "永久链接")
- en: In any Go application handling high connection volumes, the network poller plays
    a critical behind-the-scenes role. At its core, Go uses the OS-level multiplexing
    facilities—`epoll` on Linux and `kqueue` on BSD/macOS—to monitor thousands of
    sockets concurrently with minimal threads. The runtime leverages these mechanisms
    efficiently, but understanding how and why reveals opportunities for tuning, especially
    under demanding loads.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何处理高连接量的Go应用程序中，网络轮询器在幕后发挥着关键作用。在核心上，Go使用操作系统级别的多路复用设施——Linux上的`epoll`和BSD/macOS上的`kqueue`——以最小化线程的数量同时监控数千个套接字。运行时有效地利用了这些机制，但了解其如何以及为什么这样做揭示了调优的机会，尤其是在负载需求高的情况下。
- en: When a goroutine initiates a network operation like reading from a TCP connection,
    the runtime doesn't immediately block the underlying thread. Instead, it registers
    the file descriptor with the poller—using `epoll_ctl` in edge-triggered mode or
    `EV_SET` with `EVFILT_READ`—and parks the goroutine. The actual thread (M) becomes
    free to run other goroutines. When data arrives, the kernel signals the poller
    thread, which in turn wakes the appropriate goroutine by scheduling it onto a
    P’s run queue. This wakeup process minimizes contention by relying on per-P notification
    lists and avoids runtime lock bottlenecks.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个goroutine启动一个网络操作，如从TCP连接中读取时，运行时不会立即阻塞底层线程。相反，它将文件描述符注册到轮询器——使用边缘触发模式的`epoll_ctl`或`EV_SET`与`EVFILT_READ`——并将goroutine挂起。实际的线程（M）变得空闲，可以运行其他goroutines。当数据到达时，内核向轮询线程发出信号，然后轮询线程通过将其调度到P的运行队列上唤醒适当的goroutine。这个过程通过依赖每个P的通知列表来最小化竞争，并避免运行时锁瓶颈。
- en: Go uses edge-triggered notifications, which signal only on state transitions—like
    new data becoming available. This design requires the application to drain sockets
    fully during each wakeup or risk missing future events. While more complex than
    level-triggered behavior, edge-triggered mode significantly reduces syscall overhead
    under load.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Go使用边缘触发通知，仅在状态转换时发出信号——例如新数据可用。这种设计要求在每次唤醒时完全清空套接字，否则可能会错过未来的事件。虽然比电平触发行为更复杂，但边缘触发模式在负载下显著减少了系统调用开销。
- en: 'Here''s a simplified version of what happens under the hood during a read operation:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个简化版的在读取操作期间底层发生的操作：
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Internally, Go runs a dedicated poller thread that loops on `epoll_wait` or
    `kevent`, collecting batches of events (typically 512 at a time). After the call
    returns, the runtime processes these events, distributing wakeups across logical
    processors to prevent any single P from becoming a bottleneck. To further promote
    scheduling fairness, the poller thread may rotate across P’s periodically, a behavior
    governed by `GODEBUG=netpollWaitLatency`.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在内部，Go运行一个专门的轮询线程，该线程循环在`epoll_wait`或`kevent`上，每次收集一批事件（通常每次512个）。在调用返回后，运行时处理这些事件，将唤醒分布在逻辑处理器上，以防止任何单个P成为瓶颈。为了进一步促进调度公平性，轮询线程可能会定期在P之间轮换，这种行为由`GODEBUG=netpollWaitLatency`控制。
- en: Go’s runtime is optimized to reduce unnecessary syscalls and context switches.
    All file descriptors are set to non-blocking, which allows the poller thread to
    remain responsive. To avoid the thundering herd problem—where multiple threads
    wake on the same socket—the poller ensures only one goroutine handles a given
    FD event at a time.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Go的运行时经过优化，以减少不必要的系统调用和上下文切换。所有文件描述符都设置为非阻塞，这使得轮询线程可以保持响应。为了避免多个线程在同一套接字上唤醒的“雷鸣之群”问题，轮询器确保一次只有一个goroutine处理给定的FD事件。
- en: The design goes even further by aligning the circular event buffer with cache
    lines and distributing wakeups via per-P lists. These details matter at scale.
    With proper alignment and locality, Go reduces CPU cache contention when thousands
    of connections are active.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 设计更进一步，通过将环形事件缓冲区与缓存行对齐，并通过每个 P 列表分发唤醒，这些细节在规模上很重要。通过适当的对齐和局部性，Go 在数千个连接活跃时减少了
    CPU 缓存竞争。
- en: For developers looking to inspect poller behavior, enabling tracing with `GODEBUG=netpoll=1`
    can surface system-level latencies and epoll activity. Additionally, the `GODEBUG=netpollWaitLatency=200`
    flag configures the poller’s willingness to hand off to another P every 200 microseconds.
    That’s particularly helpful in debugging idle P starvation or evaluating fairness
    in high-throughput systems.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 对于想要检查轮询器行为的开发者，通过启用 `GODEBUG=netpoll=1` 的跟踪可以揭示系统级延迟和 epoll 活动。此外，`GODEBUG=netpollWaitLatency=200`
    标志配置了轮询器每 200 微秒将任务转交给另一个 P 的意愿。这在调试空闲 P 饥饿或评估高吞吐量系统中的公平性时特别有帮助。
- en: 'Here''s a small experiment that logs event activity:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个记录事件活动的简单实验：
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'You’ll see log lines like:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到这样的日志行：
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Most developers never need to think about this machinery—and they shouldn't.
    But these details become valuable in edge cases, like high-throughput HTTP proxies
    or latency-sensitive services dealing with hundreds of thousands of concurrent
    sockets. Tuning parameters like `GOMAXPROCS`, adjusting the event buffer size,
    or modifying poller wake-up intervals can yield measurable performance improvements,
    particularly in tail latencies.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数开发者无需考虑这种机制——他们也不应该考虑。但在边缘情况下，如高吞吐量 HTTP 代理或处理数十万个并发套接字的延迟敏感型服务时，这些细节变得很有价值。调整参数如
    `GOMAXPROCS`、调整事件缓冲区大小或修改轮询器唤醒间隔可以带来可衡量的性能提升，尤其是在尾部延迟方面。
- en: For example, in a system handling hundreds of thousands of concurrent HTTP/2
    streams, increasing `GOMAXPROCS` while using `GODEBUG=netpollWaitLatency=100`
    helped reduce the 99th percentile read latency by over 15%, simply by preventing
    poller starvation under I/O backpressure.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在一个处理数十万个并发 HTTP/2 流的系统中，在启用 `GODEBUG=netpollWaitLatency=100` 的同时增加 `GOMAXPROCS`，可以仅通过防止在
    I/O 反压下发生轮询器饥饿，就将 99 分位读取延迟降低了超过 15%。
- en: As with all low-level tuning, it's not about changing knobs blindly. It's about
    knowing what Go’s netpoller is doing, why it’s structured the way it is, and where
    its boundaries can be nudged for just a bit more efficiency—when measurements
    tell you it’s worth it.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 与所有低级调整一样，这并不是关于盲目地改变旋钮。这是关于了解 Go 的 netpoller 在做什么，为什么它以这种方式构建，以及在哪里可以稍微调整边界以获得更多效率——当测量表明这是值得的时候。
- en: Thread Pinning with `LockOSThread` and `GODEBUG` Flags[¶](#thread-pinning-with-lockosthread-and-godebug-flags
    "Permanent link")
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 `LockOSThread` 和 `GODEBUG` 标志进行线程固定[¶](#thread-pinning-with-lockosthread-and-godebug-flags
    "永久链接")
- en: Go offers tools like `runtime.LockOSThread()` to pin a goroutine to a specific
    OS thread, but in most real-world applications, the payoff is minimal. Benchmarks
    consistently show that for typical server workloads—especially those that are
    CPU-bound—Go’s scheduler handles thread placement well without manual intervention.
    Introducing thread pinning tends to add complexity without delivering measurable
    gains.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Go 提供了 `runtime.LockOSThread()` 这样的工具来将 goroutine 锁定到特定的操作系统线程，但在大多数实际应用中，这种收益微乎其微。基准测试一致显示，对于典型的服务器工作负载——尤其是那些
    CPU 密集型的——Go 的调度器能够很好地处理线程放置，无需人工干预。引入线程固定通常会增加复杂性，而不会带来可衡量的收益。
- en: There are exceptions. In ultra-low-latency or real-time systems, pinning can
    help reduce jitter by avoiding thread migration. But these gains typically require
    isolated CPU cores, tightly controlled environments, and strict latency targets.
    In practice, that means bare metal. On shared infrastructure—especially in cloud
    environments like AWS where cores are virtualized and noisy neighbors are common—thread
    pinning rarely delivers any measurable benefit.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 存在例外。在超低延迟或实时系统中，固定线程可以帮助通过避免线程迁移来减少抖动。但这些收益通常需要隔离的 CPU 核心、严格控制的环境和严格的延迟目标。在实践中，这意味着裸机。在共享基础设施上——尤其是在
    AWS 这样的云环境中，核心是虚拟化的，并且存在嘈杂的邻居——线程固定很少带来可衡量的好处。
- en: If you’re exploring pinning, it’s not enough to assume benefit—you need to benchmark
    it. Enabling `GODEBUG=schedtrace=1000,scheddetail=1` gives detailed insight into
    how goroutines are scheduled and whether contention or migration is actually a
    problem. Without that evidence, thread pinning is more likely to hinder than help.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在探索固定，仅仅假设有好处是不够的——你需要对其进行基准测试。启用`GODEBUG=schedtrace=1000,scheddetail=1`可以提供关于goroutine如何调度以及竞争或迁移是否真正成为问题的详细洞察。没有这样的证据，线程固定更有可能阻碍而不是帮助。
- en: 'Here''s how developers might pin threads cautiously:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是开发者可能谨慎固定线程的方法：
- en: '[PRE6]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Always pair such modifications with extensive metrics collection and scheduler
    tracing (`GODEBUG=schedtrace=1000,scheddetail=1`) to validate tangible gains over
    Go’s robust default scheduling behavior.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 总是搭配广泛的指标收集和调度跟踪（`GODEBUG=schedtrace=1000,scheddetail=1`）来验证相对于Go稳健默认调度行为的实际收益。
- en: CPU Affinity and External Tools[¶](#cpu-affinity-and-external-tools "Permanent
    link")
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CPU亲和力和外部工具[¶](#cpu-affinity-and-external-tools "永久链接")
- en: Using external tools like `taskset` or system calls such as `sched_setaffinity`
    can bind threads or processes to specific CPU cores. While theoretically beneficial
    for cache locality and predictable performance, extensive benchmarking consistently
    demonstrates limited practical value in most Go applications.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 使用像`taskset`这样的外部工具或系统调用如`sched_setaffinity`可以将线程或进程绑定到特定的CPU核心。虽然理论上对缓存局部性和可预测性能有益，但广泛的基准测试一致表明，在大多数Go应用程序中，其实际价值有限。
- en: 'Explicit CPU affinity management typically helps only in tightly controlled
    environments with:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 显式CPU亲和力管理通常仅在严格控制的环境中有所帮助：
- en: Real-time latency constraints (microsecond-level jitter).
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实时延迟约束（微秒级抖动）。
- en: Dedicated and isolated CPUs (e.g., via Linux kernel’s isolcpus).
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过Linux内核的`isolcpus`等手段，使用专用和隔离的CPU。
- en: Avoidance of thread migration on NUMA hardware.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在NUMA硬件上避免线程迁移。
- en: 'Example of cautious CPU affinity usage:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 谨慎使用CPU亲和力的示例：
- en: '[PRE7]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Without dedicated benchmarking and validation, these techniques may degrade
    performance, starve other processes, or introduce subtle latency regressions.
    Treat thread pinning and CPU affinity as highly specialized tools—effective only
    after meticulous measurement confirms their benefit.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 没有专门的基准测试和验证，这些技术可能会降低性能，使其他进程饥饿，或者引入微妙的延迟退化。将线程固定和CPU亲和力视为高度专业化的工具——只有在经过细致的测量确认其益处后才能有效。
- en: '* * *'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'Tuning Go at the scheduler level can unlock significant performance gains,
    but it demands an intimate understanding of P’s, M’s, and G’s. Blindly upping
    `GOMAXPROCS` or pinning threads without measurement can backfire. the advice is
    to treat these knobs as surgical tools: use `GODEBUG` traces to diagnose, isolate
    subsystems where affinity or pinning makes sense, and always validate with benchmarks
    and profiles.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在调度器级别调整Go可以解锁显著的性能提升，但这需要深入了解P（处理器）、M（系统线程）和G（goroutine）。盲目增加`GOMAXPROCS`或未测量就固定线程可能会适得其反。建议将这些旋钮视为外科手术工具：使用`GODEBUG`跟踪来诊断，隔离亲和力或固定有意义的子系统，并始终通过基准测试和配置文件进行验证。
- en: Go’s runtime is ever‑evolving. Upcoming work in preemptive scheduling and user‑level
    interrupts promises to reduce tail latency further and improve fairness. Until
    then, these low‑level levers remain some of the most powerful ways to squeeze
    every drop of performance from developer's Go services.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Go的运行时一直在不断发展。在抢占式调度和用户级中断方面的未来工作承诺将进一步降低尾部延迟并提高公平性。在此之前，这些低级杠杆仍然是榨取开发者Go服务性能的强大方式之一。
