- en: 'Connection Lifecycle Observability: From Dial to Close'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://goperf.dev/02-networking/connection_observability/](https://goperf.dev/02-networking/connection_observability/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Many observability systems expose only high-level HTTP metrics, but deeper insight
    comes from explicitly instrumenting each stage — DNS resolution, dialing, handshake,
    negotiation, reads/writes, and teardown. Observing the full lifecycle of a network
    connection provides the clarity needed to diagnose latency issues, identify failures,
    and relate resource usage to external behavior.
  prefs: []
  type: TYPE_NORMAL
- en: DNS Resolution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Every outbound connection begins with a name resolution step unless an IP is
    already known. DNS latency and failures often dominate connection setup time and
    can vary significantly depending on caching and resolver performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Capturing DNS resolution duration, errors and resulting address provides visibility
    into one of the least predictable phases. At this stage, it is valuable to:'
  prefs: []
  type: TYPE_NORMAL
- en: measure the time from query issuance to receiving an answer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: log the resolved IP addresses
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: surface transient and permanent failures distinctly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In Go, this can be achieved by wrapping the `net.Resolver`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This explicit measurement avoids relying on opaque metrics exposed by the OS
    resolver or libraries.
  prefs: []
  type: TYPE_NORMAL
- en: Dialing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After obtaining an address, the next phase is dialing — establishing a TCP (or
    other transport) connection. Here, round-trip latency to the target, route stability,
    and ephemeral port exhaustion can all surface.
  prefs: []
  type: TYPE_NORMAL
- en: Observing the dial phase often involves intercepting `net.Dialer`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Why trace here? Dialing failures can indicate downstream unavailability, but
    also local issues like SYN flood protection(1) or bad routing. Without a per-dial
    timestamp and error trace, identifying the locus of failure is guesswork.
  prefs: []
  type: TYPE_NORMAL
- en: When dialing fails, sometimes it’s not because of the network or the server
    being down, but because the local machine refuses to open more connections due
    to resource exhaustion or protective rate limiting.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Handshake and Negotiation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For secure connections, the next stage — cryptographic handshake — dominates.
    TLS negotiation can involve multiple round-trips, certificate validation, and
    cipher negotiation. Measuring this stage separately is necessary because it isolates
    pure network latency (dial) from cryptographic and policy enforcement costs (handshake).
  prefs: []
  type: TYPE_NORMAL
- en: The `crypto/tls` library in Go allows instrumentation of the handshake explicitly.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: A common misconception is that slow TLS handshakes always reflect bad network
    conditions; in practice, slow certificate validation or [OCSP](https://en.wikipedia.org/wiki/Online_Certificate_Status_Protocol)/[CRL](https://en.wikipedia.org/wiki/Certificate_revocation_list)
    checks are frequently to blame. Separating these phases helps pinpoint the cause.
  prefs: []
  type: TYPE_NORMAL
- en: Application Reads and Writes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once the connection is established and negotiated, application-level traffic
    proceeds as reads and writes. Observability at this stage is often the least precise
    yet most critical for correlating client-perceived latency to backend processing.
  prefs: []
  type: TYPE_NORMAL
- en: Instrumenting reads and writes directly yields fine-grained latency and throughput
    metrics. Wrapping a connection is a common strategy.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Why measure at this granularity? Reads and writes can block for reasons unrelated
    to connection establishment — buffer backpressure, TCP window exhaustion, or application
    pauses. Tracing them reveals whether observed slowness is due to I/O contention
    or upstream latency.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: Such a granularity level can affect your system performance. See [Why reduce
    logs?](#why-reduce-logs) for more details.
  prefs: []
  type: TYPE_NORMAL
- en: Teardown
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, connection closure often receives little attention, yet it can affect
    resource usage and connection pool behavior. Observing how long a connection takes
    to close and whether any errors surface during `Close()` is also useful. For TCP,
    the closure can be delayed by the OS until all `FIN`/`ACK` sequences complete,
    and sockets can linger in `TIME_WAIT`(1). Explicitly logging teardown metrics
    helps identify resource leaks.
  prefs: []
  type: TYPE_NORMAL
- en: Waiting for enough time to pass to be sure that all remaining packets on the
    connection have expired.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Correlating Spans and Errors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Capturing individual phase durations is not enough unless correlated into a
    coherent trace. Using a context-aware tracing library allows attaching phase spans
    to a single logical transaction.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, with [OpenTelemetry](https://opentelemetry.io/):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Wrapping each phase in a span enables stitching the timeline together and correlating
    specific errors with their originating stage. This level of detail is important
    when diagnosing production outages where symptoms often manifest far from their
    root cause.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting and Explaining Hangs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the more insidious problems in distributed systems is a hung connection
    — no forward progress but no immediate failure. Granular observability can distinguish
    between a hang during DNS, an unresponsive handshake, a blocked write due to full
    buffers, or a delayed FIN. For example, if the dial phase logs complete but handshake
    never finishes, attention should focus on certificate validation, MTU black holes,
    or middlebox interference. By maintaining per-phase timeouts and alerts based
    on historical baselines, such conditions can be detected and explained rather
    than simply categorized as "slow." This explains not just what failed, but why
    it failed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Beyond Logs: Metrics and Structured Events'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most likely you will need to reduce the volume of logs without losing essential
    information is critical for operating at scale. High-frequency, detailed logging
    of connection lifecycle events incurs I/O, CPU, and storage overhead. Without
    control mechanisms, it can overwhelm log aggregation systems, inflate costs, and
    even obscure the signal with noise. Log verbosity must therefore be managed carefully
    — keeping enough detail for diagnosis but avoiding excessive volume.
  prefs: []
  type: TYPE_NORMAL
- en: '[Zap](https://github.com/uber-go/zap) is used in these examples to demonstrate
    strategies for runtime configurability and structured logging, but the same principles
    apply to any capable logging library. The ability to change levels dynamically,
    emit structured data, and sample events makes it easier to implement these strategies.'
  prefs: []
  type: TYPE_NORMAL
- en: Why reduce logs?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Logging at fine granularity across thousands of connections per second can quickly
    generate terabytes of data, with much of it being repetitive and low-value. High
    log throughput can also cause contention with the application’s main execution
    paths, affecting latency. Furthermore, unbounded log growth burdens operators,
    complicates incident triage, and increases retention costs.
  prefs: []
  type: TYPE_NORMAL
- en: Techniques to control log volume
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Configurable log levels**: adjust verbosity at runtime depending on the situation
    — keeping production at `INFO` or `WARN` by default, switching to `DEBUG` or `TRACE`
    during investigation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sampling**: log only a subset of events under high load or log every Nth
    connection. Zap supports samplers that enforce rate limits and randomness.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Metrics-first, logs-for-anomalies**: promote phase durations and counters
    to metrics (e.g., Prometheus) and emit logs only when thresholds or percentiles
    are breached.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Aggregate phase data**: collect all phase timings and results per connection,
    then emit a single structured log event at the end instead of logging each phase
    as a separate line.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By combining these approaches, it is possible to retain meaningful observability
    with manageable overhead. Zap’s API simply illustrates how to implement these
    patterns effectively — **the key takeaway is that the design itself, not the library
    choice**, determines observability quality and cost.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few illustrative code snippets for these techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: Configurable log level
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Sampling logs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Aggregate per-connection log
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: While log statements and spans provide valuable information, a production-grade
    observability system benefits greatly from structured metrics collection and structured
    logging.
  prefs: []
  type: TYPE_NORMAL
- en: Prometheus is well-suited for collecting and querying time-series metrics from
    each phase, exposing them to dashboards and alerting rules. Instead of embedding
    phase durations in unstructured logs, it is advisable to design and expose clearly
    named metrics like `connection_dns_duration_seconds`, `connection_dial_errors_total`,
    and similar counters. These names are proposed examples that follow Prometheus
    naming conventions and should be defined explicitly in the implementation to ensure
    clarity and consistency.
  prefs: []
  type: TYPE_NORMAL
- en: 'For structured logging, libraries such as Uber’s `zap` offer fast, JSON-encoded
    logs that integrate cleanly with log aggregation systems like ELK or Splunk. Rather
    than free-form `Printf`, one can emit structured key-value pairs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Structured logs reduce the burden on downstream parsers and make correlating
    events between services more reliable. Combining these techniques — phase-level
    instrumentation, Prometheus metrics, and structured logs — transforms opaque failures
    into actionable insight. With this level of granularity and consistency, ambiguous
    symptoms become precise diagnoses, ensuring that both developers and operators
    can pinpoint not just that something is wrong, but exactly where and why.
  prefs: []
  type: TYPE_NORMAL
