- en: 'Practical Example: Profiling Networked Go Applications with pprof¶'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://goperf.dev/02-networking/gc-endpoint-profiling/](https://goperf.dev/02-networking/gc-endpoint-profiling/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This section walks through a demo application instrumented with benchmarking
    tools and runtime profiling to ground profiling concepts in a real-world context.
    It covers identifying performance bottlenecks, interpreting flame graphs, and
    analyzing system behavior under various simulated network conditions.
  prefs: []
  type: TYPE_NORMAL
- en: CPU Profiling in Networked Apps[¶](#cpu-profiling-in-networked-apps "Permanent
    link")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The demo application is intentionally designed to be as simple as possible to
    highlight key profiling concepts without unnecessary complexity. While the code
    and patterns used in the demo are basic, the profiling insights gained here are
    highly applicable to more complex, production-grade applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'To enable continuous profiling under load, we expose `pprof` via a dedicated
    HTTP endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: <details class="example"><summary>full `net-app`'s source code</summary>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]</details>'
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step will be to establish a connection with the profiled app and collect
    samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'View results interactively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: the actual `cpu.prof` path will be something like `$HOME/pprof/pprof.net-app.samples.cpu.004.pb.gz`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: or you can save the profiling graph as an `svg` image.
  prefs: []
  type: TYPE_NORMAL
- en: 'CPU Profiling Walkthrough: Load on the `/gc` Endpoint[¶](#cpu-profiling-walkthrough-load-on-the-gc-endpoint
    "Permanent link")'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We profiled the application during a 30-second load test targeting the `/gc`
    endpoint to see what happens under memory pressure. This handler was intentionally
    designed to trigger allocations and force garbage collection, which makes it a
    great candidate for observing runtime behavior under stress.
  prefs: []
  type: TYPE_NORMAL
- en: 'We used Go’s built-in profiler to capture a CPU trace:'
  prefs: []
  type: TYPE_NORMAL
- en: <details class="example"><summary>CPU profiling trace for the `/gc` endpoint</summary>
  prefs: []
  type: TYPE_NORMAL
- en: '[![](../Images/7a6a813e3f186af7e41a21bebb8a53e9.png)](../img/cpu.prof.png)</details>'
  prefs: []
  type: TYPE_NORMAL
- en: This gave us 3.02 seconds of sampled CPU activity out of 30 seconds of wall-clock
    time—a useful window into what the runtime and application were doing under pressure.
  prefs: []
  type: TYPE_NORMAL
- en: Where the Time Went[¶](#where-the-time-went "Permanent link")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: HTTP Stack Dominates the Surface[¶](#http-stack-dominates-the-surface "Permanent
    link")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As expected, the majority of CPU time was spent on request handling:'
  prefs: []
  type: TYPE_NORMAL
- en: '`http.(*conn).serve` accounted for nearly 58% of sampled time'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`http.serverHandler.ServeHTTP` appeared prominently as well'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This aligns with the fact that we were sustaining constant traffic. The Go HTTP
    stack is doing the bulk of the work, managing connections and dispatching requests.
  prefs: []
  type: TYPE_NORMAL
- en: Garbage Collection Overhead is Clearly Visible[¶](#garbage-collection-overhead-is-clearly-visible
    "Permanent link")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A large portion of CPU time was spent inside the garbage collector:'
  prefs: []
  type: TYPE_NORMAL
- en: '`runtime.gcDrain`, `runtime.scanobject`, and `runtime.gcBgMarkWorker` were
    all active'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combined with memory-related functions like `runtime.mallocgc`, these accounted
    for roughly 20% of total CPU time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This confirms that `gcHeavyHandler` is achieving its goal. What we care about
    is whether this kind of allocation pressure leaks into real-world handlers. If
    it does, we’re paying for it in latency and CPU churn.
  prefs: []
  type: TYPE_NORMAL
- en: I/O and Syscalls Take a Big Slice[¶](#io-and-syscalls-take-a-big-slice "Permanent
    link")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We also saw high syscall activity—especially from:'
  prefs: []
  type: TYPE_NORMAL
- en: '`syscall.syscall` (linked to `poll`, `Read`, and `Write`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bufio.Writer.Flush` and `http.response.finishRequest`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These functions reflect the cost of writing responses back to clients. For simple
    handlers, this is expected. But if your handler logic is lightweight and most
    of the time is spent just flushing data over TCP, it’s worth asking whether the
    payloads or buffer strategies could be optimized.
  prefs: []
  type: TYPE_NORMAL
- en: Scheduler Activity Is Non-Trivial[¶](#scheduler-activity-is-non-trivial "Permanent
    link")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Functions like `runtime.schedule`, `mcall`, and `findRunnable` were also on
    the board. These are Go runtime internals responsible for managing goroutines.
    Seeing them isn’t unusual during high-concurrency tests—but if they dominate,
    it often points to excessive goroutine churn or blocking behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 'Memory Profiling: Retained Heap from the `/gc` Endpoint[¶](#memory-profiling-retained-heap-from-the-gc-endpoint
    "Permanent link")'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We also captured a memory profile to complement the CPU view while hammering
    the `/gc` endpoint. This profile used the `inuse_space` metric, which shows how
    much heap memory is actively retained by each function at the time of capture.
  prefs: []
  type: TYPE_NORMAL
- en: 'We triggered the profile with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: <details class="example"><summary>Memory profiling for the `/gc` endpoint</summary>
  prefs: []
  type: TYPE_NORMAL
- en: '[![](../Images/8077552e3f37816c102f22210cddd49b.png)](../img/mem.prof.png)</details>'
  prefs: []
  type: TYPE_NORMAL
- en: 'At the time of capture, the application retained 649MB of heap memory, and
    almost all of it—99.46%—was attributed to a single function: `gcHeavyHandler`.
    This was expected. The handler simulates allocation pressure by creating 10KB
    slices in a tight loop. Every 100th slice is added to a global variable to simulate
    long-lived memory.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s what the handler does:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The flamegraph confirmed what we expected:'
  prefs: []
  type: TYPE_NORMAL
- en: '`gcHeavyHandler` accounted for nearly all memory in use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The path traced cleanly from the HTTP connection, through the Go router stack,
    into the handler logic.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No significant allocations came from elsewhere—this was a focused, controlled
    memory pressure scenario.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This type of profile is valuable because it reveals what is still being held
    in memory, not just what was allocated. This view is often the most revealing
    for diagnosing leaks, retained buffers, or forgotten references.
  prefs: []
  type: TYPE_NORMAL
- en: 'Summary: CPU and Memory Profiling of the `/gc` Endpoint[¶](#summary-cpu-and-memory-profiling-of-the-gc-endpoint
    "Permanent link")'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `/gc` endpoint was intentionally built to simulate high allocation pressure
    and GC activity. Profiling this handler under load gave us a clean, focused view
    of how the Go runtime behaves when pushed to its memory limits.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the **CPU profile**, we saw that:'
  prefs: []
  type: TYPE_NORMAL
- en: As expected, most of the time was spent in the HTTP handler path during sustained
    load.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nearly 20% of CPU samples were attributed to memory allocation and garbage collection.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Syscall activity was high, mostly from writing responses.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Go scheduler was moderately active, managing the concurrent goroutines handling
    traffic.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'From the **memory profile**, we captured 649MB of live heap usage, with **99.46%
    of it retained by `gcHeavyHandler`**. This matched our expectations: the handler
    deliberately retains every 100th 10KB allocation to simulate long-lived data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Together, these profiles give us confidence that the `/gc` endpoint behaves
    as intended under synthetic pressure:'
  prefs: []
  type: TYPE_NORMAL
- en: It creates meaningful CPU and memory load.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It exposes the cost of sustained allocations and GC cycles.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It provides a predictable environment for testing optimizations or GC tuning
    strategies.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
