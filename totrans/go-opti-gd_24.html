<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>GOMAXPROCS, epoll/kqueue, and Scheduler-Level Tuning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>GOMAXPROCS, epoll/kqueue, and Scheduler-Level Tuning</h1>
<blockquote>原文：<a href="https://goperf.dev/02-networking/a-bit-more-tuning/">https://goperf.dev/02-networking/a-bit-more-tuning/</a></blockquote>
                
                  


  
  



<p>Go applications operating at high concurrency levels frequently encounter performance ceilings that are not attributable to CPU saturation. These limitations often stem from runtime-level mechanics: how goroutines (G) are scheduled onto logical processors (P) via operating system threads (M), how blocking operations affect thread availability, and how the runtime interacts with kernel facilities like <code>epoll</code> or <code>kqueue</code> for I/O readiness.</p>
<p>Unlike surface-level code optimization, resolving these issues requires awareness of the Go scheduler’s internal design, particularly how GOMAXPROCS governs execution parallelism and how thread contention, cache locality, and syscall latency emerge under load. Misconfigured runtime settings can lead to excessive context switching, stalled P’s, and degraded throughput despite available cores.</p>
<p>System-level tuning—through CPU affinity, thread pinning, and scheduler introspection—provides a critical path to improving latency and throughput in multicore environments. When paired with precise benchmarking and observability, these adjustments allow Go services to scale more predictably and fully take advantage of modern hardware architectures.</p>
<h2 id="understanding-gomaxprocs">Understanding GOMAXPROCS</h2>
<p>In Go, <code>GOMAXPROCS</code> defines the maximum number of operating system threads (M’s) simultaneously executing user‑level Go code (G’s). It’s set to the developer's machine’s logical CPU count by default. Under the hood, the scheduler exposes P’s (processors) equal to <code>GOMAXPROCS</code>. Each P hosts a run queue of G’s and binds to a single M to execute Go code.</p>
<div class="highlight"><pre><span/><code><span class="kn">package</span><span class="w"> </span><span class="nx">main</span>

<span class="kn">import</span><span class="w"> </span><span class="p">(</span>
<span class="w">    </span><span class="s">"fmt"</span>
<span class="w">    </span><span class="s">"runtime"</span>
<span class="p">)</span>

<span class="kd">func</span><span class="w"> </span><span class="nx">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// Show current value</span>
<span class="w">    </span><span class="nx">fmt</span><span class="p">.</span><span class="nx">Printf</span><span class="p">(</span><span class="s">"GOMAXPROCS = %d\n"</span><span class="p">,</span><span class="w"> </span><span class="nx">runtime</span><span class="p">.</span><span class="nx">GOMAXPROCS</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

<span class="w">    </span><span class="c1">// Set to 4 and confirm</span>
<span class="w">    </span><span class="nx">prev</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">runtime</span><span class="p">.</span><span class="nx">GOMAXPROCS</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="w">    </span><span class="nx">fmt</span><span class="p">.</span><span class="nx">Printf</span><span class="p">(</span><span class="s">"Changed from %d to %d\n"</span><span class="p">,</span><span class="w"> </span><span class="nx">prev</span><span class="p">,</span><span class="w"> </span><span class="nx">runtime</span><span class="p">.</span><span class="nx">GOMAXPROCS</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
<span class="p">}</span>
</code></pre></div>
<p>When developers increase <code>GOMAXPROCS</code>, developers allow more P’s—and therefore more OS threads—to run Go‑routines in parallel. That often boosts performance for CPU‑bound workloads. However, more P’s also incur more context switches, more cache thrashing, and potentially more contention in shared data structures (e.g., the garbage collector’s work queues). It's important to understand that blindly scaling past the sweet spot can actually degrade latency.</p>
<h2 id="diving-into-gos-scheduler-internals">Diving into Go’s Scheduler Internals</h2>
<p>Go’s scheduler organizes three core actors: G (goroutine), M (OS thread), and P (logical processor), <a href="../networking-internals/#goroutines-and-the-runtime-scheduler">see more details here</a>. When a goroutine makes a blocking syscall, its M detaches from its P, returning the P to the global scheduler so another M can pick it up. This design prevents syscalls from starving CPU‑bound goroutines.</p>
<p>The scheduler uses work stealing: each P maintains a local run queue, and idle P’s will steal work from busier peers. If developers set GOMAXPROCS too high, developers will see diminishing returns in stolen work versus the overhead of balancing those run queues.</p>
<p>Enabling scheduler tracing via <code>GODEBUG</code> can reveal fine grained metrics:</p>
<div class="highlight"><pre><span/><code><span class="nv">GODEBUG</span><span class="o">=</span><span class="nv">schedtrace</span><span class="o">=</span><span class="m">1000</span>,scheddetail<span class="o">=</span><span class="m">1</span><span class="w"> </span>go<span class="w"> </span>run<span class="w"> </span>main.go
</code></pre></div>
<ul>
<li><code>schedtrace=1000</code> instructs the runtime to print scheduler state every 1000 milliseconds (1 second).</li>
<li><code>scheddetail=1</code> enables additional information per logical processor (P), such as individual run queue lengths.</li>
</ul>
<p>Each printed trace includes statistics like:</p>
<div class="highlight"><pre><span/><code>SCHED 3024ms: gomaxprocs=14 idleprocs=14 threads=26 spinningthreads=0 needspinning=0 idlethreads=20 runqueue=0 gcwaiting=false nmidlelocked=1 stopwait=0 sysmonwait=false
  P0: status=0 schedtick=173 syscalltick=3411 m=nil runqsize=0 gfreecnt=6 timerslen=0
  ...
  P13: status=0 schedtick=96 syscalltick=310 m=nil runqsize=0 gfreecnt=2 timerslen=0
  M25: p=nil curg=nil mallocing=0 throwing=0 preemptoff= locks=0 dying=0 spinning=false blocked=true lockedg=nil
  ...
</code></pre></div>
<p>The first line reports global scheduler state including whether garbage collection is blocking (gcwaiting), if spinning threads are needed, and idle thread counts.</p>
<p>Each P line details the logical processor's scheduler activity, including the number of times it's scheduled (schedtick), system call activity (syscalltick), timers, and free goroutine slots.</p>
<p>The M lines correspond to OS threads. Each line shows which goroutine—if any—is running on that thread, whether the thread is idle, spinning, or blocked, along with memory allocation activity and lock states.</p>
<p>This view makes it easier to spot not only classic concurrency bottlenecks but also deeper issues: scheduler delays, blocking syscalls, threads that spin without doing useful work, or CPU cores that sit idle when they shouldn’t. The output reveals patterns that aren’t visible from logs or metrics alone.</p>
<ul>
<li><code>gomaxprocs=14</code>: Number of logical processors (P’s).</li>
<li><code>idleprocs=14</code>: All processors are idle, indicating no runnable goroutines.</li>
<li><code>threads=26</code>: Number of M’s (OS threads) created.</li>
<li><code>spinningthreads=0</code>: No threads are actively searching for work.</li>
<li><code>needspinning=0</code>: No additional spinning threads are requested by the scheduler.</li>
<li><code>idlethreads=20</code>: Number of OS threads currently idle.</li>
<li><code>runqueue=0</code>: Global run queue is empty.</li>
<li><code>gcwaiting=false</code>: Garbage collector is not blocking execution.</li>
<li><code>nmidlelocked=1</code>: One P is locked to a thread that is currently idle.</li>
<li><code>stopwait=0</code>: No goroutines waiting to stop the world.</li>
<li><code>sysmonwait=false</code>: The system monitor is actively running, not sleeping.</li>
</ul>
<p>The global run queue holds goroutines that are not bound to any specific P or that overflowed local queues. In contrast, each logical processor (P) maintains a local run queue of goroutines it is responsible for scheduling. Goroutines are preferentially enqueued locally for performance: local queues avoid lock contention and improve cache locality. It may be placed on the global queue only when a P's local queue is full, or a goroutine originates from outside a P (e.g., from a syscall).</p>
<p>This dual-queue strategy reduces synchronization overhead across P’s and enables efficient scheduling under high concurrency. Understanding the ratio of local vs global queue activity helps diagnose whether the system is under-provisioned, improperly balanced, or suffering from excessive cross-P migrations.</p>
<p>These insights help quantify how efficiently goroutines are scheduled, how much parallelism is actually utilized, and whether the system is under- or over-provisioned in terms of logical processors. Observing these patterns under load is crucial when adjusting <code>GOMAXPROCS</code>, diagnosing tail latency, or identifying scheduler contention.</p>
<h2 id="netpoller-deep-dive-into-epoll-on-linux-and-kqueue-on-bsd">Netpoller: Deep Dive into epoll on Linux and kqueue on BSD</h2>
<p>In any Go application handling high connection volumes, the network poller plays a critical behind-the-scenes role. At its core, Go uses the OS-level multiplexing facilities—<code>epoll</code> on Linux and <code>kqueue</code> on BSD/macOS—to monitor thousands of sockets concurrently with minimal threads. The runtime leverages these mechanisms efficiently, but understanding how and why reveals opportunities for tuning, especially under demanding loads.</p>
<p>When a goroutine initiates a network operation like reading from a TCP connection, the runtime doesn't immediately block the underlying thread. Instead, it registers the file descriptor with the poller—using <code>epoll_ctl</code> in edge-triggered mode or <code>EV_SET</code> with <code>EVFILT_READ</code>—and parks the goroutine. The actual thread (M) becomes free to run other goroutines. When data arrives, the kernel signals the poller thread, which in turn wakes the appropriate goroutine by scheduling it onto a P’s run queue. This wakeup process minimizes contention by relying on per-P notification lists and avoids runtime lock bottlenecks.</p>
<p>Go uses edge-triggered notifications, which signal only on state transitions—like new data becoming available. This design requires the application to drain sockets fully during each wakeup or risk missing future events. While more complex than level-triggered behavior, edge-triggered mode significantly reduces syscall overhead under load.</p>
<p>Here's a simplified version of what happens under the hood during a read operation:</p>
<div class="highlight"><pre><span/><code><span class="kd">func</span><span class="w"> </span><span class="nx">pollAndRead</span><span class="p">(</span><span class="nx">conn</span><span class="w"> </span><span class="nx">net</span><span class="p">.</span><span class="nx">Conn</span><span class="p">)</span><span class="w"> </span><span class="p">([]</span><span class="kt">byte</span><span class="p">,</span><span class="w"> </span><span class="kt">error</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="nx">buf</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nb">make</span><span class="p">([]</span><span class="kt">byte</span><span class="p">,</span><span class="w"> </span><span class="mi">4096</span><span class="p">)</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nx">n</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">conn</span><span class="p">.</span><span class="nx">Read</span><span class="p">(</span><span class="nx">buf</span><span class="p">)</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="nx">n</span><span class="w"> </span><span class="p">&gt;</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="k">return</span><span class="w"> </span><span class="nx">buf</span><span class="p">[:</span><span class="nx">n</span><span class="p">],</span><span class="w"> </span><span class="kc">nil</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="p">!</span><span class="nx">isTemporary</span><span class="p">(</span><span class="nx">err</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="k">return</span><span class="w"> </span><span class="kc">nil</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">        </span><span class="c1">// Data not ready yet — goroutine will be parked until poller wakes it</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div>
<p>Internally, Go runs a dedicated poller thread that loops on <code>epoll_wait</code> or <code>kevent</code>, collecting batches of events (typically 512 at a time). After the call returns, the runtime processes these events, distributing wakeups across logical processors to prevent any single P from becoming a bottleneck. To further promote scheduling fairness, the poller thread may rotate across P’s periodically, a behavior governed by <code>GODEBUG=netpollWaitLatency</code>.</p>
<p>Go’s runtime is optimized to reduce unnecessary syscalls and context switches. All file descriptors are set to non-blocking, which allows the poller thread to remain responsive. To avoid the thundering herd problem—where multiple threads wake on the same socket—the poller ensures only one goroutine handles a given FD event at a time.</p>
<p>The design goes even further by aligning the circular event buffer with cache lines and distributing wakeups via per-P lists. These details matter at scale. With proper alignment and locality, Go reduces CPU cache contention when thousands of connections are active.</p>
<p>For developers looking to inspect poller behavior, enabling tracing with <code>GODEBUG=netpoll=1</code> can surface system-level latencies and epoll activity. Additionally, the <code>GODEBUG=netpollWaitLatency=200</code> flag configures the poller’s willingness to hand off to another P every 200 microseconds. That’s particularly helpful in debugging idle P starvation or evaluating fairness in high-throughput systems.</p>
<p>Here's a small experiment that logs event activity:</p>
<div class="highlight"><pre><span/><code><span class="nv">GODEBUG</span><span class="o">=</span><span class="nv">netpoll</span><span class="o">=</span><span class="m">1</span><span class="w"> </span>go<span class="w"> </span>run<span class="w"> </span>main.go
</code></pre></div>
<p>You’ll see log lines like:</p>
<div class="highlight"><pre><span/><code>runtime: netpoll: poll returned n=3
runtime: netpoll: waking g=102 for fd=5
</code></pre></div>
<p>Most developers never need to think about this machinery—and they shouldn't. But these details become valuable in edge cases, like high-throughput HTTP proxies or latency-sensitive services dealing with hundreds of thousands of concurrent sockets. Tuning parameters like <code>GOMAXPROCS</code>, adjusting the event buffer size, or modifying poller wake-up intervals can yield measurable performance improvements, particularly in tail latencies.</p>
<p>For example, in a system handling hundreds of thousands of concurrent HTTP/2 streams, increasing <code>GOMAXPROCS</code> while using <code>GODEBUG=netpollWaitLatency=100</code> helped reduce the 99th percentile read latency by over 15%, simply by preventing poller starvation under I/O backpressure.</p>
<p>As with all low-level tuning, it's not about changing knobs blindly. It's about knowing what Go’s netpoller is doing, why it’s structured the way it is, and where its boundaries can be nudged for just a bit more efficiency—when measurements tell you it’s worth it.</p>
<h2 id="thread-pinning-with-lockosthread-and-godebug-flags">Thread Pinning with <code>LockOSThread</code> and <code>GODEBUG</code> Flags</h2>
<p>Go offers tools like <code>runtime.LockOSThread()</code> to pin a goroutine to a specific OS thread, but in most real-world applications, the payoff is minimal. Benchmarks consistently show that for typical server workloads—especially those that are CPU-bound—Go’s scheduler handles thread placement well without manual intervention. Introducing thread pinning tends to add complexity without delivering measurable gains.</p>
<p>There are exceptions. In ultra-low-latency or real-time systems, pinning can help reduce jitter by avoiding thread migration. But these gains typically require isolated CPU cores, tightly controlled environments, and strict latency targets. In practice, that means bare metal. On shared infrastructure—especially in cloud environments like AWS where cores are virtualized and noisy neighbors are common—thread pinning rarely delivers any measurable benefit.</p>
<p>If you’re exploring pinning, it’s not enough to assume benefit—you need to benchmark it. Enabling <code>GODEBUG=schedtrace=1000,scheddetail=1</code> gives detailed insight into how goroutines are scheduled and whether contention or migration is actually a problem. Without that evidence, thread pinning is more likely to hinder than help.</p>
<p>Here's how developers might pin threads cautiously:</p>
<div class="highlight"><pre><span/><code><span class="nx">runtime</span><span class="p">.</span><span class="nx">LockOSThread</span><span class="p">()</span>
<span class="k">defer</span><span class="w"> </span><span class="nx">runtime</span><span class="p">.</span><span class="nx">UnlockOSThread</span><span class="p">()</span>

<span class="c1">// perform critical latency-sensitive work here</span>
</code></pre></div>
<p>Always pair such modifications with extensive metrics collection and scheduler tracing (<code>GODEBUG=schedtrace=1000,scheddetail=1</code>) to validate tangible gains over Go’s robust default scheduling behavior.</p>
<h2 id="cpu-affinity-and-external-tools">CPU Affinity and External Tools</h2>
<p>Using external tools like <code>taskset</code> or system calls such as <code>sched_setaffinity</code> can bind threads or processes to specific CPU cores. While theoretically beneficial for cache locality and predictable performance, extensive benchmarking consistently demonstrates limited practical value in most Go applications.</p>
<p>Explicit CPU affinity management typically helps only in tightly controlled environments with:</p>
<ul>
<li>Real-time latency constraints (microsecond-level jitter).</li>
<li>Dedicated and isolated CPUs (e.g., via Linux kernel’s isolcpus).</li>
<li>Avoidance of thread migration on NUMA hardware.</li>
</ul>
<p>Example of cautious CPU affinity usage:</p>
<div class="highlight"><pre><span/><code><span class="kd">func</span><span class="w"> </span><span class="nx">setAffinity</span><span class="p">(</span><span class="nx">cpuList</span><span class="w"> </span><span class="p">[]</span><span class="kt">int</span><span class="p">)</span><span class="w"> </span><span class="kt">error</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="nx">pid</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">os</span><span class="p">.</span><span class="nx">Getpid</span><span class="p">()</span>
<span class="w">    </span><span class="kd">var</span><span class="w"> </span><span class="nx">mask</span><span class="w"> </span><span class="nx">unix</span><span class="p">.</span><span class="nx">CPUSet</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="nx">_</span><span class="p">,</span><span class="w"> </span><span class="nx">cpu</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="k">range</span><span class="w"> </span><span class="nx">cpuList</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nx">mask</span><span class="p">.</span><span class="nx">Set</span><span class="p">(</span><span class="nx">cpu</span><span class="p">)</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="nx">unix</span><span class="p">.</span><span class="nx">SchedSetaffinity</span><span class="p">(</span><span class="nx">pid</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="nx">mask</span><span class="p">)</span>
<span class="p">}</span>

<span class="kd">func</span><span class="w"> </span><span class="nx">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="nx">runtime</span><span class="p">.</span><span class="nx">LockOSThread</span><span class="p">()</span>
<span class="w">    </span><span class="k">defer</span><span class="w"> </span><span class="nx">runtime</span><span class="p">.</span><span class="nx">UnlockOSThread</span><span class="p">()</span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">setAffinity</span><span class="p">([]</span><span class="kt">int</span><span class="p">{</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">});</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nx">log</span><span class="p">.</span><span class="nx">Fatalf</span><span class="p">(</span><span class="s">"CPU affinity failed: %v"</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="p">)</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="c1">// perform critical work with confirmed benefit</span>
<span class="p">}</span>
</code></pre></div>
<p>Without dedicated benchmarking and validation, these techniques may degrade performance, starve other processes, or introduce subtle latency regressions. Treat thread pinning and CPU affinity as highly specialized tools—effective only after meticulous measurement confirms their benefit.</p>
<hr/>
<p>Tuning Go at the scheduler level can unlock significant performance gains, but it demands an intimate understanding of P’s, M’s, and G’s. Blindly upping <code>GOMAXPROCS</code> or pinning threads without measurement can backfire. the advice is to treat these knobs as surgical tools: use <code>GODEBUG</code> traces to diagnose, isolate subsystems where affinity or pinning makes sense, and always validate with benchmarks and profiles.</p>
<p>Go’s runtime is ever‑evolving. Upcoming work in preemptive scheduling and user‑level interrupts promises to reduce tail latency further and improve fairness. Until then, these low‑level levers remain some of the most powerful ways to squeeze every drop of performance from developer's Go services.</p>









  




                
                  
</body>
</html>