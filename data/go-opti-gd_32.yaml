- en: GOMAXPROCS, epoll/kqueue和调度器级别的调整[¶]
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[https://goperf.dev/02-networking/a-bit-more-tuning/](https://goperf.dev/02-networking/a-bit-more-tuning/)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 在高并发级别运行的Go应用程序经常遇到性能瓶颈，这些瓶颈并非由CPU饱和引起。这些限制通常源于运行时级别的机制：goroutines（G）如何通过操作系统线程（M）调度到逻辑处理器（P），阻塞操作如何影响线程可用性，以及运行时如何与内核设施（如`epoll`或`kqueue`）交互以进行I/O就绪。
  prefs: []
  type: TYPE_NORMAL
- en: 与表面代码优化不同，解决这些问题需要了解Go调度器的内部设计，特别是GOMAXPROCS如何控制执行并行性，以及线程竞争、缓存局部性和系统调用延迟如何在负载下出现。错误的运行时设置可能导致上下文切换过多、P停滞和吞吐量下降，尽管有可用的核心。
  prefs: []
  type: TYPE_NORMAL
- en: 系统级别的调整——通过CPU亲和性、线程固定和调度器内省——是提高多核环境中延迟和吞吐量的关键路径。当与精确基准测试和可观察性相结合时，这些调整使Go服务能够更可预测地扩展，并充分利用现代硬件架构。
  prefs: []
  type: TYPE_NORMAL
- en: 理解GOMAXPROCS[¶](#understanding-gomaxprocs "永久链接")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 在Go中，`GOMAXPROCS`定义了同时执行用户级Go代码（G）的最大操作系统线程数（M）。默认情况下，它设置为开发者的机器的逻辑CPU数量。在底层，调度器暴露了等于`GOMAXPROCS`的P（处理器）。每个P托管一个G的运行队列，并绑定到一个M来执行Go代码。
  prefs: []
  type: TYPE_NORMAL
- en: '```go'
  prefs: []
  type: TYPE_NORMAL
- en: package main
  prefs: []
  type: TYPE_NORMAL
- en: import (
  prefs: []
  type: TYPE_NORMAL
- en: '"fmt"'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '"runtime"'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: func main() {
  prefs: []
  type: TYPE_NORMAL
- en: // Show current value
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: fmt.Printf("GOMAXPROCS = %d\n", runtime.GOMAXPROCS(0))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: // Set to 4 and confirm
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: prev := runtime.GOMAXPROCS(4)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: fmt.Printf("Changed from %d to %d\n", prev, runtime.GOMAXPROCS(0))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: 当开发者增加`GOMAXPROCS`时，开发者允许更多的P（因此更多的OS线程）并行运行Go-routines。这通常能提升CPU密集型工作的性能。然而，更多的P也会带来更多的上下文切换、更多的缓存冲突，以及共享数据结构（例如，垃圾收集器的工作队列）的潜在竞争。重要的是要理解，盲目地超过最佳点实际上可能会降低延迟。
  prefs: []
  type: TYPE_NORMAL
- en: 深入Go的调度器内部[¶](#diving-into-gos-scheduler-internals "永久链接")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Go的调度器组织了三个核心参与者：G（goroutine）、M（OS线程）和P（逻辑处理器），[更多信息请见此处](../networking-internals/#goroutines-and-the-runtime-scheduler)。当一个goroutine执行阻塞系统调用时，它的M会从它的P上分离，将P返回给全局调度器，以便另一个M可以取走。这种设计防止了系统调用使CPU密集型goroutine饿死。
  prefs: []
  type: TYPE_NORMAL
- en: 调度器使用工作窃取：每个P维护一个本地运行队列，空闲的P将从忙碌的同伴那里窃取工作。如果开发者将GOMAXPROCS设置得太高，开发者将看到窃取工作与平衡这些运行队列的开销之间的回报递减。
  prefs: []
  type: TYPE_NORMAL
- en: 通过`GODEBUG`启用调度器跟踪可以揭示细粒度的指标：
  prefs: []
  type: TYPE_NORMAL
- en: '```go'
  prefs: []
  type: TYPE_NORMAL
- en: GODEBUG=schedtrace=1000,scheddetail=1 go run main.go
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: '`schedtrace=1000` 指示运行时每1000毫秒（1秒）打印一次调度器状态。'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scheddetail=1` 启用每个逻辑处理器（P）的附加信息，例如单个运行队列长度。'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 每个打印的跟踪包括如下统计信息：
  prefs: []
  type: TYPE_NORMAL
- en: '```go'
  prefs: []
  type: TYPE_NORMAL
- en: 'SCHED 3024ms: gomaxprocs=14 idleprocs=14 threads=26 spinningthreads=0 needspinning=0
    idlethreads=20 runqueue=0 gcwaiting=false nmidlelocked=1 stopwait=0 sysmonwait=false'
  prefs: []
  type: TYPE_NORMAL
- en: 'P0: status=0 schedtick=173 syscalltick=3411 m=nil runqsize=0 gfreecnt=6 timerslen=0'
  prefs: []
  type: TYPE_NORMAL
- en: '...'
  prefs: []
  type: TYPE_NORMAL
- en: 'P13: status=0 schedtick=96 syscalltick=310 m=nil runqsize=0 gfreecnt=2 timerslen=0'
  prefs: []
  type: TYPE_NORMAL
- en: 'M25: p=nil curg=nil mallocing=0 throwing=0 preemptoff= locks=0 dying=0 spinning=false
    blocked=true lockedg=nil'
  prefs: []
  type: TYPE_NORMAL
- en: '...'
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: 第一行报告全局调度器状态，包括垃圾收集是否阻塞（gcwaiting）、是否需要自旋线程以及空闲线程计数。
  prefs: []
  type: TYPE_NORMAL
- en: 每个P行详细说明了逻辑处理器的调度活动，包括其被调度的次数（schedtick）、系统调用活动（syscalltick）、计时器和空闲goroutine槽位。
  prefs: []
  type: TYPE_NORMAL
- en: M行对应于操作系统线程。每一行显示哪个goroutine（如果有）正在该线程上运行，线程是否空闲、自旋或阻塞，以及内存分配活动和锁状态。
  prefs: []
  type: TYPE_NORMAL
- en: 这种视图不仅有助于发现经典的并发瓶颈，还能发现更深层次的问题：调度延迟、阻塞的系统调用、不进行有用工作的自旋线程，或不应空闲的CPU核心。输出揭示了仅从日志或指标中无法看到的模式。
  prefs: []
  type: TYPE_NORMAL
- en: '`gomaxprocs=14`: 逻辑处理器（P）的数量。'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`idleprocs=14`: 所有处理器都空闲，表示没有可运行的goroutines。'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`threads=26`: 创建的M的数量（操作系统线程）。'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spinningthreads=0`: 没有线程正在积极寻找工作。'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`needspinning=0`: 调度器不需要额外的自旋线程。'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`idlethreads=20`: 当前空闲的操作系统线程数量。'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`runqueue=0`: 全局运行队列为空。'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gcwaiting=false`: 垃圾收集器不会阻塞执行。'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nmidlelocked=1`: 一个P被锁定到一个当前空闲的线程上。'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stopwait=0`: 没有goroutines正在等待停止世界。'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sysmonwait=false`: 系统监控器正在积极运行，而不是休眠。'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 全局运行队列包含未绑定到任何特定P或超出本地队列的goroutines。相比之下，每个逻辑处理器（P）维护一个它负责调度的goroutines的本地运行队列。为了提高性能，goroutines优先在本地队列中入队：本地队列避免了锁竞争并提高了缓存局部性。只有在P的本地队列已满或goroutine来自P外部（例如，来自系统调用）时，才将其放置在全局队列上。
  prefs: []
  type: TYPE_NORMAL
- en: 这种双队列策略降低了P之间的同步开销，并在高并发下实现了高效的调度。了解本地与全局队列活动比率有助于诊断系统是否配置不足、不平衡或遭受过多的跨P迁移。
  prefs: []
  type: TYPE_NORMAL
- en: 这些见解有助于量化goroutines的调度效率，实际利用了多少并行性，以及从逻辑处理器的角度来看，系统是否配置不足或过剩。在调整`GOMAXPROCS`、诊断尾部延迟或识别调度竞争时，在负载下观察这些模式至关重要。
  prefs: []
  type: TYPE_NORMAL
- en: Netpoller：深入Linux上的epoll和BSD上的kqueue[¶](#netpoller-deep-dive-into-epoll-on-linux-and-kqueue-on-bsd
    "永久链接")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 在任何处理高连接量的Go应用程序中，网络轮询器在幕后发挥着关键作用。在核心上，Go使用操作系统级别的多路复用设施——Linux上的`epoll`和BSD/macOS上的`kqueue`——以最小化线程的数量同时监控数千个套接字。运行时有效地利用了这些机制，但了解其如何以及为什么这样做揭示了调优的机会，尤其是在负载需求高的情况下。
  prefs: []
  type: TYPE_NORMAL
- en: 当一个goroutine启动一个网络操作，如从TCP连接中读取时，运行时不会立即阻塞底层线程。相反，它将文件描述符注册到轮询器——使用边缘触发模式的`epoll_ctl`或`EV_SET`与`EVFILT_READ`——并将goroutine挂起。实际的线程（M）变得空闲，可以运行其他goroutines。当数据到达时，内核向轮询线程发出信号，然后轮询线程通过将其调度到P的运行队列上唤醒适当的goroutine。这个过程通过依赖每个P的通知列表来最小化竞争，并避免运行时锁瓶颈。
  prefs: []
  type: TYPE_NORMAL
- en: Go使用边缘触发通知，仅在状态转换时发出信号——例如新数据可用。这种设计要求在每次唤醒时完全清空套接字，否则可能会错过未来的事件。虽然比电平触发行为更复杂，但边缘触发模式在负载下显著减少了系统调用开销。
  prefs: []
  type: TYPE_NORMAL
- en: 下面是一个简化版的在读取操作期间底层发生的操作：
  prefs: []
  type: TYPE_NORMAL
- en: '```go'
  prefs: []
  type: TYPE_NORMAL
- en: func pollAndRead(conn net.Conn) ([]byte, error) {
  prefs: []
  type: TYPE_NORMAL
- en: buf := make([]byte, 4096)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: for {
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: n, err := conn.Read(buf)
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: if n > 0 {
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: return buf[:n], nil
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: if err != nil && !isTemporary(err) {
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: return nil, err
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: // Data not ready yet — goroutine will be parked until poller wakes it
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: 在内部，Go运行一个专门的轮询线程，该线程循环在`epoll_wait`或`kevent`上，每次收集一批事件（通常每次512个）。在调用返回后，运行时处理这些事件，将唤醒分布在逻辑处理器上，以防止任何单个P成为瓶颈。为了进一步促进调度公平性，轮询线程可能会定期在P之间轮换，这种行为由`GODEBUG=netpollWaitLatency`控制。
  prefs: []
  type: TYPE_NORMAL
- en: Go的运行时经过优化，以减少不必要的系统调用和上下文切换。所有文件描述符都设置为非阻塞，这使得轮询线程可以保持响应。为了避免多个线程在同一套接字上唤醒的“雷鸣之群”问题，轮询器确保一次只有一个goroutine处理给定的FD事件。
  prefs: []
  type: TYPE_NORMAL
- en: 设计更进一步，通过将环形事件缓冲区与缓存行对齐，并通过每个 P 列表分发唤醒，这些细节在规模上很重要。通过适当的对齐和局部性，Go 在数千个连接活跃时减少了
    CPU 缓存竞争。
  prefs: []
  type: TYPE_NORMAL
- en: 对于想要检查轮询器行为的开发者，通过启用 `GODEBUG=netpoll=1` 的跟踪可以揭示系统级延迟和 epoll 活动。此外，`GODEBUG=netpollWaitLatency=200`
    标志配置了轮询器每 200 微秒将任务转交给另一个 P 的意愿。这在调试空闲 P 饥饿或评估高吞吐量系统中的公平性时特别有帮助。
  prefs: []
  type: TYPE_NORMAL
- en: 这是一个记录事件活动的简单实验：
  prefs: []
  type: TYPE_NORMAL
- en: '```go'
  prefs: []
  type: TYPE_NORMAL
- en: GODEBUG=netpoll=1 go run main.go
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: 你会看到这样的日志行：
  prefs: []
  type: TYPE_NORMAL
- en: '```go'
  prefs: []
  type: TYPE_NORMAL
- en: 'runtime: netpoll: poll returned n=3'
  prefs: []
  type: TYPE_NORMAL
- en: 'runtime: netpoll: waking g=102 for fd=5'
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: 大多数开发者无需考虑这种机制——他们也不应该考虑。但在边缘情况下，如高吞吐量 HTTP 代理或处理数十万个并发套接字的延迟敏感型服务时，这些细节变得很有价值。调整参数如
    `GOMAXPROCS`、调整事件缓冲区大小或修改轮询器唤醒间隔可以带来可衡量的性能提升，尤其是在尾部延迟方面。
  prefs: []
  type: TYPE_NORMAL
- en: 例如，在一个处理数十万个并发 HTTP/2 流的系统中，在启用 `GODEBUG=netpollWaitLatency=100` 的同时增加 `GOMAXPROCS`，可以仅通过防止在
    I/O 反压下发生轮询器饥饿，就将 99 分位读取延迟降低了超过 15%。
  prefs: []
  type: TYPE_NORMAL
- en: 与所有低级调整一样，这并不是关于盲目地改变旋钮。这是关于了解 Go 的 netpoller 在做什么，为什么它以这种方式构建，以及在哪里可以稍微调整边界以获得更多效率——当测量表明这是值得的时候。
  prefs: []
  type: TYPE_NORMAL
- en: 使用 `LockOSThread` 和 `GODEBUG` 标志进行线程固定[¶](#thread-pinning-with-lockosthread-and-godebug-flags
    "永久链接")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Go 提供了 `runtime.LockOSThread()` 这样的工具来将 goroutine 锁定到特定的操作系统线程，但在大多数实际应用中，这种收益微乎其微。基准测试一致显示，对于典型的服务器工作负载——尤其是那些
    CPU 密集型的——Go 的调度器能够很好地处理线程放置，无需人工干预。引入线程固定通常会增加复杂性，而不会带来可衡量的收益。
  prefs: []
  type: TYPE_NORMAL
- en: 存在例外。在超低延迟或实时系统中，固定线程可以帮助通过避免线程迁移来减少抖动。但这些收益通常需要隔离的 CPU 核心、严格控制的环境和严格的延迟目标。在实践中，这意味着裸机。在共享基础设施上——尤其是在
    AWS 这样的云环境中，核心是虚拟化的，并且存在嘈杂的邻居——线程固定很少带来可衡量的好处。
  prefs: []
  type: TYPE_NORMAL
- en: 如果你正在探索固定，仅仅假设有好处是不够的——你需要对其进行基准测试。启用`GODEBUG=schedtrace=1000,scheddetail=1`可以提供关于goroutine如何调度以及竞争或迁移是否真正成为问题的详细洞察。没有这样的证据，线程固定更有可能阻碍而不是帮助。
  prefs: []
  type: TYPE_NORMAL
- en: 下面是开发者可能谨慎固定线程的方法：
  prefs: []
  type: TYPE_NORMAL
- en: '```go'
  prefs: []
  type: TYPE_NORMAL
- en: runtime.LockOSThread()
  prefs: []
  type: TYPE_NORMAL
- en: defer runtime.UnlockOSThread()
  prefs: []
  type: TYPE_NORMAL
- en: // perform critical latency-sensitive work here
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: 总是搭配广泛的指标收集和调度跟踪（`GODEBUG=schedtrace=1000,scheddetail=1`）来验证相对于Go稳健默认调度行为的实际收益。
  prefs: []
  type: TYPE_NORMAL
- en: CPU亲和力和外部工具[¶](#cpu-affinity-and-external-tools "永久链接")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 使用像`taskset`这样的外部工具或系统调用如`sched_setaffinity`可以将线程或进程绑定到特定的CPU核心。虽然理论上对缓存局部性和可预测性能有益，但广泛的基准测试一致表明，在大多数Go应用程序中，其实际价值有限。
  prefs: []
  type: TYPE_NORMAL
- en: 显式CPU亲和力管理通常仅在严格控制的环境中有所帮助：
  prefs: []
  type: TYPE_NORMAL
- en: 实时延迟约束（微秒级抖动）。
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 通过Linux内核的`isolcpus`等手段，使用专用和隔离的CPU。
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 在NUMA硬件上避免线程迁移。
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 谨慎使用CPU亲和力的示例：
  prefs: []
  type: TYPE_NORMAL
- en: '```go'
  prefs: []
  type: TYPE_NORMAL
- en: func setAffinity(cpuList []int) error {
  prefs: []
  type: TYPE_NORMAL
- en: pid := os.Getpid()
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: var mask unix.CPUSet
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: for _, cpu := range cpuList {
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: mask.Set(cpu)
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: return unix.SchedSetaffinity(pid, &mask)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: func main() {
  prefs: []
  type: TYPE_NORMAL
- en: runtime.LockOSThread()
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: defer runtime.UnlockOSThread()
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: if err := setAffinity([]int{2, 3}); err != nil {
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'log.Fatalf("CPU affinity failed: %v", err)'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: // perform critical work with confirmed benefit
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: 没有专门的基准测试和验证，这些技术可能会降低性能，使其他进程饥饿，或者引入微妙的延迟退化。将线程固定和CPU亲和力视为高度专业化的工具——只有在经过细致的测量确认其益处后才能有效。
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 在调度器级别调整Go可以解锁显著的性能提升，但这需要深入了解P（处理器）、M（系统线程）和G（goroutine）。盲目增加`GOMAXPROCS`或未测量就固定线程可能会适得其反。建议将这些旋钮视为外科手术工具：使用`GODEBUG`跟踪来诊断，隔离亲和力或固定有意义的子系统，并始终通过基准测试和配置文件进行验证。
  prefs: []
  type: TYPE_NORMAL
- en: Go的运行时一直在不断发展。在抢占式调度和用户级中断方面的未来工作承诺将进一步降低尾部延迟并提高公平性。在此之前，这些低级杠杆仍然是榨取开发者Go服务性能的强大方式之一。
  prefs: []
  type: TYPE_NORMAL
